[
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html",
    "title": "Lab 5",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndrugdose = pd.read_csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/drug.csv')\n\ndrugdose.T.head()\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n\n\n\n\nPatientID\n69\n162\n181\n209\n308\n331\n340\n350\n360\n363\n...\n9679\n9682\n9693\n9714\n9735\n9865\n9878\n9941\n9947\n9990\n\n\nDrugDose\n2\n4\n1\n4\n2\n4\n4\n1\n2\n4\n...\n2\n2\n4\n2\n1\n3\n1\n2\n1\n2\n\n\nDisease\nB\nA\nB\nA\nA\nC\nC\nB\nB\nA\n...\nB\nB\nA\nC\nC\nB\nC\nB\nA\nC\n\n\nBloodP\n13\n-47\n12\n-4\n4\n37\n-19\n-9\n-17\n-41\n...\n15\n21\n6\n-32\n-7\n-24\n19\n23\n5\n-24\n\n\n\n\n4 rows × 170 columns"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#a",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#a",
    "title": "Lab 5",
    "section": "1.1 a",
    "text": "1.1 a\n\n\nCode\nax = sns.catplot(\n    drugdose, kind=\"bar\",\n    x=\"DrugDose\", y=\"BloodP\", hue=\"Disease\",\n    errorbar=None, palette=\"dark\", alpha=.6, height=6\n)\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\nThere seems to be a significant difference in blood pressure for the 100mg drug dose with disease B as well as the 200mg drug dose with disease B"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#b",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#b",
    "title": "Lab 5",
    "section": "1.2 b",
    "text": "1.2 b\n\n\nCode\ndrug_lm = smf.ols('BloodP ~ C(DrugDose) * C(Disease)', drugdose).fit()\n\nsm.stats.anova_lm(drug_lm, typ=2)\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(DrugDose)\n202.577573\n3.0\n0.156057\n9.256570e-01\n\n\nC(Disease)\n19276.486901\n2.0\n22.274702\n3.005823e-09\n\n\nC(DrugDose):C(Disease)\n17146.316981\n6.0\n6.604404\n3.021199e-06\n\n\nResidual\n68366.458868\n158.0\nNaN\nNaN\n\n\n\n\n\n\n\n\nInteraction between DrugDose and Disease seems to be significant in explaining BloodP"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#c",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#c",
    "title": "Lab 5",
    "section": "1.3 c",
    "text": "1.3 c\n\n\nCode\nunique_disease = np.sort(drugdose['Disease'].unique())\n\nfor disease in unique_disease:\n    sliced_data = smf.ols('BloodP ~ C(DrugDose)', drugdose[drugdose['Disease'] == disease]).fit()\n    print(f'\\nDisease: {disease}')\n    print(sm.stats.anova_lm(sliced_data)['PR(&gt;F)'])\n\n\n\nDisease: A\nC(DrugDose)    0.001123\nResidual            NaN\nName: PR(&gt;F), dtype: float64\n\nDisease: B\nC(DrugDose)    0.00027\nResidual           NaN\nName: PR(&gt;F), dtype: float64\n\nDisease: C\nC(DrugDose)    0.81447\nResidual           NaN\nName: PR(&gt;F), dtype: float64\n\n\n\nFor diseases A and B there seems to be significant differences in drug dose on blood pressure. Disease C does not seem to have a significant difference in drug dose on blood pressure."
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#a-1",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#a-1",
    "title": "Lab 5",
    "section": "2.1 a",
    "text": "2.1 a\n\n\nCode\ndisks_lm = smf.ols('Time ~ C(Technician) * C(Brand)', disks).fit()\n\nprint(disks_lm.summary())\nsm.stats.anova_lm(disks_lm, typ=2)\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   Time   R-squared:                       0.619\nModel:                            OLS   Adj. R-squared:                  0.569\nMethod:                 Least Squares   F-statistic:                     12.38\nDate:                Thu, 07 Sep 2023   Prob (F-statistic):           1.66e-13\nTime:                        11:03:41   Log-Likelihood:                -376.10\nNo. Observations:                  96   AIC:                             776.2\nDf Residuals:                      84   BIC:                             807.0\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n=========================================================================================================\n                                            coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------------\nIntercept                                37.5000      4.599      8.154      0.000      28.354      46.646\nC(Technician)[T.Bob]                     19.3750      6.504      2.979      0.004       6.441      32.309\nC(Technician)[T.Justin]                   5.0000      6.504      0.769      0.444      -7.934      17.934\nC(Technician)[T.Karen]                   35.5000      6.504      5.458      0.000      22.566      48.434\nC(Brand)[T.2]                            -6.2500      6.504     -0.961      0.339     -19.184       6.684\nC(Brand)[T.3]                            12.1250      6.504      1.864      0.066      -0.809      25.059\nC(Technician)[T.Bob]:C(Brand)[T.2]       21.5000      9.198      2.337      0.022       3.209      39.791\nC(Technician)[T.Justin]:C(Brand)[T.2]     8.7500      9.198      0.951      0.344      -9.541      27.041\nC(Technician)[T.Karen]:C(Brand)[T.2]    -11.0000      9.198     -1.196      0.235     -29.291       7.291\nC(Technician)[T.Bob]:C(Brand)[T.3]      -25.8750      9.198     -2.813      0.006     -44.166      -7.584\nC(Technician)[T.Justin]:C(Brand)[T.3]   -10.1250      9.198     -1.101      0.274     -28.416       8.166\nC(Technician)[T.Karen]:C(Brand)[T.3]     -0.1250      9.198     -0.014      0.989     -18.416      18.166\n==============================================================================\nOmnibus:                        2.790   Durbin-Watson:                   2.199\nProb(Omnibus):                  0.248   Jarque-Bera (JB):                2.505\nSkew:                           0.396   Prob(JB):                        0.286\nKurtosis:                       2.997   Cond. No.                         17.9\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\n\n\n\nsum_sq\ndf\nF\nPR(&gt;F)\n\n\n\n\nC(Technician)\n14797.875000\n3.0\n29.151194\n5.153348e-13\n\n\nC(Brand)\n343.145833\n2.0\n1.013974\n3.671708e-01\n\n\nC(Technician):C(Brand)\n7907.437500\n6.0\n7.788660\n1.122763e-06\n\n\nResidual\n14213.500000\n84.0\nNaN\nNaN\n\n\n\n\n\n\n\n\nAt a significant level of \\(\\alpha = 0.05\\), the overall F-test is significant in our model\nSimilarly, the interaction is also significant"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#b-1",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#b-1",
    "title": "Lab 5",
    "section": "2.2 b",
    "text": "2.2 b\nSince our interaction is significant, we no longer care about the significance of our main effects. With model hierarchy, we keep the main effects in our model as well"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/lab_5.html#c-1",
    "href": "notes/analytics/ANOVA/n-Way/lab_5.html#c-1",
    "title": "Lab 5",
    "section": "2.3 c",
    "text": "2.3 c\n\n\nCode\nimport statsmodels.stats.multicomp as mc\n\nunique_brands = np.sort(disks['Brand'].unique())\n\nfor b in unique_brands:\n    sliced_data = disks[disks['Brand'] == b]\n    sliced_ols = smf.ols('Time ~ C(Technician)', sliced_data).fit()\n    print(f'\\nBrand: {b}')\n    print(mc.MultiComparison(sliced_data['Time'], sliced_data['Technician']).tukeyhsd(alpha=0.01))\n    print(sm.stats.anova_lm(sliced_ols))\n\n\n\nBrand: 1\n Multiple Comparison of Means - Tukey HSD, FWER=0.01 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\nAngela    Bob   19.375 0.0118  -0.3933 39.1433  False\nAngela Justin      5.0 0.8233 -14.7683 24.7683  False\nAngela  Karen     35.5    0.0  15.7317 55.2683   True\n   Bob Justin  -14.375 0.0847 -34.1433  5.3933  False\n   Bob  Karen   16.125 0.0442  -3.6433 35.8933  False\nJustin  Karen     30.5 0.0001  10.7317 50.2683   True\n-----------------------------------------------------\n                 df      sum_sq      mean_sq          F    PR(&gt;F)\nC(Technician)   3.0  6115.09375  2038.364583  15.208129  0.000005\nResidual       28.0  3752.87500   134.031250        NaN       NaN\n\nBrand: 2\n Multiple Comparison of Means - Tukey HSD, FWER=0.01 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\nAngela    Bob   40.875    0.0  15.6475 66.1025   True\nAngela Justin    13.75 0.2673 -11.4775 38.9775  False\nAngela  Karen     24.5 0.0127  -0.7275 49.7275  False\n   Bob Justin  -27.125 0.0052 -52.3525 -1.8975   True\n   Bob  Karen  -16.375 0.1434 -41.6025  8.8525  False\nJustin  Karen    10.75  0.477 -14.4775 35.9775  False\n-----------------------------------------------------\n                 df      sum_sq      mean_sq          F    PR(&gt;F)\nC(Technician)   3.0  7159.09375  2386.364583  10.932522  0.000063\nResidual       28.0  6111.87500   218.281250        NaN       NaN\n\nBrand: 3\n Multiple Comparison of Means - Tukey HSD, FWER=0.01 \n=====================================================\ngroup1 group2 meandiff p-adj   lower    upper  reject\n-----------------------------------------------------\nAngela    Bob     -6.5 0.7259 -27.7799 14.7799  False\nAngela Justin   -5.125 0.8433 -26.4049 16.1549  False\nAngela  Karen   35.375    0.0  14.0951 56.6549   True\n   Bob Justin    1.375 0.9961 -19.9049 22.6549  False\n   Bob  Karen   41.875    0.0  20.5951 63.1549   True\nJustin  Karen     40.5    0.0  19.2201 61.7799   True\n-----------------------------------------------------\n                 df    sum_sq      mean_sq         F        PR(&gt;F)\nC(Technician)   3.0  9431.125  3143.708333  20.24118  3.537879e-07\nResidual       28.0  4348.750   155.312500       NaN           NaN\n\n\n\nThere are differences between different technicians for each brand of disk drive"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html",
    "href": "notes/analytics/ANOVA/n-Way/index.html",
    "title": "More Complex ANOVA & Regression",
    "section": "",
    "text": "flowchart LR\n    A[Continuous Target Variable] --&gt; B[One-Way ANOVA]\n    A --&gt; C[Two-Way ANOVA]\n    A --&gt; D[n-Way ANOVA]"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html#post-hoc-testing",
    "href": "notes/analytics/ANOVA/n-Way/index.html#post-hoc-testing",
    "title": "More Complex ANOVA & Regression",
    "section": "4.1 Post-Hoc Testing",
    "text": "4.1 Post-Hoc Testing\nWe have statistical differences among the categories and we want to know where these differences exist.\n\n\nCode\ntukey_ames2 &lt;- TukeyHSD(ames_aov2)\nprint(tukey_ames2)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sale_Price ~ Heating_QC + Central_Air, data = train)\n\n$Heating_QC\n                       diff        lwr       upr     p adj\nFair-Poor          49176.42 -63650.448 162003.29 0.7571980\nTypical-Poor       67781.01 -42800.320 178362.35 0.4506761\nGood-Poor          87753.89 -23040.253 198548.03 0.1945181\nExcellent-Poor    146288.89  35818.859 256758.92 0.0028361\nTypical-Fair       18604.59  -6326.425  43535.61 0.2484556\nGood-Fair          38577.47  12718.894  64436.04 0.0004622\nExcellent-Fair     97112.47  72679.867 121545.07 0.0000000\nGood-Typical       19972.87   7050.230  32895.52 0.0002470\nExcellent-Typical  78507.88  68746.678  88269.07 0.0000000\nExcellent-Good     58535.00  46602.229  70467.78 0.0000000\n\n$Central_Air\n        diff      lwr      upr p adj\nY-N 43256.57 31508.27 55004.87     0\n\n\nCode\nplot(tukey_ames2, las = 1)\n\n\n\n\n\n\n\n\n\ndiff refers to the average difference in Sale_Price between the two categories\n\nKeep in mind that if you increase your sample size, you should decrease your significance level. P-values always go down with an increase in sample size."
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html#r-code",
    "href": "notes/analytics/ANOVA/n-Way/index.html#r-code",
    "title": "More Complex ANOVA & Regression",
    "section": "5.1 R Code",
    "text": "5.1 R Code\n\n\nCode\names_aov_int &lt;- aov(Sale_Price ~ Heating_QC * Central_Air, data = train)\nsummary(ames_aov_int)\n\n\n                         Df    Sum Sq   Mean Sq F value   Pr(&gt;F)    \nHeating_QC                4 2.891e+12 7.228e+11 147.897  &lt; 2e-16 ***\nCentral_Air               1 2.903e+11 2.903e+11  59.403 1.99e-14 ***\nHeating_QC:Central_Air    4 3.972e+10 9.930e+09   2.032   0.0875 .  \nResiduals              2041 9.975e+12 4.887e+09                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nWhen you’re looking at significance, make sure to look at the interaction’s significance first.\n\nIf interaction exists, we no longer care if the individual variables are significant or not because it’s the interaction that matters."
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html#model-hierarchy",
    "href": "notes/analytics/ANOVA/n-Way/index.html#model-hierarchy",
    "title": "More Complex ANOVA & Regression",
    "section": "5.2 Model Hierarchy",
    "text": "5.2 Model Hierarchy\nIdea of model hierarchy: If higher-order terms are significant, then we should keep all the main effect terms that are a part of the higher-order terms as well."
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html#slicing",
    "href": "notes/analytics/ANOVA/n-Way/index.html#slicing",
    "title": "More Complex ANOVA & Regression",
    "section": "5.3 Slicing",
    "text": "5.3 Slicing\nIf the interaction term was significant, the number of level pairs we would have to consider can be overwhelming. Slicing performs an F-test for means for one variable within the level of another variable.\nAn example is subsetting the data into Central_Air: Yes and Central_Air: No and then seeing the significance of Heating_QC:\n\n\nCode\nCA_aov &lt;- train %&gt;%\n    group_by(Central_Air) %&gt;%\n    nest() %&gt;%\n    mutate(aov = map(data, ~ summary(aov(Sale_Price ~ Heating_QC, data = .x))))\nprint(CA_aov$aov)\n\n\n[[1]]\n              Df    Sum Sq   Mean Sq F value Pr(&gt;F)    \nHeating_QC     4 2.242e+12 5.606e+11   108.5 &lt;2e-16 ***\nResiduals   1899 9.809e+12 5.165e+09                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n[[2]]\n             Df    Sum Sq   Mean Sq F value  Pr(&gt;F)   \nHeating_QC    4 1.774e+10 4.435e+09   3.793 0.00582 **\nResiduals   142 1.660e+11 1.169e+09                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/index.html#assumptions",
    "href": "notes/analytics/ANOVA/n-Way/index.html#assumptions",
    "title": "More Complex ANOVA & Regression",
    "section": "5.4 Assumptions",
    "text": "5.4 Assumptions\nSame as One-Way ANOVA:\n\nIndependence of observations\nEquality of variance\n\nLevene Test only available for interactions\n\nNormality of categories"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/lab_3.html",
    "href": "notes/analytics/ANOVA/introduction/lab_3.html",
    "title": "1 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ngarlic &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/garlic.csv\")\nglimpse(garlic)\n\n\nRows: 32\nColumns: 3\n$ BedID      &lt;int&gt; 101, 102, 103, 104, 105, 106, 107, 108, 201, 202, 203, 204,…\n$ Fertilizer &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3,…\n$ BulbWt     &lt;dbl&gt; 0.2391642, 0.2582814, 0.2047856, 0.2433666, 0.2726395, 0.21…"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/lab_3.html#a",
    "href": "notes/analytics/ANOVA/introduction/lab_3.html#a",
    "title": "1 1",
    "section": "2.1 a",
    "text": "2.1 a\nVerifying normality:\n\n\nCode\ngarlic_lm &lt;- lm(BulbWt ~ factor(Fertilizer), data = garlic)\nggplot(garlic, aes(x = BulbWt, fill = factor(Fertilizer))) +\n    geom_density(alpha = 0.2, position = \"identity\") +\n    labs(x = \"Bulb Weight\")\n\n\n\n\n\n\nGroups do not appear to be Normally distributed\n\nVerifying equal variance:\n\n\nCode\nfligner.test(BulbWt ~ factor(Fertilizer), data = garlic)$p.value\n\n\n[1] 0.4744925\n\n\n\nThrough the Fligner test, we do not have enough evidence to claim that there are different variances\n\n\n\nCode\nkruskal.test(BulbWt ~ factor(Fertilizer), data = garlic)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  BulbWt by factor(Fertilizer)\nKruskal-Wallis chi-squared = 12.384, df = 3, p-value = 0.006178\n\n\n\nGiven the p-value, we believe that there is a significant difference in location. Values in one or more of the groups tend to be higher than others"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/lab_3.html#b",
    "href": "notes/analytics/ANOVA/introduction/lab_3.html#b",
    "title": "1 1",
    "section": "2.2 b",
    "text": "2.2 b\n4 choose 2 so 6 tests"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/lab_3.html#c",
    "href": "notes/analytics/ANOVA/introduction/lab_3.html#c",
    "title": "1 1",
    "section": "2.3 c",
    "text": "2.3 c\n\n\nCode\n1 - (1 - 0.05)^6\n\n\n[1] 0.2649081\n\n\nWe solve this problem of experiment-wise error rate by using Tukey’s Honest Significant Difference."
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/lab_3.html#d",
    "href": "notes/analytics/ANOVA/introduction/lab_3.html#d",
    "title": "1 1",
    "section": "2.4 d",
    "text": "2.4 d\n\n\nCode\ngarlic_aov &lt;- aov(BulbWt ~ factor(Fertilizer), data = garlic)\n\ntukey_garlic &lt;- TukeyHSD(garlic_aov)\nprint(tukey_garlic)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = BulbWt ~ factor(Fertilizer), data = garlic)\n\n$`factor(Fertilizer)`\n            diff          lwr         upr     p adj\n2-1 -0.030285751 -0.076646102  0.01607460 0.3020823\n3-1  0.007007653 -0.039352697  0.05336800 0.9758350\n4-1 -0.061634930 -0.107995281 -0.01527458 0.0058337\n3-2  0.037293404 -0.009066947  0.08365375 0.1489779\n4-2 -0.031349179 -0.077709530  0.01501117 0.2737971\n4-3 -0.068642583 -0.115002934 -0.02228223 0.0020089\n\n\n\nFertilizers 4 and 1 as well as Fertilizers 4 and 3 are statistically different from each other. Pairs 2-1, 3-1, 3-2, 4-2 are not statistically different."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/lab_7.html",
    "href": "notes/analytics/linear-regression/model-selection/lab_7.html",
    "title": "Lab 7",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(AppliedPredictiveModeling)\ndata(FuelEconomy)\n\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(across(!c(EngDispl, FE), as.factor))\n\n\n\n1 a\n\n\nCode\nfull.model &lt;- lm(FE ~ ., data = cars2010)\nempty.model &lt;- lm(FE ~ 1, data = cars2010)\n\nfor.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ), direction = \"forward\", k = qchisq(0.10, 1, lower.tail = FALSE)\n)\n\n\nStart:  AIC=4462.12\nFE ~ 1\n\n                      Df Sum of Sq   RSS    AIC\n+ EngDispl             1     38551 23629 3393.7\n+ NumCyl               8     37510 24669 3460.4\n+ DriveDesc            4     27081 35099 3839.9\n+ CarlineClassDesc    16     20274 41906 4068.6\n+ Transmission        15     10894 51285 4289.5\n+ ExhaustValvesPerCyl  2      7168 55012 4331.9\n+ IntakeValvePerCyl    3      5212 56968 4373.3\n+ TransLockup          1      4598 57582 4379.8\n+ NumGears             5      3807 58372 4405.7\n+ VarValveTiming       1       971 61209 4447.4\n+ AirAspirationMethod  2       819 61361 4452.9\n+ VarValveLift         1       576 61604 4454.5\n+ TransCreeperGear     1       301 61878 4459.4\n&lt;none&gt;                             62180 4462.1\n\nStep:  AIC=3393.74\nFE ~ EngDispl\n\n                      Df Sum of Sq   RSS    AIC\n+ CarlineClassDesc    16    6046.9 17582 3109.8\n+ DriveDesc            4    5489.6 18139 3111.9\n+ NumCyl               8    2065.5 21564 3314.1\n+ Transmission        15    1837.0 21792 3344.7\n+ NumGears             5     944.9 22684 3362.1\n+ IntakeValvePerCyl    3     796.8 22832 3363.9\n+ ExhaustValvesPerCyl  2     682.3 22947 3366.7\n+ TransLockup          1     556.5 23072 3370.1\n+ VarValveTiming       1     316.8 23312 3381.5\n+ TransCreeperGear     1     147.1 23482 3389.5\n+ AirAspirationMethod  2     183.5 23446 3390.5\n&lt;none&gt;                             23629 3393.7\n+ VarValveLift         1      49.3 23580 3394.1\n\nStep:  AIC=3109.81\nFE ~ EngDispl + CarlineClassDesc\n\n                      Df Sum of Sq   RSS    AIC\n+ NumCyl               8   3095.27 14487 2917.1\n+ DriveDesc            4   2769.57 14812 2930.9\n+ Transmission        15   2214.76 15367 3001.3\n+ IntakeValvePerCyl    3   1583.01 15999 3013.5\n+ ExhaustValvesPerCyl  2   1321.06 16261 3028.8\n+ NumGears             5    785.78 16796 3072.7\n+ AirAspirationMethod  2    554.30 17028 3079.8\n+ TransLockup          1    157.41 17425 3102.6\n+ VarValveTiming       1     47.86 17534 3109.5\n+ TransCreeperGear     1     44.15 17538 3109.7\n&lt;none&gt;                             17582 3109.8\n+ VarValveLift         1      2.69 17579 3112.3\n\nStep:  AIC=2917.09\nFE ~ EngDispl + CarlineClassDesc + NumCyl\n\n                      Df Sum of Sq   RSS    AIC\n+ DriveDesc            4   2018.52 12468 2761.8\n+ Transmission        15   1373.88 13113 2847.4\n+ IntakeValvePerCyl    2    550.79 13936 2879.6\n+ ExhaustValvesPerCyl  1    472.21 14015 2883.1\n+ AirAspirationMethod  2    417.97 14069 2890.1\n+ NumGears             5    469.35 14018 2894.2\n+ TransLockup          1     57.39 14430 2915.4\n&lt;none&gt;                             14487 2917.1\n+ VarValveTiming       1     34.17 14453 2917.2\n+ TransCreeperGear     1      9.42 14477 2919.1\n+ VarValveLift         1      7.55 14479 2919.2\n\nStep:  AIC=2761.81\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc\n\n                      Df Sum of Sq   RSS    AIC\n+ Transmission        15   1139.29 11329 2696.3\n+ NumGears             5    504.45 11964 2729.6\n+ IntakeValvePerCyl    2    399.50 12069 2731.2\n+ ExhaustValvesPerCyl  1    336.65 12132 2734.2\n+ AirAspirationMethod  2    114.72 12354 2757.0\n+ VarValveLift         1     61.68 12407 2759.0\n+ TransLockup          1     60.82 12408 2759.1\n&lt;none&gt;                             12468 2761.8\n+ VarValveTiming       1     27.11 12441 2762.1\n+ TransCreeperGear     1     13.62 12455 2763.3\n\nStep:  AIC=2696.32\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission\n\n                      Df Sum of Sq   RSS    AIC\n+ IntakeValvePerCyl    2    461.88 10867 2655.7\n+ ExhaustValvesPerCyl  1    383.34 10946 2660.9\n+ NumGears             2    153.67 11175 2686.6\n+ VarValveLift         1    113.24 11216 2687.9\n+ AirAspirationMethod  2    107.19 11222 2691.2\n+ TransLockup          1     58.68 11270 2693.3\n&lt;none&gt;                             11329 2696.3\n+ TransCreeperGear     1     24.24 11305 2696.7\n+ VarValveTiming       1      0.83 11328 2698.9\n\nStep:  AIC=2655.65\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl\n\n                      Df Sum of Sq   RSS    AIC\n+ VarValveLift         1   137.817 10729 2644.2\n+ AirAspirationMethod  2   159.280 10708 2644.7\n+ NumGears             2   122.472 10745 2648.5\n+ TransCreeperGear     1    43.328 10824 2653.9\n+ TransLockup          1    43.023 10824 2654.0\n&lt;none&gt;                             10867 2655.7\n+ ExhaustValvesPerCyl  1    10.801 10856 2657.2\n+ VarValveTiming       1     9.160 10858 2657.4\n\nStep:  AIC=2644.23\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift\n\n                      Df Sum of Sq   RSS    AIC\n+ AirAspirationMethod  2   112.550 10617 2638.0\n+ NumGears             2   110.633 10619 2638.2\n+ TransLockup          1    46.480 10683 2642.1\n+ TransCreeperGear     1    33.554 10696 2643.5\n&lt;none&gt;                             10729 2644.2\n+ ExhaustValvesPerCyl  1    12.071 10717 2645.7\n+ VarValveTiming       1     9.141 10720 2646.0\n\nStep:  AIC=2637.96\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift + AirAspirationMethod\n\n                      Df Sum of Sq   RSS    AIC\n+ NumGears             2   113.576 10503 2631.5\n+ TransLockup          1    59.507 10557 2634.4\n+ TransCreeperGear     1    36.860 10580 2636.8\n+ ExhaustValvesPerCyl  1    27.327 10589 2637.8\n&lt;none&gt;                             10617 2638.0\n+ VarValveTiming       1     5.601 10611 2640.1\n\nStep:  AIC=2631.47\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift + AirAspirationMethod + \n    NumGears\n\n                      Df Sum of Sq   RSS    AIC\n+ TransLockup          1    65.211 10438 2627.3\n+ TransCreeperGear     1    34.620 10469 2630.5\n&lt;none&gt;                             10503 2631.5\n+ ExhaustValvesPerCyl  1    22.284 10481 2631.8\n+ VarValveTiming       1     5.577 10498 2633.6\n\nStep:  AIC=2627.28\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift + AirAspirationMethod + \n    NumGears + TransLockup\n\n                      Df Sum of Sq   RSS    AIC\n+ TransCreeperGear     1    32.717 10405 2626.5\n&lt;none&gt;                             10438 2627.3\n+ ExhaustValvesPerCyl  1    21.528 10416 2627.7\n+ VarValveTiming       1     5.583 10432 2629.4\n\nStep:  AIC=2626.51\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift + AirAspirationMethod + \n    NumGears + TransLockup + TransCreeperGear\n\n                      Df Sum of Sq   RSS    AIC\n+ ExhaustValvesPerCyl  1    39.294 10366 2625.0\n&lt;none&gt;                             10405 2626.5\n+ VarValveTiming       1     1.042 10404 2629.1\n\nStep:  AIC=2625.03\nFE ~ EngDispl + CarlineClassDesc + NumCyl + DriveDesc + Transmission + \n    IntakeValvePerCyl + VarValveLift + AirAspirationMethod + \n    NumGears + TransLockup + TransCreeperGear + ExhaustValvesPerCyl\n\n                 Df Sum of Sq   RSS    AIC\n&lt;none&gt;                        10366 2625.0\n+ VarValveTiming  1    2.9656 10363 2627.4\n\n\n\nFirst variable added is EngDispl\nLast variable added was ExhaustValvesPerCyl\n\n\n\n2 b\n\n\nCode\nstep.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ), direction = \"both\", k = log(nrow(cars2010))\n)\n\n\nStart:  AIC=4466.42\nFE ~ 1\n\n                      Df Sum of Sq   RSS    AIC\n+ EngDispl             1     38551 23629 3402.3\n+ NumCyl               8     37510 24669 3499.1\n+ DriveDesc            4     27081 35099 3861.4\n+ CarlineClassDesc    16     20274 41906 4141.7\n+ ExhaustValvesPerCyl  2      7168 55012 4344.9\n+ Transmission        15     10894 51285 4358.3\n+ TransLockup          1      4598 57582 4388.4\n+ IntakeValvePerCyl    3      5212 56968 4390.5\n+ NumGears             5      3807 58372 4431.5\n+ VarValveTiming       1       971 61209 4456.0\n+ VarValveLift         1       576 61604 4463.1\n+ AirAspirationMethod  2       819 61361 4465.8\n&lt;none&gt;                             62180 4466.4\n+ TransCreeperGear     1       301 61878 4468.1\n\nStep:  AIC=3402.35\nFE ~ EngDispl\n\n                      Df Sum of Sq   RSS    AIC\n+ DriveDesc            4      5490 18139 3137.7\n+ CarlineClassDesc    16      6047 17582 3187.3\n+ NumCyl               8      2066 21564 3357.2\n+ TransLockup          1       556 23073 3383.0\n+ ExhaustValvesPerCyl  2       682 22947 3383.9\n+ IntakeValvePerCyl    3       797 22832 3385.4\n+ NumGears             5       945 22684 3392.2\n+ VarValveTiming       1       317 23312 3394.4\n&lt;none&gt;                             23629 3402.3\n+ TransCreeperGear     1       147 23482 3402.4\n+ VarValveLift         1        49 23580 3407.0\n+ AirAspirationMethod  2       183 23446 3407.7\n+ Transmission        15      1837 21792 3417.9\n- EngDispl             1     38551 62180 4466.4\n\nStep:  AIC=3137.71\nFE ~ EngDispl + DriveDesc\n\n                      Df Sum of Sq   RSS    AIC\n+ CarlineClassDesc    16    3326.9 14813 3025.6\n+ NumCyl               8    1406.8 16733 3104.4\n+ NumGears             5    1076.1 17063 3105.1\n+ Transmission        15    1951.3 16188 3116.9\n+ TransLockup          1     429.3 17710 3118.2\n+ VarValveLift         1     342.4 17797 3123.6\n+ ExhaustValvesPerCyl  2     382.7 17757 3128.1\n+ IntakeValvePerCyl    3     481.3 17658 3129.0\n+ VarValveTiming       1     217.9 17921 3131.3\n&lt;none&gt;                             18139 3137.7\n+ TransCreeperGear     1      93.7 18046 3139.0\n+ AirAspirationMethod  2       8.5 18131 3151.2\n- DriveDesc            4    5489.6 23629 3402.3\n- EngDispl             1   16959.3 35099 3861.4\n\nStep:  AIC=3025.57\nFE ~ EngDispl + DriveDesc + CarlineClassDesc\n\n                      Df Sum of Sq   RSS    AIC\n+ NumCyl               8    2344.2 12468 2890.9\n+ IntakeValvePerCyl    3    1195.8 13617 2953.4\n+ ExhaustValvesPerCyl  2     957.0 13856 2965.6\n+ Transmission        15    1652.0 13161 2999.8\n+ NumGears             5     597.2 14215 3015.1\n+ TransLockup          1     104.8 14708 3024.7\n+ VarValveLift         1     100.8 14712 3025.0\n&lt;none&gt;                             14812 3025.6\n+ AirAspirationMethod  2     153.2 14659 3028.1\n+ TransCreeperGear     1      38.4 14774 3029.7\n+ VarValveTiming       1      35.8 14777 3029.9\n- CarlineClassDesc    16    3326.9 18139 3137.7\n- DriveDesc            4    2769.6 17582 3187.3\n- EngDispl             1   12424.8 27237 3692.8\n\nStep:  AIC=2890.92\nFE ~ EngDispl + DriveDesc + CarlineClassDesc + NumCyl\n\n                      Df Sum of Sq   RSS    AIC\n+ ExhaustValvesPerCyl  1     336.6 12132 2867.6\n+ IntakeValvePerCyl    2     399.5 12069 2868.9\n+ NumGears             5     504.5 11964 2880.2\n+ Transmission        15    1139.3 11329 2890.0\n&lt;none&gt;                             12468 2890.9\n+ VarValveLift         1      61.7 12407 2892.4\n+ TransLockup          1      60.8 12408 2892.5\n+ AirAspirationMethod  2     114.7 12354 2894.7\n+ VarValveTiming       1      27.1 12441 2895.5\n+ TransCreeperGear     1      13.6 12455 2896.7\n- EngDispl             1     481.6 12950 2925.9\n- NumCyl               8    2344.2 14812 3025.6\n- DriveDesc            4    2018.5 14487 3029.0\n- CarlineClassDesc    16    4264.3 16733 3104.4\n\nStep:  AIC=2867.63\nFE ~ EngDispl + DriveDesc + CarlineClassDesc + NumCyl + ExhaustValvesPerCyl\n\n                      Df Sum of Sq   RSS    AIC\n+ NumGears             5     645.1 11487 2842.2\n+ Transmission        15    1186.0 10946 2858.9\n+ VarValveTiming       1      94.5 12037 2866.0\n+ VarValveLift         1      87.9 12044 2866.6\n&lt;none&gt;                             12132 2867.6\n+ AirAspirationMethod  2     150.1 11982 2867.9\n+ TransLockup          1      61.3 12070 2869.0\n+ TransCreeperGear     1      57.8 12074 2869.4\n+ IntakeValvePerCyl    2      71.1 12061 2875.2\n- ExhaustValvesPerCyl  1     336.6 12468 2890.9\n- EngDispl             1     674.5 12806 2920.5\n- NumCyl               7    1723.9 13856 2965.7\n- DriveDesc            4    1883.0 14015 2999.3\n- CarlineClassDesc    16    4578.1 16710 3109.9\n\nStep:  AIC=2842.19\nFE ~ EngDispl + DriveDesc + CarlineClassDesc + NumCyl + ExhaustValvesPerCyl + \n    NumGears\n\n                      Df Sum of Sq   RSS    AIC\n+ AirAspirationMethod  2     230.4 11256 2833.8\n+ VarValveLift         1      89.3 11397 2840.6\n&lt;none&gt;                             11487 2842.2\n+ TransCreeperGear     1      70.1 11416 2842.4\n+ VarValveTiming       1      29.7 11457 2846.3\n+ IntakeValvePerCyl    2      78.0 11409 2848.7\n+ TransLockup          1       5.1 11482 2848.7\n+ Transmission        12     657.4 10829 2861.1\n- NumGears             5     645.1 12132 2867.6\n- ExhaustValvesPerCyl  1     477.3 11964 2880.2\n- EngDispl             1     701.9 12188 2900.8\n- NumCyl               7    1661.8 13148 2942.7\n- DriveDesc            4    2012.5 13499 2992.9\n- CarlineClassDesc    16    3701.5 15188 3039.3\n\nStep:  AIC=2833.79\nFE ~ EngDispl + DriveDesc + CarlineClassDesc + NumCyl + ExhaustValvesPerCyl + \n    NumGears + AirAspirationMethod\n\n                      Df Sum of Sq   RSS    AIC\n+ TransCreeperGear     1      87.8 11168 2832.1\n&lt;none&gt;                             11256 2833.8\n+ VarValveLift         1      53.0 11203 2835.6\n+ VarValveTiming       1      24.0 11232 2838.4\n+ TransLockup          1       5.2 11251 2840.3\n+ IntakeValvePerCyl    2      65.5 11191 2841.3\n- AirAspirationMethod  2     230.4 11487 2842.2\n+ Transmission        12     620.0 10636 2855.2\n- NumGears             5     725.4 11982 2867.9\n- ExhaustValvesPerCyl  1     561.0 11817 2880.6\n- EngDispl             1     800.7 12057 2902.8\n- NumCyl               7    1623.7 12880 2933.9\n- DriveDesc            4    1659.9 12916 2958.0\n- CarlineClassDesc    16    3863.1 15119 3048.3\n\nStep:  AIC=2832.13\nFE ~ EngDispl + DriveDesc + CarlineClassDesc + NumCyl + ExhaustValvesPerCyl + \n    NumGears + AirAspirationMethod + TransCreeperGear\n\n                      Df Sum of Sq   RSS    AIC\n&lt;none&gt;                             11168 2832.1\n- TransCreeperGear     1      87.8 11256 2833.8\n+ VarValveLift         1      42.5 11126 2834.9\n+ VarValveTiming       1       8.2 11160 2838.3\n+ TransLockup          1       2.9 11166 2838.9\n+ IntakeValvePerCyl    2      44.3 11124 2841.8\n- AirAspirationMethod  2     248.0 11416 2842.4\n+ Transmission        12     613.6 10555 2853.7\n- NumGears             5     748.6 11917 2868.9\n- ExhaustValvesPerCyl  1     633.5 11802 2886.2\n- EngDispl             1     852.6 12021 2906.6\n- NumCyl               7    1544.6 12713 2926.5\n- DriveDesc            4    1648.4 12817 2956.5\n- CarlineClassDesc    16    3835.2 15004 3046.8\n\n\n\n8 variables remain after stepwise\nDid not get the same models between the two different methods. Stepwise had less"
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html",
    "href": "notes/analytics/linear-regression/model-selection/index.html",
    "title": "Model Selection",
    "section": "",
    "text": "Information criteria are metrics that are commonly used to “select” variables for the model.\nSelection algorithm is an automated technique to evaluate variables based on some selection criteria.\n\nStepwise Selection (forward, backward, stepwise)\nAll-regression Selection (\\(R^2\\), \\(R_a^2\\), Mallow’s \\(C_p\\))\n\nAll-regression selection tends to be unusable for datasets of a large size so we focus on stepwise selection for now."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#akaike-information-criterion",
    "href": "notes/analytics/linear-regression/model-selection/index.html#akaike-information-criterion",
    "title": "Model Selection",
    "section": "3.1 Akaike Information Criterion",
    "text": "3.1 Akaike Information Criterion\n\\[\nAIC = -2\\log(L) + 2k\n\\]\nCrude, large-sample approximation of leave-one-out cross-validation."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#bayesian-information-criterion",
    "href": "notes/analytics/linear-regression/model-selection/index.html#bayesian-information-criterion",
    "title": "Model Selection",
    "section": "3.2 Bayesian Information Criterion",
    "text": "3.2 Bayesian Information Criterion\n\\[\nBIC = -2\\log(L) + k\\log(n)\n\\]\nFavors smaller models and penalizes model complexity more."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#r-code",
    "href": "notes/analytics/linear-regression/model-selection/index.html#r-code",
    "title": "Model Selection",
    "section": "4.1 R Code",
    "text": "4.1 R Code\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(AmesHousing)\n\names &lt;- make_ordinal_ames()\ntrain &lt;- sample_frac(ames, 0.7)\ntrain_sel &lt;- train %&gt;%\n    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %&gt;%\n    replace(is.na(.), 0)\n\n\n\n\nCode\nfull.model &lt;- lm(Sale_Price ~ ., data = train_sel)\nempty.model &lt;- lm(Sale_Price ~ 1, data = train_sel)\n\nfor.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ),\n    direction = \"forward\",\n    k = 2,\n    trace = FALSE\n)\nfor.model\n\n\n\nCall:\nlm(formula = Sale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + \n    Garage_Area + Bldg_Type + Lot_Area + Fireplaces + Central_Air + \n    Roof_Style + Half_Bath + Full_Bath + First_Flr_SF + TotRms_AbvGrd + \n    Second_Flr_SF, data = train_sel)\n\nCoefficients:\n                (Intercept)               Overall_Qual.L  \n                  49308.311                   226471.803  \n             Overall_Qual.Q               Overall_Qual.C  \n                  82180.741                    38253.369  \n             Overall_Qual^4               Overall_Qual^5  \n                 -16484.729                    -8753.436  \n             Overall_Qual^6               Overall_Qual^7  \n                 -14290.374                     -557.745  \n             Overall_Qual^8               Overall_Qual^9  \n                  -2270.430                     -817.895  \n                Gr_Liv_Area  House_StyleOne_and_Half_Unf  \n                      7.251                     9702.783  \n       House_StyleOne_Story            House_StyleSFoyer  \n                  15666.481                    32716.305  \n            House_StyleSLvl  House_StyleTwo_and_Half_Fin  \n                   8472.968                    -4423.776  \nHouse_StyleTwo_and_Half_Unf         House_StyleTwo_Story  \n                 -10757.587                     5306.240  \n                Garage_Area            Bldg_TypeTwoFmCon  \n                     33.458                   -13910.202  \n            Bldg_TypeDuplex               Bldg_TypeTwnhs  \n                 -29913.678                   -11760.427  \n            Bldg_TypeTwnhsE                     Lot_Area  \n                  -5395.254                        0.579  \n                 Fireplaces                 Central_AirY  \n                   7424.436                    13095.719  \n            Roof_StyleGable            Roof_StyleGambrel  \n                  -8675.172                   -16732.110  \n              Roof_StyleHip            Roof_StyleMansard  \n                  -4868.546                   -59767.670  \n             Roof_StyleShed                    Half_Bath  \n                 -34105.646                     9907.065  \n                  Full_Bath                 First_Flr_SF  \n                   9811.399                       64.897  \n              TotRms_AbvGrd                Second_Flr_SF  \n                  -2503.751                       44.967  \n\n\n\nGood practice to specify the full and empty models\nk = 2 is selecting the AIC criteria"
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#python-code",
    "href": "notes/analytics/linear-regression/model-selection/index.html#python-code",
    "title": "Model Selection",
    "section": "4.2 Python Code",
    "text": "4.2 Python Code\n\n\nCode\n# from sklearn.feature_selection import SequentialFeatureSelector\n# import statsmodels.formula.api as smf\n\n# model = \n\n\nThe model obtained through forward selection is not necessarily a model where all variables are significant."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#other-criteria",
    "href": "notes/analytics/linear-regression/model-selection/index.html#other-criteria",
    "title": "Model Selection",
    "section": "4.3 Other Criteria",
    "text": "4.3 Other Criteria\nUsing k = qchisq(0.05, 1, lower.tail = FALSE) would replace the selection criteria with p-value selection. \\(\\alpha = 0.05\\) in this case.\nk = log(nrow(train_sel)) would use the BIC criterion."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#r-code-1",
    "href": "notes/analytics/linear-regression/model-selection/index.html#r-code-1",
    "title": "Model Selection",
    "section": "5.1 R Code",
    "text": "5.1 R Code\n\n\nCode\nfull.model &lt;- lm(Sale_Price ~ ., data = train_sel)\nempty.model &lt;- lm(Sale_Price ~ 1, data = train_sel)\n\nback.model &lt;- step(\n    full.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ),\n    direction = \"backward\",\n    k = 2,\n    trace = FALSE\n)\n\nback.model\n\n\n\nCall:\nlm(formula = Sale_Price ~ Lot_Area + Bldg_Type + House_Style + \n    Overall_Qual + Roof_Style + Central_Air + First_Flr_SF + \n    Second_Flr_SF + Full_Bath + Half_Bath + Fireplaces + Garage_Area + \n    TotRms_AbvGrd, data = train_sel)\n\nCoefficients:\n                (Intercept)                     Lot_Area  \n                  4.955e+04                    5.791e-01  \n          Bldg_TypeTwoFmCon              Bldg_TypeDuplex  \n                 -1.398e+04                   -3.000e+04  \n             Bldg_TypeTwnhs              Bldg_TypeTwnhsE  \n                 -1.176e+04                   -5.378e+03  \nHouse_StyleOne_and_Half_Unf         House_StyleOne_Story  \n                  9.319e+03                    1.523e+04  \n          House_StyleSFoyer              House_StyleSLvl  \n                  3.235e+04                    8.129e+03  \nHouse_StyleTwo_and_Half_Fin  House_StyleTwo_and_Half_Unf  \n                 -1.700e+03                   -1.086e+04  \n       House_StyleTwo_Story               Overall_Qual.L  \n                  5.209e+03                    2.264e+05  \n             Overall_Qual.Q               Overall_Qual.C  \n                  8.228e+04                    3.833e+04  \n             Overall_Qual^4               Overall_Qual^5  \n                 -1.645e+04                   -8.733e+03  \n             Overall_Qual^6               Overall_Qual^7  \n                 -1.422e+04                   -5.365e+02  \n             Overall_Qual^8               Overall_Qual^9  \n                 -2.253e+03                   -8.077e+02  \n            Roof_StyleGable            Roof_StyleGambrel  \n                 -8.721e+03                   -1.689e+04  \n              Roof_StyleHip            Roof_StyleMansard  \n                 -4.914e+03                   -5.980e+04  \n             Roof_StyleShed                 Central_AirY  \n                 -3.409e+04                    1.311e+04  \n               First_Flr_SF                Second_Flr_SF  \n                  7.207e+01                    5.170e+01  \n                  Full_Bath                    Half_Bath  \n                  9.859e+03                    9.913e+03  \n                 Fireplaces                  Garage_Area  \n                  7.440e+03                    3.340e+01  \n              TotRms_AbvGrd  \n                 -2.452e+03  \n\n\nIt’s good idea to perform different types of selection to get an idea of what features to include in your final model."
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/index.html#r-code-2",
    "href": "notes/analytics/linear-regression/model-selection/index.html#r-code-2",
    "title": "Model Selection",
    "section": "6.1 R Code",
    "text": "6.1 R Code\n\n\nCode\nfull.model &lt;- lm(Sale_Price ~ ., data = train_sel)\nempty.model &lt;- lm(Sale_Price ~ 1, data = train_sel)\nstep.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ),\n    direction = \"both\", k = 2\n)\n\n\nStart:  AIC=46387.11\nSale_Price ~ 1\n\n                Df  Sum of Sq        RSS   AIC\n+ Overall_Qual   9 9.6786e+12 3.9328e+12 43859\n+ Gr_Liv_Area    1 7.0185e+12 6.5929e+12 44902\n+ First_Flr_SF   1 5.9158e+12 7.6956e+12 45220\n+ Garage_Area    1 5.7015e+12 7.9100e+12 45276\n+ Full_Bath      1 4.0091e+12 9.6023e+12 45674\n+ TotRms_AbvGrd  1 3.2798e+12 1.0332e+13 45824\n+ Fireplaces     1 3.0429e+12 1.0569e+13 45870\n+ Roof_Style     5 1.1930e+12 1.2418e+13 46209\n+ Lot_Area       1 9.9240e+11 1.2619e+13 46234\n+ Central_Air    1 8.9825e+11 1.2713e+13 46249\n+ Half_Bath      1 8.6579e+11 1.2746e+13 46254\n+ House_Style    7 9.1007e+11 1.2701e+13 46259\n+ Second_Flr_SF  1 7.7180e+11 1.2840e+13 46269\n+ Bldg_Type      4 5.0175e+11 1.3110e+13 46318\n+ Street         1 2.5318e+10 1.3586e+13 46385\n&lt;none&gt;                        1.3611e+13 46387\n\nStep:  AIC=43858.69\nSale_Price ~ Overall_Qual\n\n                Df  Sum of Sq        RSS   AIC\n+ Gr_Liv_Area    1 1.1390e+12 2.7938e+12 43159\n+ First_Flr_SF   1 8.2206e+11 3.1108e+12 43380\n+ Garage_Area    1 5.0899e+11 3.4239e+12 43576\n+ TotRms_AbvGrd  1 4.6679e+11 3.4661e+12 43602\n+ Fireplaces     1 4.3575e+11 3.4971e+12 43620\n+ Lot_Area       1 4.3156e+11 3.5013e+12 43622\n+ Full_Bath      1 3.4238e+11 3.5905e+12 43674\n+ Bldg_Type      4 2.1987e+11 3.7130e+12 43749\n+ Second_Flr_SF  1 1.1069e+11 3.8222e+12 43802\n+ Central_Air    1 1.0281e+11 3.8300e+12 43806\n+ Half_Bath      1 9.6265e+10 3.8366e+12 43810\n+ Roof_Style     5 9.8243e+10 3.8346e+12 43817\n+ House_Style    7 5.3069e+10 3.8798e+12 43845\n&lt;none&gt;                        3.9328e+12 43859\n+ Street         1 6.5122e+07 3.9328e+12 43861\n- Overall_Qual   9 9.6786e+12 1.3611e+13 46387\n\nStep:  AIC=43159.35\nSale_Price ~ Overall_Qual + Gr_Liv_Area\n\n                Df  Sum of Sq        RSS   AIC\n+ House_Style    7 3.7094e+11 2.4229e+12 42881\n+ First_Flr_SF   1 3.0997e+11 2.4839e+12 42920\n+ Second_Flr_SF  1 2.7039e+11 2.5234e+12 42953\n+ Garage_Area    1 2.5273e+11 2.5411e+12 42967\n+ Lot_Area       1 1.6985e+11 2.6240e+12 43033\n+ Fireplaces     1 1.3298e+11 2.6608e+12 43061\n+ Central_Air    1 1.1378e+11 2.6800e+12 43076\n+ Bldg_Type      4 1.0111e+11 2.6927e+12 43092\n+ Roof_Style     5 8.3606e+10 2.7102e+12 43107\n+ TotRms_AbvGrd  1 4.9729e+10 2.7441e+12 43125\n+ Full_Bath      1 5.0158e+09 2.7888e+12 43158\n+ Half_Bath      1 4.2670e+09 2.7896e+12 43158\n&lt;none&gt;                        2.7938e+12 43159\n+ Street         1 3.0361e+08 2.7935e+12 43161\n- Gr_Liv_Area    1 1.1390e+12 3.9328e+12 43859\n- Overall_Qual   9 3.7991e+12 6.5929e+12 44902\n\nStep:  AIC=42881.18\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style\n\n                Df  Sum of Sq        RSS   AIC\n+ Garage_Area    1 1.2429e+11 2.2986e+12 42775\n+ Bldg_Type      4 1.1851e+11 2.3044e+12 42786\n+ Lot_Area       1 9.0059e+10 2.3328e+12 42805\n+ Fireplaces     1 7.3465e+10 2.3494e+12 42820\n+ Central_Air    1 6.4141e+10 2.3587e+12 42828\n+ Half_Bath      1 2.7168e+10 2.3957e+12 42860\n+ TotRms_AbvGrd  1 2.4413e+10 2.3985e+12 42862\n+ Roof_Style     5 3.1845e+10 2.3910e+12 42864\n+ First_Flr_SF   1 2.1082e+10 2.4018e+12 42865\n+ Second_Flr_SF  1 9.8702e+09 2.4130e+12 42875\n&lt;none&gt;                        2.4229e+12 42881\n+ Street         1 6.5162e+08 2.4222e+12 42883\n+ Full_Bath      1 3.1011e+08 2.4226e+12 42883\n- House_Style    7 3.7094e+11 2.7938e+12 43159\n- Gr_Liv_Area    1 1.4569e+12 3.8798e+12 43845\n- Overall_Qual   9 2.6563e+12 5.0792e+12 44381\n\nStep:  AIC=42775.18\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area\n\n                Df  Sum of Sq        RSS   AIC\n+ Bldg_Type      4 9.9484e+10 2.1991e+12 42692\n+ Fireplaces     1 7.7120e+10 2.2215e+12 42707\n+ Lot_Area       1 6.9201e+10 2.2294e+12 42714\n+ Central_Air    1 4.0309e+10 2.2583e+12 42741\n+ TotRms_AbvGrd  1 2.2264e+10 2.2763e+12 42757\n+ Roof_Style     5 2.9837e+10 2.2688e+12 42758\n+ Half_Bath      1 2.0578e+10 2.2780e+12 42759\n+ First_Flr_SF   1 1.2447e+10 2.2862e+12 42766\n+ Second_Flr_SF  1 5.2779e+09 2.2933e+12 42772\n&lt;none&gt;                        2.2986e+12 42775\n+ Street         1 4.8080e+07 2.2986e+12 42777\n+ Full_Bath      1 1.8678e+07 2.2986e+12 42777\n- Garage_Area    1 1.2429e+11 2.4229e+12 42881\n- House_Style    7 2.4250e+11 2.5411e+12 42967\n- Gr_Liv_Area    1 1.0810e+12 3.3796e+12 43564\n- Overall_Qual   9 1.9714e+12 4.2700e+12 44027\n\nStep:  AIC=42692.43\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type\n\n                Df  Sum of Sq        RSS   AIC\n+ Lot_Area       1 5.7797e+10 2.1413e+12 42640\n+ Fireplaces     1 5.3007e+10 2.1461e+12 42644\n+ Central_Air    1 2.6218e+10 2.1729e+12 42670\n+ Roof_Style     5 2.9830e+10 2.1693e+12 42674\n+ Half_Bath      1 1.8890e+10 2.1802e+12 42677\n+ First_Flr_SF   1 1.7475e+10 2.1816e+12 42678\n+ TotRms_AbvGrd  1 9.1215e+09 2.1900e+12 42686\n+ Second_Flr_SF  1 7.9895e+09 2.1911e+12 42687\n+ Full_Bath      1 6.1306e+09 2.1930e+12 42689\n&lt;none&gt;                        2.1991e+12 42692\n+ Street         1 3.5384e+08 2.1988e+12 42694\n- Bldg_Type      4 9.9484e+10 2.2986e+12 42775\n- Garage_Area    1 1.0526e+11 2.3044e+12 42786\n- House_Style    7 2.6840e+11 2.4675e+12 42915\n- Gr_Liv_Area    1 1.0416e+12 3.2408e+12 43486\n- Overall_Qual   9 1.6326e+12 3.8317e+12 43813\n\nStep:  AIC=42639.81\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area\n\n                Df  Sum of Sq        RSS   AIC\n+ Fireplaces     1 4.0525e+10 2.1008e+12 42603\n+ Central_Air    1 2.3631e+10 2.1177e+12 42619\n+ Roof_Style     5 2.8521e+10 2.1128e+12 42622\n+ Half_Bath      1 1.7687e+10 2.1236e+12 42625\n+ First_Flr_SF   1 1.0898e+10 2.1304e+12 42631\n+ Full_Bath      1 6.9609e+09 2.1344e+12 42635\n+ TotRms_AbvGrd  1 6.8886e+09 2.1344e+12 42635\n+ Second_Flr_SF  1 4.0098e+09 2.1373e+12 42638\n&lt;none&gt;                        2.1413e+12 42640\n+ Street         1 1.1335e+09 2.1402e+12 42641\n- Lot_Area       1 5.7797e+10 2.1991e+12 42692\n- Bldg_Type      4 8.8079e+10 2.2294e+12 42714\n- Garage_Area    1 9.0624e+10 2.2319e+12 42723\n- House_Style    7 2.2397e+11 2.3653e+12 42830\n- Gr_Liv_Area    1 8.8831e+11 3.0296e+12 43350\n- Overall_Qual   9 1.6705e+12 3.8118e+12 43805\n\nStep:  AIC=42602.62\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces\n\n                Df  Sum of Sq        RSS   AIC\n+ Central_Air    1 2.0982e+10 2.0798e+12 42584\n+ Roof_Style     5 2.5934e+10 2.0749e+12 42587\n+ Half_Bath      1 1.3786e+10 2.0870e+12 42591\n+ Full_Bath      1 1.2634e+10 2.0882e+12 42592\n+ First_Flr_SF   1 7.9246e+09 2.0929e+12 42597\n+ TotRms_AbvGrd  1 6.0760e+09 2.0947e+12 42599\n+ Second_Flr_SF  1 2.4859e+09 2.0983e+12 42602\n&lt;none&gt;                        2.1008e+12 42603\n+ Street         1 1.3531e+09 2.0994e+12 42603\n- Fireplaces     1 4.0525e+10 2.1413e+12 42640\n- Lot_Area       1 4.5315e+10 2.1461e+12 42644\n- Bldg_Type      4 6.8112e+10 2.1689e+12 42660\n- Garage_Area    1 9.5457e+10 2.1963e+12 42692\n- House_Style    7 1.9057e+11 2.2914e+12 42767\n- Gr_Liv_Area    1 6.8568e+11 2.7865e+12 43180\n- Overall_Qual   9 1.6377e+12 3.7385e+12 43767\n\nStep:  AIC=42584.03\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air\n\n                Df  Sum of Sq        RSS   AIC\n+ Roof_Style     5 2.3879e+10 2.0559e+12 42570\n+ Full_Bath      1 1.2884e+10 2.0669e+12 42573\n+ Half_Bath      1 1.0579e+10 2.0692e+12 42576\n+ First_Flr_SF   1 8.5564e+09 2.0713e+12 42578\n+ TotRms_AbvGrd  1 6.7639e+09 2.0731e+12 42579\n+ Second_Flr_SF  1 2.7821e+09 2.0770e+12 42583\n&lt;none&gt;                        2.0798e+12 42584\n+ Street         1 1.5371e+09 2.0783e+12 42585\n- Central_Air    1 2.0982e+10 2.1008e+12 42603\n- Fireplaces     1 3.7876e+10 2.1177e+12 42619\n- Lot_Area       1 4.3536e+10 2.1234e+12 42625\n- Bldg_Type      4 5.6366e+10 2.1362e+12 42631\n- Garage_Area    1 8.0082e+10 2.1599e+12 42660\n- House_Style    7 1.7426e+11 2.2541e+12 42735\n- Gr_Liv_Area    1 6.7616e+11 2.7560e+12 43159\n- Overall_Qual   9 1.6464e+12 3.7262e+12 43762\n\nStep:  AIC=42570.35\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style\n\n                Df  Sum of Sq        RSS   AIC\n+ Half_Bath      1 1.2657e+10 2.0433e+12 42560\n+ Full_Bath      1 1.1807e+10 2.0441e+12 42561\n+ First_Flr_SF   1 8.2634e+09 2.0477e+12 42564\n+ TotRms_AbvGrd  1 7.5653e+09 2.0484e+12 42565\n+ Second_Flr_SF  1 2.5689e+09 2.0534e+12 42570\n&lt;none&gt;                        2.0559e+12 42570\n+ Street         1 1.5834e+09 2.0544e+12 42571\n- Roof_Style     5 2.3879e+10 2.0798e+12 42584\n- Central_Air    1 1.8927e+10 2.0749e+12 42587\n- Fireplaces     1 3.5505e+10 2.0914e+12 42603\n- Lot_Area       1 4.3312e+10 2.0992e+12 42611\n- Bldg_Type      4 5.7137e+10 2.1131e+12 42619\n- Garage_Area    1 7.7724e+10 2.1337e+12 42644\n- House_Style    7 1.5430e+11 2.2102e+12 42705\n- Gr_Liv_Area    1 6.5674e+11 2.7127e+12 43137\n- Overall_Qual   9 1.5782e+12 3.6341e+12 43721\n\nStep:  AIC=42559.68\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style + \n    Half_Bath\n\n                Df  Sum of Sq        RSS   AIC\n+ Full_Bath      1 2.2460e+10 2.0208e+12 42539\n+ First_Flr_SF   1 1.1168e+10 2.0321e+12 42550\n+ TotRms_AbvGrd  1 6.6957e+09 2.0366e+12 42555\n+ Second_Flr_SF  1 4.4132e+09 2.0389e+12 42557\n&lt;none&gt;                        2.0433e+12 42560\n+ Street         1 1.6758e+09 2.0416e+12 42560\n- Half_Bath      1 1.2657e+10 2.0559e+12 42570\n- Central_Air    1 1.5721e+10 2.0590e+12 42573\n- Roof_Style     5 2.5957e+10 2.0692e+12 42576\n- Fireplaces     1 3.1983e+10 2.0753e+12 42590\n- Lot_Area       1 4.2973e+10 2.0863e+12 42600\n- Bldg_Type      4 5.7342e+10 2.1006e+12 42608\n- Garage_Area    1 7.4966e+10 2.1182e+12 42632\n- House_Style    7 1.6681e+11 2.2101e+12 42707\n- Gr_Liv_Area    1 6.4712e+11 2.6904e+12 43122\n- Overall_Qual   9 1.5761e+12 3.6194e+12 43714\n\nStep:  AIC=42539.01\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style + \n    Half_Bath + Full_Bath\n\n                Df  Sum of Sq        RSS   AIC\n+ First_Flr_SF   1 1.3554e+10 2.0073e+12 42527\n+ TotRms_AbvGrd  1 7.9202e+09 2.0129e+12 42533\n+ Second_Flr_SF  1 6.1295e+09 2.0147e+12 42535\n&lt;none&gt;                        2.0208e+12 42539\n+ Street         1 1.5161e+09 2.0193e+12 42539\n- Central_Air    1 1.5081e+10 2.0359e+12 42552\n- Roof_Style     5 2.5536e+10 2.0464e+12 42555\n- Full_Bath      1 2.2460e+10 2.0433e+12 42560\n- Half_Bath      1 2.3310e+10 2.0441e+12 42561\n- Fireplaces     1 3.8140e+10 2.0590e+12 42575\n- Lot_Area       1 4.2795e+10 2.0636e+12 42580\n- Bldg_Type      4 7.0416e+10 2.0912e+12 42601\n- Garage_Area    1 6.9447e+10 2.0903e+12 42606\n- House_Style    7 1.7045e+11 2.1913e+12 42691\n- Gr_Liv_Area    1 4.3760e+11 2.4584e+12 42939\n- Overall_Qual   9 1.5073e+12 3.5281e+12 43664\n\nStep:  AIC=42527.21\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style + \n    Half_Bath + Full_Bath + First_Flr_SF\n\n                Df  Sum of Sq        RSS   AIC\n+ TotRms_AbvGrd  1 9.2160e+09 1.9980e+12 42520\n+ Second_Flr_SF  1 8.9845e+09 1.9983e+12 42520\n&lt;none&gt;                        2.0073e+12 42527\n+ Street         1 1.4992e+09 2.0058e+12 42528\n- First_Flr_SF   1 1.3554e+10 2.0208e+12 42539\n- Central_Air    1 1.5279e+10 2.0225e+12 42541\n- Roof_Style     5 2.5274e+10 2.0325e+12 42543\n- Full_Bath      1 2.4846e+10 2.0321e+12 42550\n- Half_Bath      1 2.8024e+10 2.0353e+12 42554\n- Fireplaces     1 3.4393e+10 2.0417e+12 42560\n- Lot_Area       1 3.7100e+10 2.0444e+12 42563\n- House_Style    7 5.1979e+10 2.0592e+12 42566\n- Garage_Area    1 6.1955e+10 2.0692e+12 42588\n- Bldg_Type      4 7.5956e+10 2.0832e+12 42595\n- Gr_Liv_Area    1 9.7174e+10 2.1044e+12 42622\n- Overall_Qual   9 1.4420e+12 3.4493e+12 43620\n\nStep:  AIC=42519.77\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style + \n    Half_Bath + Full_Bath + First_Flr_SF + TotRms_AbvGrd\n\n                Df  Sum of Sq        RSS   AIC\n+ Second_Flr_SF  1 8.4078e+09 1.9896e+12 42513\n&lt;none&gt;                        1.9980e+12 42520\n+ Street         1 1.6201e+09 1.9964e+12 42520\n- TotRms_AbvGrd  1 9.2160e+09 2.0073e+12 42527\n- First_Flr_SF   1 1.4850e+10 2.0129e+12 42533\n- Central_Air    1 1.6103e+10 2.0142e+12 42534\n- Roof_Style     5 2.6015e+10 2.0241e+12 42536\n- Full_Bath      1 2.6354e+10 2.0244e+12 42545\n- Half_Bath      1 2.7318e+10 2.0254e+12 42546\n- Fireplaces     1 3.3635e+10 2.0317e+12 42552\n- House_Style    7 4.6648e+10 2.0447e+12 42553\n- Lot_Area       1 3.5234e+10 2.0333e+12 42554\n- Garage_Area    1 6.0006e+10 2.0581e+12 42578\n- Bldg_Type      4 6.6554e+10 2.0646e+12 42579\n- Gr_Liv_Area    1 1.0626e+11 2.1043e+12 42624\n- Overall_Qual   9 1.4449e+12 3.4430e+12 43618\n\nStep:  AIC=42513.12\nSale_Price ~ Overall_Qual + Gr_Liv_Area + House_Style + Garage_Area + \n    Bldg_Type + Lot_Area + Fireplaces + Central_Air + Roof_Style + \n    Half_Bath + Full_Bath + First_Flr_SF + TotRms_AbvGrd + Second_Flr_SF\n\n                Df  Sum of Sq        RSS   AIC\n- Gr_Liv_Area    1 2.0878e+08 1.9898e+12 42511\n&lt;none&gt;                        1.9896e+12 42513\n+ Street         1 1.6751e+09 1.9880e+12 42513\n- Second_Flr_SF  1 8.4078e+09 1.9980e+12 42520\n- TotRms_AbvGrd  1 8.6394e+09 1.9983e+12 42520\n- First_Flr_SF   1 1.6139e+10 2.0058e+12 42528\n- Central_Air    1 1.6361e+10 2.0060e+12 42528\n- Roof_Style     5 2.6070e+10 2.0157e+12 42530\n- Full_Bath      1 2.5226e+10 2.0149e+12 42537\n- Half_Bath      1 2.5401e+10 2.0150e+12 42537\n- House_Style    7 4.2545e+10 2.0322e+12 42543\n- Fireplaces     1 3.3076e+10 2.0227e+12 42545\n- Lot_Area       1 3.5199e+10 2.0248e+12 42547\n- Garage_Area    1 5.8031e+10 2.0477e+12 42570\n- Bldg_Type      4 6.7352e+10 2.0570e+12 42573\n- Overall_Qual   9 1.4395e+12 3.4292e+12 43612\n\nStep:  AIC=42511.34\nSale_Price ~ Overall_Qual + House_Style + Garage_Area + Bldg_Type + \n    Lot_Area + Fireplaces + Central_Air + Roof_Style + Half_Bath + \n    Full_Bath + First_Flr_SF + TotRms_AbvGrd + Second_Flr_SF\n\n                Df  Sum of Sq        RSS   AIC\n&lt;none&gt;                        1.9898e+12 42511\n+ Street         1 1.6987e+09 1.9882e+12 42512\n+ Gr_Liv_Area    1 2.0878e+08 1.9896e+12 42513\n- TotRms_AbvGrd  1 8.4349e+09 1.9983e+12 42518\n- Central_Air    1 1.6410e+10 2.0063e+12 42526\n- Roof_Style     5 2.6087e+10 2.0159e+12 42528\n- Half_Bath      1 2.5433e+10 2.0153e+12 42535\n- Full_Bath      1 2.5541e+10 2.0154e+12 42535\n- House_Style    7 4.2750e+10 2.0326e+12 42541\n- Fireplaces     1 3.3235e+10 2.0231e+12 42543\n- Lot_Area       1 3.5216e+10 2.0251e+12 42545\n- Garage_Area    1 5.7881e+10 2.0477e+12 42568\n- Bldg_Type      4 6.7805e+10 2.0577e+12 42572\n- Second_Flr_SF  1 1.1446e+11 2.1043e+12 42624\n- First_Flr_SF   1 3.5400e+11 2.3438e+12 42845\n- Overall_Qual   9 1.4397e+12 3.4296e+12 43610"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/lab_11.html",
    "href": "notes/analytics/linear-regression/regularized/lab_11.html",
    "title": "Lab 11",
    "section": "",
    "text": "Code\nlibrary(AppliedPredictiveModeling)\nlibrary(glmnet)\n\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ndata(FuelEconomy)\nstr(cars2010)\n\n\n'data.frame':   1107 obs. of  14 variables:\n $ EngDispl           : num  4.7 4.7 4.2 4.2 5.2 5.2 2 6 3 3 ...\n $ NumCyl             : int  8 8 8 8 10 10 4 12 6 6 ...\n $ Transmission       : Factor w/ 16 levels \"Other\",\"A4\",\"A5\",..: 6 11 11 6 6 11 14 14 14 11 ...\n $ FE                 : num  28 25.6 26.8 25 24.8 ...\n $ AirAspirationMethod: Factor w/ 3 levels \"NaturallyAspirated\",..: 1 1 1 1 1 1 3 3 1 1 ...\n $ NumGears           : int  6 6 6 6 6 6 6 6 6 6 ...\n $ TransLockup        : num  1 1 1 1 0 0 0 0 1 0 ...\n $ TransCreeperGear   : num  0 0 0 0 0 0 0 0 0 0 ...\n $ DriveDesc          : Factor w/ 5 levels \"AllWheelDrive\",..: 5 5 1 1 1 1 1 1 5 5 ...\n $ IntakeValvePerCyl  : int  2 2 2 2 2 2 2 2 2 2 ...\n $ ExhaustValvesPerCyl: int  2 2 2 2 2 2 2 2 2 2 ...\n $ CarlineClassDesc   : Factor w/ 17 levels \"Other\",\"2Seaters\",..: 2 2 2 2 2 2 2 2 2 2 ...\n $ VarValveTiming     : num  1 1 1 1 1 1 1 1 1 1 ...\n $ VarValveLift       : num  0 0 0 0 0 0 0 0 1 1 ...\n\n\n\n\n\n\nCode\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(across(!c(EngDispl, FE), as.factor))\n\nx &lt;- model.matrix(FE ~ ., data = cars2010)[, -1]\ny &lt;- cars2010$FE\ncars2010_lasso &lt;- glmnet(x = x, y = y, data = cars2010, alpha = 1)\nplot(cars2010_lasso, xvar = \"lambda\")\n\n\n\n\n\n\n\n\n\n\nCode\ncars2010_lasso_cv &lt;- cv.glmnet(x = x, y = y, data = cars2010, alpha = 1)\nsprintf(\"Lambda Min: %f\", cars2010_lasso_cv$lambda.min)\n\n\n[1] \"Lambda Min: 0.003149\"\n\n\nCode\nsprintf(\"Lambda 1se: %f\", cars2010_lasso_cv$lambda.1se)\n\n\n[1] \"Lambda 1se: 0.081725\"\n\n\nCode\ncoef(cars2010_lasso, s = c(cars2010_lasso_cv$lambda.min, cars2010_lasso_cv$lambda.1se))\n\n\n61 x 2 sparse Matrix of class \"dgCMatrix\"\n                                                        s1           s2\n(Intercept)                                     33.4645943 41.426052394\nEngDispl                                        -2.2484039 -2.612376600\nNumCyl3                                         19.8854245 16.331381759\nNumCyl4                                          5.2160082  2.868003410\nNumCyl5                                          2.3331041  .          \nNumCyl6                                          1.6502816  .          \nNumCyl8                                          .         -0.469191710\nNumCyl10                                        -2.4726369 -2.125125452\nNumCyl12                                        -3.6178672 -3.300995112\nNumCyl16                                        -2.6276443  .          \nTransmissionA4                                  -0.1003125  .          \nTransmissionA5                                   .         -0.044325848\nTransmissionA6                                   .          0.905142859\nTransmissionA7                                   4.2967759  1.208841875\nTransmissionAM6                                 -4.5107268 -2.336212099\nTransmissionAM7                                 -0.6218908 -2.016822319\nTransmissionAV                                  -0.2210950  1.944530026\nTransmissionAVS6                                -2.5179098  .          \nTransmissionM5                                  -0.4113433  .          \nTransmissionM6                                  -2.0986945  .          \nTransmissionS4                                  -3.5686068 -2.648852091\nTransmissionS5                                  -0.6307208 -0.396420900\nTransmissionS6                                  -0.1950499  0.805590210\nTransmissionS7                                   2.6604006  .          \nTransmissionS8                                   .          0.701385206\nAirAspirationMethodSupercharged                 -1.6260213 -0.639479111\nAirAspirationMethodTurbocharged                 -1.0991194 -0.554921596\nNumGears4                                       -2.6785588 -0.457944419\nNumGears5                                       -2.3854979 -0.008066124\nNumGears6                                       -0.7563294  .          \nNumGears7                                       -5.1972241  .          \nNumGears8                                        1.3172267  1.486295656\nTransLockup1                                    -0.8631401 -0.462933334\nTransCreeperGear1                               -1.0187792 -0.379864714\nDriveDescFourWheelDrive                         -0.4422495 -0.226950598\nDriveDescParttimeFourWheelDrive                 -0.2729337  .          \nDriveDescTwoWheelDriveFront                      4.3185833  4.389609017\nDriveDescTwoWheelDriveRear                       1.1850298  1.040633387\nIntakeValvePerCyl1                               7.1818246  1.768522064\nIntakeValvePerCyl2                               5.7431452  .          \nIntakeValvePerCyl3                               0.8339699 -1.443732357\nExhaustValvesPerCyl1                             1.6971178  0.361322872\nExhaustValvesPerCyl2                             0.1346006  .          \nCarlineClassDesc2Seaters                         2.6259772  0.026404931\nCarlineClassDescCompactCars                      3.6497525  1.661753262\nCarlineClassDescLargeCars                        2.3903097  0.184839120\nCarlineClassDescMidsizeCars                      3.2554331  1.290408355\nCarlineClassDescMinicompactCars                  3.4442945  1.076833209\nCarlineClassDescSmallPickupTrucks2WD            -1.9683362 -2.874215693\nCarlineClassDescSmallPickupTrucks4WD            -1.0853059 -2.189519097\nCarlineClassDescSmallStationWagons               2.0295958  .          \nCarlineClassDescSpecialPurposeVehicleminivan2WD -2.1862947 -2.821841435\nCarlineClassDescSpecialPurposeVehicleSUV2WD     -1.6632754 -2.858148646\nCarlineClassDescSpecialPurposeVehicleSUV4WD     -0.7290329 -2.323183753\nCarlineClassDescStandardPickupTrucks2WD         -1.8792024 -2.498829779\nCarlineClassDescStandardPickupTrucks4WD         -2.0980581 -3.091291967\nCarlineClassDescSubcompactCars                   3.2221469  0.808901028\nCarlineClassDescVansCargoTypes                  -4.2187289 -4.643555906\nCarlineClassDescVansPassengerType               -4.4107156 -4.522207504\nVarValveTiming1                                  0.1854179  0.138945127\nVarValveLift1                                    0.8714607  0.953636281"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/lab_11.html#a",
    "href": "notes/analytics/linear-regression/regularized/lab_11.html#a",
    "title": "Lab 11",
    "section": "",
    "text": "Code\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(across(!c(EngDispl, FE), as.factor))\n\nx &lt;- model.matrix(FE ~ ., data = cars2010)[, -1]\ny &lt;- cars2010$FE\ncars2010_lasso &lt;- glmnet(x = x, y = y, data = cars2010, alpha = 1)\nplot(cars2010_lasso, xvar = \"lambda\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/lab_11.html#b",
    "href": "notes/analytics/linear-regression/regularized/lab_11.html#b",
    "title": "Lab 11",
    "section": "",
    "text": "Code\ncars2010_lasso_cv &lt;- cv.glmnet(x = x, y = y, data = cars2010, alpha = 1)\nsprintf(\"Lambda Min: %f\", cars2010_lasso_cv$lambda.min)\n\n\n[1] \"Lambda Min: 0.003149\"\n\n\nCode\nsprintf(\"Lambda 1se: %f\", cars2010_lasso_cv$lambda.1se)\n\n\n[1] \"Lambda 1se: 0.081725\"\n\n\nCode\ncoef(cars2010_lasso, s = c(cars2010_lasso_cv$lambda.min, cars2010_lasso_cv$lambda.1se))\n\n\n61 x 2 sparse Matrix of class \"dgCMatrix\"\n                                                        s1           s2\n(Intercept)                                     33.4645943 41.426052394\nEngDispl                                        -2.2484039 -2.612376600\nNumCyl3                                         19.8854245 16.331381759\nNumCyl4                                          5.2160082  2.868003410\nNumCyl5                                          2.3331041  .          \nNumCyl6                                          1.6502816  .          \nNumCyl8                                          .         -0.469191710\nNumCyl10                                        -2.4726369 -2.125125452\nNumCyl12                                        -3.6178672 -3.300995112\nNumCyl16                                        -2.6276443  .          \nTransmissionA4                                  -0.1003125  .          \nTransmissionA5                                   .         -0.044325848\nTransmissionA6                                   .          0.905142859\nTransmissionA7                                   4.2967759  1.208841875\nTransmissionAM6                                 -4.5107268 -2.336212099\nTransmissionAM7                                 -0.6218908 -2.016822319\nTransmissionAV                                  -0.2210950  1.944530026\nTransmissionAVS6                                -2.5179098  .          \nTransmissionM5                                  -0.4113433  .          \nTransmissionM6                                  -2.0986945  .          \nTransmissionS4                                  -3.5686068 -2.648852091\nTransmissionS5                                  -0.6307208 -0.396420900\nTransmissionS6                                  -0.1950499  0.805590210\nTransmissionS7                                   2.6604006  .          \nTransmissionS8                                   .          0.701385206\nAirAspirationMethodSupercharged                 -1.6260213 -0.639479111\nAirAspirationMethodTurbocharged                 -1.0991194 -0.554921596\nNumGears4                                       -2.6785588 -0.457944419\nNumGears5                                       -2.3854979 -0.008066124\nNumGears6                                       -0.7563294  .          \nNumGears7                                       -5.1972241  .          \nNumGears8                                        1.3172267  1.486295656\nTransLockup1                                    -0.8631401 -0.462933334\nTransCreeperGear1                               -1.0187792 -0.379864714\nDriveDescFourWheelDrive                         -0.4422495 -0.226950598\nDriveDescParttimeFourWheelDrive                 -0.2729337  .          \nDriveDescTwoWheelDriveFront                      4.3185833  4.389609017\nDriveDescTwoWheelDriveRear                       1.1850298  1.040633387\nIntakeValvePerCyl1                               7.1818246  1.768522064\nIntakeValvePerCyl2                               5.7431452  .          \nIntakeValvePerCyl3                               0.8339699 -1.443732357\nExhaustValvesPerCyl1                             1.6971178  0.361322872\nExhaustValvesPerCyl2                             0.1346006  .          \nCarlineClassDesc2Seaters                         2.6259772  0.026404931\nCarlineClassDescCompactCars                      3.6497525  1.661753262\nCarlineClassDescLargeCars                        2.3903097  0.184839120\nCarlineClassDescMidsizeCars                      3.2554331  1.290408355\nCarlineClassDescMinicompactCars                  3.4442945  1.076833209\nCarlineClassDescSmallPickupTrucks2WD            -1.9683362 -2.874215693\nCarlineClassDescSmallPickupTrucks4WD            -1.0853059 -2.189519097\nCarlineClassDescSmallStationWagons               2.0295958  .          \nCarlineClassDescSpecialPurposeVehicleminivan2WD -2.1862947 -2.821841435\nCarlineClassDescSpecialPurposeVehicleSUV2WD     -1.6632754 -2.858148646\nCarlineClassDescSpecialPurposeVehicleSUV4WD     -0.7290329 -2.323183753\nCarlineClassDescStandardPickupTrucks2WD         -1.8792024 -2.498829779\nCarlineClassDescStandardPickupTrucks4WD         -2.0980581 -3.091291967\nCarlineClassDescSubcompactCars                   3.2221469  0.808901028\nCarlineClassDescVansCargoTypes                  -4.2187289 -4.643555906\nCarlineClassDescVansPassengerType               -4.4107156 -4.522207504\nVarValveTiming1                                  0.1854179  0.138945127\nVarValveLift1                                    0.8714607  0.953636281"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/lab_4.html",
    "href": "notes/analytics/linear-regression/SLR/lab_4.html",
    "title": "1 1",
    "section": "",
    "text": "1 1\n\n\nCode\nlibrary(AppliedPredictiveModeling)\ndata(FuelEconomy)\n\n\n\n\nCode\ncor(cars2010[, c(\"EngDispl\", \"NumCyl\", \"ExhaustValvesPerCyl\", \"VarValveTiming\", \"FE\")])\n\n\n                       EngDispl       NumCyl ExhaustValvesPerCyl VarValveTiming\nEngDispl             1.00000000  0.906260027          -0.4784380   -0.068256030\nNumCyl               0.90626003  1.000000000          -0.3398518    0.005399291\nExhaustValvesPerCyl -0.47843804 -0.339851831           1.0000000    0.279339052\nVarValveTiming      -0.06825603  0.005399291           0.2793391    1.000000000\nFE                  -0.78739383 -0.740217981           0.3356529    0.124952779\n                            FE\nEngDispl            -0.7873938\nNumCyl              -0.7402180\nExhaustValvesPerCyl  0.3356529\nVarValveTiming       0.1249528\nFE                   1.0000000\n\n\nCode\npairs(cars2010[, c(\"EngDispl\", \"NumCyl\", \"ExhaustValvesPerCyl\", \"VarValveTiming\", \"FE\")])\n\n\n\n\n\nCode\nengdispl_lm &lt;- lm(FE ~ EngDispl, data = cars2010)\npar(nfrow = c(2, 2))\n\n\nWarning in par(nfrow = c(2, 2)): \"nfrow\" is not a graphical parameter\n\n\nCode\nplot(engdispl_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nsummary(engdispl_lm)\n\n\n\nCall:\nlm(formula = FE ~ EngDispl, data = cars2010)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-14.486  -3.192  -0.365   2.671  27.215 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  50.5632     0.3985  126.89   &lt;2e-16 ***\nEngDispl     -4.5209     0.1065  -42.46   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.624 on 1105 degrees of freedom\nMultiple R-squared:   0.62, Adjusted R-squared:  0.6196 \nF-statistic:  1803 on 1 and 1105 DF,  p-value: &lt; 2.2e-16\n\n\n\nHighest correlation with FE is EnglDispl\n\n\n\nCode\ncor.test(cars2010$FE, cars2010$EngDispl)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  cars2010$FE and cars2010$EngDispl\nt = -42.46, df = 1105, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.8087913 -0.7639144\nsample estimates:\n       cor \n-0.7873938 \n\n\n\nP-value is significant at a 0.05 level and we have evidence that the correlation coefficient is not equal to 0\n\n\n\nCode\ncor(cars2010[, c(\"EngDispl\", \"NumCyl\", \"ExhaustValvesPerCyl\", \"VarValveTiming\")])\n\n\n                       EngDispl       NumCyl ExhaustValvesPerCyl VarValveTiming\nEngDispl             1.00000000  0.906260027          -0.4784380   -0.068256030\nNumCyl               0.90626003  1.000000000          -0.3398518    0.005399291\nExhaustValvesPerCyl -0.47843804 -0.339851831           1.0000000    0.279339052\nVarValveTiming      -0.06825603  0.005399291           0.2793391    1.000000000\n\n\n\nEnglDispl and NumCyl have a large correlation between one another\n\n\n\nCode\nslr &lt;- lm(FE ~ EngDispl, cars2010)\nsummary(slr)$r.squared\n\n\n[1] 0.619989\n\n\n\nF-statistic is 1803 with a p-value of 2.2e-16. Overall model is significant\n\\(y = 50.5632 - 4.5209x_1\\)\n\\(R^2\\) is 0.620 which means that 62% of the variability in FE can be explained by EngDispl alone\n\n\n\n2 2\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(ggplot2)\nicecream &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/icecream.csv\", sep = \" \")\nglimpse(icecream)\n\n\nRows: 50\nColumns: 2\n$ Temperature &lt;int&gt; 65, 87, 78, 68, 98, 86, 62, 86, 71, 85, 78, 90, 63, 80, 80…\n$ Sales       &lt;dbl&gt; 180.25, 218.75, 202.44, 176.50, 212.18, 210.68, 165.30, 22…\n\n\n\n\nCode\nsales_slr &lt;- lm(Sales ~ Temperature, icecream)\npar(mfrow = c(2, 2))\nplot(sales_slr)\n\n\n\n\n\nCode\nggplot(icecream, aes(x = Temperature, y = Sales)) +\n    geom_point() +\n    stat_smooth(method = \"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nsummary(sales_slr)\n\n\n\nCall:\nlm(formula = Sales ~ Temperature, data = icecream)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.603  -8.159   1.005   7.212  23.666 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 119.3895    10.1931  11.713 1.12e-15 ***\nTemperature   1.0889     0.1241   8.771 1.54e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.07 on 48 degrees of freedom\nMultiple R-squared:  0.6158,    Adjusted R-squared:  0.6078 \nF-statistic: 76.94 on 1 and 48 DF,  p-value: 1.543e-11\n\n\n\nResiduals seem to be Normally distributed from the QQ-Plot\nError variance is constant and does not show a pattern\nLinearity of the mean\nOverall F-test is significant so Temperature seems to be significant in explaining Sales\nParameter estimate for Temperature is 1.0889\n\n\n\n3 3\n\n\nCode\nminntemp &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/minntemp.csv\", sep = \" \")\nglimpse(minntemp)\n\n\nRows: 795\nColumns: 3\n$ Temp   &lt;dbl&gt; 73.04, 73.04, 73.04, 69.98, 69.08, 69.98, 69.98, 71.96, 75.02, …\n$ TimeSq &lt;int&gt; 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, …\n$ Time   &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, …\n\n\n\n\nCode\ntemp_slr &lt;- lm(Temp ~ Time, minntemp)\npar(mfrow = c(2, 2))\nplot(temp_slr)\n\n\n\n\n\nCode\nggplot(minntemp, aes(x = Time, y = Temp)) +\n    geom_point() +\n    stat_smooth(method = \"lm\")\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nCode\nsummary(temp_slr)\n\n\n\nCall:\nlm(formula = Temp ~ Time, data = minntemp)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-20.8189  -5.4495  -0.5359   5.1432  21.1455 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 70.755366   0.588230 120.285   &lt;2e-16 ***\nTime         0.002301   0.001280   1.797   0.0727 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.285 on 793 degrees of freedom\nMultiple R-squared:  0.004056,  Adjusted R-squared:  0.0028 \nF-statistic: 3.229 on 1 and 793 DF,  p-value: 0.07271\n\n\n\nNo linearity of the mean, seems quadratic\nNo constant variance\nNormally distributed errors\nNo statistical evidence that time is related to temperature at a confidence level of 0.05"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html",
    "href": "notes/analytics/linear-regression/SLR/index.html",
    "title": "Ordinary Least Squares Regression",
    "section": "",
    "text": "Code\nlibrary(reticulate)\nuse_condaenv(\"msa\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#hypothesis-test-for-correlation",
    "href": "notes/analytics/linear-regression/SLR/index.html#hypothesis-test-for-correlation",
    "title": "Ordinary Least Squares Regression",
    "section": "1.1 Hypothesis Test for Correlation",
    "text": "1.1 Hypothesis Test for Correlation\nParameter representing population correlation is \\(\\rho\\) and is estimated by \\(r\\)\n\n\\(H_0: \\rho = 0\\)\n\\(H_a: \\rho \\neq 0\\)\n\nHowever, rejecting \\(H_0\\) only means that \\(\\rho\\) is not exactly 0 so we need to see if the relationship is practically significant.\nNote that outliers affect correlation and correlation does not imply causation."
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#r-code",
    "href": "notes/analytics/linear-regression/SLR/index.html#r-code",
    "title": "Ordinary Least Squares Regression",
    "section": "1.2 R Code",
    "text": "1.2 R Code\n\n\nCode\nlibrary(tidyverse)\nlibrary(AmesHousing)\n\nset.seed(123)\n\names &lt;- make_ordinal_ames()\names &lt;- ames |&gt; mutate(id = row_number())\ntrain &lt;- ames |&gt; sample_frac(0.7)\ntest &lt;- anti_join(ames, train, by = \"id\")\n\ndim(train)\n\n\n[1] 2051   82\n\n\nCode\ndim(test)\n\n\n[1] 879  82\n\n\n\n\nCode\ncor.test(train$Gr_Liv_Area, train$Sale_Price)\n\n\n\n    Pearson's product-moment correlation\n\ndata:  train$Gr_Liv_Area and train$Sale_Price\nt = 44.185, df = 2049, p-value &lt; 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6756538 0.7200229\nsample estimates:\n     cor \n0.698509 \n\n\n\n\nCode\ncor(train[, c(\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\")])\n\n\n              Year_Built Total_Bsmt_SF First_Flr_SF Gr_Liv_Area Sale_Price\nYear_Built     1.0000000     0.4037104    0.3095407   0.2454325  0.5668889\nTotal_Bsmt_SF  0.4037104     1.0000000    0.8120419   0.4643838  0.6276502\nFirst_Flr_SF   0.3095407     0.8120419    1.0000000   0.5707205  0.6085229\nGr_Liv_Area    0.2454325     0.4643838    0.5707205   1.0000000  0.6985090\nSale_Price     0.5668889     0.6276502    0.6085229   0.6985090  1.0000000\n\n\nWe can also generate a plot matrix of the variable associations with pairs:\n\n\nCode\npairs(train[, c(\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\")])"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#python-code",
    "href": "notes/analytics/linear-regression/SLR/index.html#python-code",
    "title": "Ordinary Least Squares Regression",
    "section": "1.3 Python Code",
    "text": "1.3 Python Code\n\n\nCode\nimport pandas as pd\nimport numpy as np\n\ntrain = r.train\n\nnp.corrcoef(train[\"Gr_Liv_Area\"], train[\"Sale_Price\"])\n\n\narray([[1.        , 0.69850904],\n       [0.69850904, 1.        ]])\n\n\nCode\nnp.corrcoef(\n    train[[\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\"]],\n    rowvar=False,\n)\n\n\narray([[1.        , 0.40371038, 0.3095407 , 0.24543253, 0.56688895],\n       [0.40371038, 1.        , 0.81204187, 0.46438378, 0.62765021],\n       [0.3095407 , 0.81204187, 1.        , 0.57072054, 0.60852293],\n       [0.24543253, 0.46438378, 0.57072054, 1.        , 0.69850904],\n       [0.56688895, 0.62765021, 0.60852293, 0.69850904, 1.        ]])"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#correlation-does-not-imply-causation",
    "href": "notes/analytics/linear-regression/SLR/index.html#correlation-does-not-imply-causation",
    "title": "Ordinary Least Squares Regression",
    "section": "1.4 Correlation Does NOT Imply Causation",
    "text": "1.4 Correlation Does NOT Imply Causation\nA strong correlation does not mean that a change in one variable causes a change in the other. Correlations can be misleading if both variables are affected by other variables."
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#explained-vs.-unexplained-variability",
    "href": "notes/analytics/linear-regression/SLR/index.html#explained-vs.-unexplained-variability",
    "title": "Ordinary Least Squares Regression",
    "section": "2.1 Explained vs. Unexplained Variability",
    "text": "2.1 Explained vs. Unexplained Variability\nWe are trying to explain variation in the response variable. We can’t explain all of it due to random, uncontrollable error but we can model it.\n\n\n\nVariability Explained in SLR\n\n\nWith linear regression, we are trying to minimize a loss function called sum of squared errors. This is essentially measuring the difference between our predictions and the actual response values we observed in the data.\nWe square the differences so they don’t cancel each other out and we have a loss that we can optimize our model on.\n\\[\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) ^2\n\\]\nSSE makes up the amount of unexplained variability in our model."
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#the-baseline-model",
    "href": "notes/analytics/linear-regression/SLR/index.html#the-baseline-model",
    "title": "Ordinary Least Squares Regression",
    "section": "2.2 The Baseline Model",
    "text": "2.2 The Baseline Model\n\n\\(H_0: \\beta_1 = 0\\)\n\\(H_a: \\beta_1 \\neq 0\\)\n\nFor SLR, the global F-Test, parameter t-test and the test of Pearson’s correlation are all equivalent.\nWhen we can’t reject the null hypothesis we are saying that the independent variable doesn’t explain any of the variability in the response."
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/index.html#assumptions-of-simple-linear-regression",
    "href": "notes/analytics/linear-regression/SLR/index.html#assumptions-of-simple-linear-regression",
    "title": "Ordinary Least Squares Regression",
    "section": "2.3 Assumptions of Simple Linear Regression",
    "text": "2.3 Assumptions of Simple Linear Regression\n\nLinearity of the mean\n\nAs I change values in the independent variable, the line should go through the different means of the response linearly\nIf violated, misspecified model\n\nErrors are normally distributed\n\nIf violated, our test results are erroneous\n\nErrors have equal variance (homoskedasticity)\n\nIf violated, standard errors are compromised\n\nErrors are independent\n\nIf violated, standard errors are compromised\n\n\nThere is also an assumption of no perfect collinearity. Under multicollinearity, we can’t believe in our parameter estimates. The parameter estimates would be biased as there are multiple variables supplying the same information.\n\n2.3.1 Testing of Assumptions\n\nNormality can be verified using a histogram, QQ-Plot or normality test\nEqual variances can be verified through residuals versus predicted values\nIndependence can look at residual plots for potential autocorrelation\nLinearity in the mean can be tested through a residual plot and finding that there is no pattern in residual plot\n\n\n\nCode\nslr &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = train)\npar(mfrow = c(2, 2))\n\nplot(slr)\n\n\n\n\n\nCode\nsummary(slr)\n\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-478762  -30030   -1405   22273  335855 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 14045.872   3942.503   3.563 0.000375 ***\nGr_Liv_Area   110.726      2.506  44.185  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57430 on 2049 degrees of freedom\nMultiple R-squared:  0.4879,    Adjusted R-squared:  0.4877 \nF-statistic:  1952 on 1 and 2049 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCode\nimport statsmodels.formula.api as smf\n\nmodel_slr = smf.ols(\"Sale_Price ~ Gr_Liv_Area\", data=train).fit()\n\nmodel_slr.pvalues\n\n\nIntercept       3.754768e-04\nGr_Liv_Area    4.195282e-300\ndtype: float64"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/lab_8.html",
    "href": "notes/analytics/linear-regression/diagnostics/lab_8.html",
    "title": "Lab 8",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncafeteria &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/cafeteria.csv\")\n\n\n\n1 1\n\n\nCode\ncafeteria_lm &lt;- lm(Sales ~ Dispensers, data = cafeteria)\n\nggplot(cafeteria_lm, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0)\n\n\n\n\n\n\nWe see a curved pattern in the residuals vs. fitted values plot\nMay want to introduce higher order terms to account for the pattern\n\n\n\n2 2\nForward selection by hand:\n\n\nCode\nempty.model &lt;- lm(Sales ~ 1, data = cafeteria)\nfull.model &lt;- lm(Sales ~ I(Dispensers^4) + I(Dispensers^3) + I(Dispensers^2) + Dispensers, data = cafeteria)\n\n\n\n\nCode\nc(AIC(empty.model), AIC(lm(Sales ~ Dispensers, data = cafeteria)), AIC(lm(Sales ~ Dispensers + I(Dispensers^2), data = cafeteria)), AIC(lm(Sales ~ Dispensers + I(Dispensers^2) + I(Dispensers^3), data = cafeteria)), AIC(full.model))\n\n\n[1] 176.0696 126.8844 101.9835 102.6002 104.5498\n\n\n\nBest degree for the model is the second-order: 2\n\n\n\nCode\nselected.model &lt;- lm(Sales ~ Dispensers + I(Dispensers^2), data = cafeteria)\n\nggplot(selected.model, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0)\n\n\n\n\n\nCode\nggplot(selected.model, aes(sample = .resid)) +\n    stat_qq() +\n    stat_qq_line()\n\n\n\n\n\nCode\nhist(selected.model$resid)\n\n\n\n\n\n\nQQ-Plot shows evidence of normality\nHistogram shows a fairly Normal dist."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/breakout_6.html",
    "href": "notes/analytics/linear-regression/MLR/breakout_6.html",
    "title": "Breakout 6",
    "section": "",
    "text": "1 1\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\n\nset.seed(123)\nbike &lt;- bike %&gt;% mutate(id = row_number())\ntrain &lt;- bike %&gt;% sample_frac(0.7)\ntest &lt;- anti_join(bike, train, by = \"id\")\ndim(train)\n\n\n[1] 12165    17\n\n\nCode\ndim(test)\n\n\n[1] 5214   17\n\n\n\n\n2 2\n\n\nCode\nbike_lm &lt;- lm(cnt ~ temp + hum + windspeed, train)\nsummary(bike_lm)\n\n\n\nCall:\nlm(formula = cnt ~ temp + hum + windspeed, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-327.84 -102.59  -32.51   65.79  707.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  172.430      7.443  23.166   &lt;2e-16 ***\ntemp         365.042      7.458  48.946   &lt;2e-16 ***\nhum         -268.340      7.776 -34.508   &lt;2e-16 ***\nwindspeed     22.749     12.208   1.863   0.0624 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 157.5 on 12161 degrees of freedom\nMultiple R-squared:  0.2489,    Adjusted R-squared:  0.2487 \nF-statistic:  1343 on 3 and 12161 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(R_a^2 is 0.2487\\)\nThe variables temp, hum are significant at the 0.01 level\n\n\n\n3 3\n\n\nCode\nbike_lm2 &lt;- lm(cnt ~ atemp + hum + windspeed, train)\nsummary(bike_lm2)\n\n\n\nCall:\nlm(formula = cnt ~ atemp + hum + windspeed, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-349.42 -102.16  -32.60   65.91  702.10 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  155.087      7.631  20.323  &lt; 2e-16 ***\natemp        411.554      8.365  49.198  &lt; 2e-16 ***\nhum         -270.493      7.766 -34.831  &lt; 2e-16 ***\nwindspeed     45.606     12.225   3.731 0.000192 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 157.4 on 12161 degrees of freedom\nMultiple R-squared:  0.2502,    Adjusted R-squared:   0.25 \nF-statistic:  1352 on 3 and 12161 DF,  p-value: &lt; 2.2e-16\n\n\n\n\\(R_a^2\\) is 0.25\natemp, hum, and windspeed are all significant at the 0.01 level\n\n\n\n4 4\nOur second model with atemp has a higher \\(R_a^2\\) value."
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/breakout_9.html",
    "href": "notes/analytics/linear-regression/influence/breakout_9.html",
    "title": "Breakout 9",
    "section": "",
    "text": "Code\nlibrary(car)\n\n\nLoading required package: carData\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ dplyr::recode() masks car::recode()\n✖ purrr::some()   masks car::some()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nglimpse(mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,…\n$ cyl  &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,…\n$ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16…\n$ hp   &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180…\n$ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,…\n$ wt   &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.…\n$ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18…\n$ vs   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,…\n$ am   &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,…\n$ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,…\n$ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,…\n\n\n\n\nCode\nlm.model &lt;- lm(mpg ~ ., data = mtcars)\nv &lt;- vif(lm.model)\nv[v &gt; 10]\n\n\n     cyl     disp       wt \n15.37383 21.62024 15.16489 \n\n\nCode\nwhich.min(abs(cor(mtcars)[\"mpg\", c(\"cyl\", \"disp\", \"wt\")]))\n\n\ndisp \n   2 \n\n\n\n\nCode\nmtcars.subset &lt;- mtcars %&gt;%\n    select(-disp)\n\nlm.model &lt;- lm(mpg ~ ., data = mtcars.subset)\nv &lt;- vif(lm.model)\nv[v &gt; 10]\n\n\n     cyl \n14.28474 \n\n\n\n\nCode\nmtcars.subset &lt;- mtcars.subset %&gt;%\n    select(-cyl)\n\nlm.model &lt;- lm(mpg ~ ., data = mtcars.subset)\nv &lt;- vif(lm.model)\nv[v &gt; 10]\n\n\nnamed numeric(0)\n\n\n\n1 Model Selection\n\n\nCode\nempty.model &lt;- lm(mpg ~ 1, data = mtcars.subset)\nfull.model &lt;- lm(mpg ~ ., data = mtcars.subset)\n\nfor.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ),\n    direction = \"forward\",\n    k = 2,\n    trace = FALSE\n)\nsummary(for.model)\n\n\n\nCall:\nlm(formula = mpg ~ wt + hp + am + qsec, data = mtcars.subset)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4975 -1.5902 -0.1122  1.1795  4.5404 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept) 17.44019    9.31887   1.871  0.07215 . \nwt          -3.23810    0.88990  -3.639  0.00114 **\nhp          -0.01765    0.01415  -1.247  0.22309   \nam           2.92550    1.39715   2.094  0.04579 * \nqsec         0.81060    0.43887   1.847  0.07573 . \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.435 on 27 degrees of freedom\nMultiple R-squared:  0.8579,    Adjusted R-squared:  0.8368 \nF-statistic: 40.74 on 4 and 27 DF,  p-value: 4.589e-11\n\n\n\n\nCode\nggplot(for.model, aes(sample = .resid)) +\n    stat_qq() +\n    stat_qq_line()\n\n\n\n\n\nCode\nhist(resid(for.model))\n\n\n\n\n\nCode\nggplot(for.model, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_line(y = 0)"
  },
  {
    "objectID": "notes/analytics/index.html",
    "href": "notes/analytics/index.html",
    "title": "Analytics",
    "section": "",
    "text": "ARIMA Models\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nSep 13, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Workshop\n\n\n\n\n\n\n\nanalytics\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nModel Assessment\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\nmodeling\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nARIMA Models: AR and MA Models\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nSep 7, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nCorrelation Functions\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nDiagnostics and Subset Selection\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nStationarity\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nLinear Trend for Exponential Smoothing\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nExponential Smoothing Models\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nData Considerations\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nEstimation Methods for Logistic Regression\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nAug 25, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Forecasting & Time Series Structure\n\n\n\n\n\n\n\nanalytics\n\n\ntime series\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Logistic Regression Review\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nAug 22, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Logistic Regression\n\n\n\n\n\n\n\nanalytics\n\n\nlogistic regression\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nCategorical Data Analysis\n\n\n\n\n\n\n\nanalytics\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJul 17, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nModel Building and Scoring for Prediction\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\nmodeling\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nCorrelated Error Terms\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\ncorrelation\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nDiagnostics\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\ndiagnostics\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nModel Selection\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\nmodeling\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nMultiple Linear Regression\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nMore Complex ANOVA & Regression\n\n\n\n\n\n\n\nanalytics\n\n\nANOVA\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nOrdinary Least Squares Regression\n\n\n\n\n\n\n\nanalytics\n\n\nlinear regression\n\n\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to ANOVA and Regression\n\n\n\n\n\n\n\nanalytics\n\n\nANOVA\n\n\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Statistical Inference\n\n\n\n\n\n\n\nanalytics\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJun 28, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nExploratory Data Analysis\n\n\n\n\n\n\n\nanalytics\n\n\nEDA\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html",
    "href": "notes/analytics/timeseries/stationarity/index.html",
    "title": "Stationarity",
    "section": "",
    "text": "ARMA stands for AutoRegressive Moving Averages. AR and MA terms are used to model the dependency structure in the data.\nARMA models are based on statistical methods (will assume a distribution). The best model will be found by an iterative process.\n\n\nModels can have singal due to a seasonal pattern or due to trend. They can also have signal due to “correlation structure” which can be in the form of Autoregressive and moving averages.\nIn order to model the dependency in the data, we need to take care of the functional form and any random walks first.\n\n\n\nARIMA Modeling Flowchart"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#signal",
    "href": "notes/analytics/timeseries/stationarity/index.html#signal",
    "title": "Stationarity",
    "section": "",
    "text": "Models can have singal due to a seasonal pattern or due to trend. They can also have signal due to “correlation structure” which can be in the form of Autoregressive and moving averages.\nIn order to model the dependency in the data, we need to take care of the functional form and any random walks first.\n\n\n\nARIMA Modeling Flowchart"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#stationarity",
    "href": "notes/analytics/timeseries/stationarity/index.html#stationarity",
    "title": "Stationarity",
    "section": "2.1 Stationarity",
    "text": "2.1 Stationarity\nTo model AR and MA terms, we need to have stationarity first.\nWeak stationarity is where there is no predictable pattern, we have constant variance and converges to a constant mean in the “long run.”"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#random-walk",
    "href": "notes/analytics/timeseries/stationarity/index.html#random-walk",
    "title": "Stationarity",
    "section": "2.2 Random Walk",
    "text": "2.2 Random Walk\n\\[\nY_t = Y_{t-1} + \\epsilon_t\n\\]\nThe current observation only depends on the previous observation and some error. There is no correlation/dependency throughout the entire series.\nIf we have a random walk, we try to take differences:\n\\[\nY_t - Y_{t-1} = \\epsilon_t\n\\]\nHow do we know if we have a random walk or not?"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#augmented-dickey-fuller-unit-root-test",
    "href": "notes/analytics/timeseries/stationarity/index.html#augmented-dickey-fuller-unit-root-test",
    "title": "Stationarity",
    "section": "3.1 Augmented Dickey-Fuller Unit Root Test",
    "text": "3.1 Augmented Dickey-Fuller Unit Root Test\nProvides a statistical test for detecting a random walk.\n\n\\(H_0:\\) Differencing is required (non-stationary data; random walk)\n\\(H_a:\\) Stationary mean about zero (if the series is centered about 0); Stationary mean otherwise\n\nCalled “unit root test” because it looks to see if the equation with differenced series has a unit root (\\(\\phi = 1\\))\n\\[\nY_t = \\phi Y_{t-1} + \\epsilon_t\n\\]\nUnit roots can exist in models with more than one lag of Y.\n\nLag 0 tests are equivalent to what we have prev. seen\nLag 1 tests consider models with differenced series of Y and first lag of differenced series\nLag 2 tests consider models with differenced series of Y and first and second lag of differenced series\n\nWhen testing for stationarity, you should go to at least a lag 2 while looking at ALL of the tests.\n\nRPython\n\n\n\n\nCode\nlibrary(aTSA)\n\nfile_dir &lt;- \"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/\"\nquotes &lt;- read.csv(paste(file_dir, \"fpp_insurance.csv\", sep = \"\"))\nquotes_ts &lt;- ts(quotes$Quotes, start = 2002, frequency = 12)\n\nadf.test(quotes_ts)\n\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag     ADF p.value\n[1,]   0 -0.3061   0.550\n[2,]   1 -0.5980   0.458\n[3,]   2 -0.0632   0.620\n[4,]   3 -0.0950   0.611\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -2.66  0.0939\n[2,]   1 -3.42  0.0192\n[3,]   2 -2.45  0.1608\n[4,]   3 -2.36  0.1943\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -2.62  0.3212\n[2,]   1 -3.36  0.0772\n[3,]   2 -2.41  0.4012\n[4,]   3 -2.29  0.4463\n---- \nNote: in fact, p.value = 0.01 means p.value &lt;= 0.01 \n\n\nWe look at the type 2 test since our series does not seem to be centered around 0. Based on our p-values we believe that"
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#adf-test-process",
    "href": "notes/analytics/timeseries/stationarity/index.html#adf-test-process",
    "title": "Stationarity",
    "section": "3.2 ADF Test Process",
    "text": "3.2 ADF Test Process\n\nFirst decide if you are doing zero mean or single mean test\nDecide how many lags you would like to look at (common in industry to do 3 - 5 lags)\nSee if you reject ANY of these hypotheses\nIf you reject all hypotheses then you are ready to start modeling AR and MA terms\nIf you fail to reject at least one, you have a random walk and will take differences and start modeling AR and MA terms on the difference."
  },
  {
    "objectID": "notes/analytics/timeseries/stationarity/index.html#over-differencing",
    "href": "notes/analytics/timeseries/stationarity/index.html#over-differencing",
    "title": "Stationarity",
    "section": "3.3 Over-differencing",
    "text": "3.3 Over-differencing\nWhen you difference and you don’t need to difference or take too many differences you create over-differencing.\nOver-differencing introduces more dependence on error terms in the model. There are more moving average terms that don’t really exist."
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/esm/index.html",
    "href": "notes/analytics/timeseries/exponential-smoothing/esm/index.html",
    "title": "Exponential Smoothing Models",
    "section": "",
    "text": "Time series data assumes that observations at a certain time point depend on previous observations in time."
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/esm/index.html#component-form",
    "href": "notes/analytics/timeseries/exponential-smoothing/esm/index.html#component-form",
    "title": "Exponential Smoothing Models",
    "section": "2.1 Component Form",
    "text": "2.1 Component Form\nSingle ESM can be written in component form:\n\\[\n\\hat{Y}_{t+1} = L_t\n\\]\n\\[\nL_t = \\theta Y_t + (1 - \\theta)L_{t - 1}\n\\]"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/esm/index.html#parameter-estimation",
    "href": "notes/analytics/timeseries/exponential-smoothing/esm/index.html#parameter-estimation",
    "title": "Exponential Smoothing Models",
    "section": "2.2 Parameter Estimation",
    "text": "2.2 Parameter Estimation\nTypical method for calculating optimal value of \\(\\theta\\) in ESM is one-step ahead forecasts.\nValue of \\(\\theta\\) that minimizes the one-step ahead forecast errors is considered the optimal value:\n\\[\n\\text{SSE} = \\sum_{t=1}^T (Y_t - \\hat{Y}_t)^2\n\\]\nEstimates that are not statistically significant should not be disqualified. Estimates are fine even without normality however normality is needed is trying to construct a confidence interval.\n\nPython\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing\n\nsteel = pd.read_csv(\n    \"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/steel.csv\"\n)\ndf = pd.date_range(\"1/1/1984\", \"12/1/1991\", freq=\"MS\")\nsteel.set_index(pd.to_datetime(df), inplace=True)\n\nfit = SimpleExpSmoothing(steel[\"steelshp\"]).fit()\nfit.params[\"smoothing_level\"]\n\n\n0.45493602336086547"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html",
    "title": "ARIMA Models: AR and MA Models",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(tseries)\nlibrary(forecast)\nY &lt;- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/ar2.csv\")\nx &lt;- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/MA2.csv\")"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-ar1",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-ar1",
    "title": "ARIMA Models: AR and MA Models",
    "section": "2.1 Correlation Functions for AR(1)",
    "text": "2.1 Correlation Functions for AR(1)\nACF decreases exponentially as the number of lags increases. However, the PACF has a significant spike at the first lag, followed by nothing after.\n\n\n\nAR(1)-ACF\n\n\n\n\n\nAR(1)-PACF\n\n\nOverall, the effect of shocks last over a long period of time. However, the effect of shocks that happened long ago has little effect on the present IF the value for \\(|\\phi| &lt; 1\\). Stationarity–the dependence of previous observations declines over time.\n\nIf \\(\\phi = 1\\), then we have a Random Walk and NOT AR model\nIf \\(\\phi &gt; 1\\), then today depends on tomorrow which does not make sense"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ar2-model",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ar2-model",
    "title": "ARIMA Models: AR and MA Models",
    "section": "2.2 AR(2) Model",
    "text": "2.2 AR(2) Model\nA time series that is a linear function of 2 past values plus error is an AR process of order 2:\n\\[\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + e_t\n\\]\nAR(2) models have a pattern in PACF plots for when it comes to stationarity (2 spikes). The effect of shocks that happened long ago has little effect on the present if the value for \\(\\left| \\phi_1 + \\phi_2 \\right| &lt; 1\\)"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#arp-model",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#arp-model",
    "title": "ARIMA Models: AR and MA Models",
    "section": "2.3 AR(p) Model",
    "text": "2.3 AR(p) Model\nTime series that is a linear function of \\(p\\) past values plus error is called AR process or order \\(p\\).\n\\[\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\cdots + \\phi_pY_{t-p} + e_t\n\\]\nIf the model is just an AR model, then the PACF has significant spikes at hte lags up to \\(p\\) lags, followed by nothing after.\n\nRPython\n\n\n\n\nCode\nY_ts &lt;- ts(Y)\nY_ARIMA &lt;- Arima(Y_ts, order = c(2, 0, 0))\n\nggAcf(Y_ARIMA$residuals)\n\n\n\n\n\nCode\nggPacf(Y_ARIMA$residuals)\n\n\n\n\n\n\n\n\n\n\n\nThe coefficients from the AR(2) model are not very interpretable. If you are reporting to a client you would just say that you fit an AR(2) model."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ma1",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ma1",
    "title": "ARIMA Models: AR and MA Models",
    "section": "3.1 MA(1)",
    "text": "3.1 MA(1)\n\\[\nY_t = \\omega + e_t - \\theta e_{t-1}\n\\]\nThis is true for all observations (each observation is dependent on the error from the previous observation). Therefore, in an MA(1) model, individual shocks only last for a short time.\nIn the MA model, we do not have the same restrictions as AR models (but do want them to be invertible)."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-ma1",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-ma1",
    "title": "ARIMA Models: AR and MA Models",
    "section": "3.2 Correlation Functions for MA(1)",
    "text": "3.2 Correlation Functions for MA(1)\nThe ACF has a significant spike at the first lag, followed by nothing after.\nPACF decreases exponentially as the number of lags increases. For \\(q\\) &gt; 1, however, these patterns do not imply as the model becomes more complicated."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#maq",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#maq",
    "title": "ARIMA Models: AR and MA Models",
    "section": "3.3 MA(q)",
    "text": "3.3 MA(q)\nTime series that is a linear function of \\(q\\) past errors is a moving average process of order \\(q\\).\n\\[\nY_t = \\omega + e_t - \\theta_1e_{t-1} - \\theta_2e_{t-2} - \\cdots - \\theta_qe_{t-q}\n\\]"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-maq",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#correlation-functions-for-maq",
    "title": "ARIMA Models: AR and MA Models",
    "section": "3.4 Correlation Functions for MA(q)",
    "text": "3.4 Correlation Functions for MA(q)\nACF has significant spikes at lags up to lag \\(q\\), followed by nothing after.\n\nRPython\n\n\nMA(2) Model\n\n\nCode\nggAcf(x)\n\n\n\n\n\nCode\nggPacf(x)\n\n\n\n\n\n\n\nCode\nx_ts &lt;- ts(x)\nx_ARIMA &lt;- Arima(x_ts, order = c(0, 0, 2))\nsummary(x_ARIMA)\n\n\nSeries: x_ts \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n          ma1     ma2    mean\n      -0.2460  0.4772  0.0250\ns.e.   0.0857  0.0923  0.0567\n\nsigma^2 = 0.2207:  log likelihood = -65.1\nAIC=138.2   AICc=138.63   BIC=148.62\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0008828966 0.4627151 0.3808289 74.99115 114.4434 0.5453401\n                     ACF1\nTraining set -0.002299708\n\n\nCode\nggAcf(x_ARIMA$residuals)\n\n\n\n\n\nCode\nggPacf(x_ARIMA$residuals)\n\n\n\n\n\n\n\n\n\n\n\nNote that we can compare the AIC/BIC between AR, MA, or ARIMA models but we cannot compare them to the metrics given by ESM models."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ljung-box-chi2-test-for-white-noise",
    "href": "notes/analytics/timeseries/ARIMA/ARMA/index.html#ljung-box-chi2-test-for-white-noise",
    "title": "ARIMA Models: AR and MA Models",
    "section": "5.1 Ljung-Box \\(\\chi^2\\) Test for White Noise",
    "text": "5.1 Ljung-Box \\(\\chi^2\\) Test for White Noise\nLjung-Box can be applied to original data or to the residuals after fitting a model.\n\n\\(H_0:\\) Series has no autocorrelation.\n\\(H_a:\\) One or more autocorrelations up to lag \\(m\\) are not zero.\n\nEssentially, rejecting our null hypothesis means that more modeling should be done.\n\\[\n\\chi_m^2 = n(n + 2) \\sum_{k=1}^{m} \\frac{\\beta_k^2}{n - k}\n\\]\n\nRPython\n\n\nLooking at original data first:\n\n\nCode\nindex1 &lt;- seq(1, 10)\nwhite_lb &lt;- rep(NA, 10)\n\nfor (i in 1:10) {\n    white_lb[i] &lt;- Box.test(Y, lag = i, type = \"Ljung-Box\", fitdf = 0)$p.value\n}\n\nwhite_dat &lt;- data.frame(cbind(white_lb, index1))\ncolnames(white_dat) &lt;- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test p-values\", x = \"Lags\", y = \"p-values\")\n\n\n\n\n\nLooking at the fitted model:\n\n\nCode\nY_ARIMA &lt;- Arima(Y, order = c(2, 0, 0))\nwhite_lb &lt;- rep(NA, 10)\n\nfor (i in 3:10) {\n    white_lb[i] &lt;- Box.test(Y_ARIMA$residuals, lag = i, type = \"Ljung-Box\", fitdf = 2)$p.value\n}\nwhite_dat &lt;- data.frame(cbind(white_lb[3:10], index1[3:10]))\ncolnames(white_dat) &lt;- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test when this is white noise\", x = \"Lags\", y = \"p-values\")\n\n\n\n\n\nNote that you have to start your tests on the next lag term. In this example we are using AR(2) so our tests start on lag 3."
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/breakout_2.html",
    "href": "notes/analytics/EDA/statistical-inference/breakout_2.html",
    "title": "Yang MSA",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\ncovid &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/coviddata.csv\")\n\nglimpse(covid)\n\n\nRows: 1,551\nColumns: 5\n$ X                    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15…\n$ zip                  &lt;int&gt; 90210, 90221, 90232, 90243, 90254, 90265, 90276, …\n$ region               &lt;chr&gt; \"West\", \"West\", \"West\", \"West\", \"West\", \"West\", \"…\n$ population           &lt;int&gt; 50509, 52659, 50305, 53724, 47560, 50325, 47920, …\n$ covidDeathsPerCapita &lt;dbl&gt; 19.075, 17.728, 18.022, 17.896, 17.165, 17.275, 1…\n\n\n\nYou’ve been asked to explore differences between rates of death from Covid in the east vs. the west. You download some data (covidPerCapita) from the CDC that has Covid death rates per capita (covidDeathsPerCapita) by zip code (zip). Not all zip codes are reporting, and even handling the data from those that do report is difficult so you select a random sample of zip codes in the east and the west.\n\nConduct a t-test for the difference of means in per capita deaths from Covid in the east region vs the west region. Don’t forget to verify assumptions. Note anything of interest to you, including the results of any tests performed, violations of assumptions, etc.\n\n\nVerifying normality:\n\n\nCode\nggplot(covid, aes(sample = covidDeathsPerCapita, color = region)) +\n    stat_qq() +\n    stat_qq_line()\n\n\n\n\n\n\nBoth regions are approximately normally distributed for per capita deaths\n\nTesting for difference in variances:\n\n\nCode\nvar.test(covidDeathsPerCapita ~ region, data = covid)$p.value\n\n\n[1] 0.1738449\n\n\n\nAt a p-value of 0.174 we do not reject the null hypothesis that the variances are not significantly different\nWe can proceed with a two-sample t-test with equal variances\n\n\n\nCode\nt.test(covidDeathsPerCapita ~ region, data = covid, var.equal = TRUE)\n\n\n\n    Two Sample t-test\n\ndata:  covidDeathsPerCapita by region\nt = 53.6, df = 1549, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group East and group West is not equal to 0\n95 percent confidence interval:\n 1.928196 2.074681\nsample estimates:\nmean in group East mean in group West \n          20.00643           18.00500 \n\n\nWe have a very low p-value such that we reject the null hypothesis that our means are similar. We believe that the mean deaths are significantly different between the two regions.\nAlthough our tests from this sample allow us to conclude that the means are different, we have to be careful because our sample does not include all zip codes. Is our data balanced? Did we have issues with the way the data was sampled?\n\n\nCode\nt.test(covidDeathsPerCapita ~ region, data = covid, var.equal = TRUE, alternative = \"greater\")\n\n\n\n    Two Sample t-test\n\ndata:  covidDeathsPerCapita by region\nt = 53.6, df = 1549, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group East and group West is greater than 0\n95 percent confidence interval:\n 1.939983      Inf\nsample estimates:\nmean in group East mean in group West \n          20.00643           18.00500 \n\n\n\n\nCode\nlength(covid[covid$region == \"West\", ]) == length(covid[covid$region == \"East\", ])\n\n\n[1] TRUE"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html",
    "href": "notes/analytics/EDA/statistical-inference/index.html",
    "title": "Introduction to Statistical Inference",
    "section": "",
    "text": "Last time we talked about different statistical measures like mean and standard deviation. These are called point estimates.\nThere is variability among samples. However, we can have a margin of error for our estimate via the Central Limit Theorem"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#procedure",
    "href": "notes/analytics/EDA/statistical-inference/index.html#procedure",
    "title": "Introduction to Statistical Inference",
    "section": "3.1 Procedure",
    "text": "3.1 Procedure\n\nStart with a null hypothesis \\(H_0\\) about a parameter of interest. We assume \\(H_0\\) is true.\nSelect an acceptable significance level \\(\\alpha\\) which represents the likelihood that you incorrectly reject \\(H_0\\) (probability of Type I error)\nAlternative hypothesis \\(H_a\\) is the logical opposite. Note that alternative hypothesis is the “significantly different” statement–no equal signs should appear in alternative\nCollect data, compute statistic\nDetermine the probability that you observed a statistic as extreme or more extreme as the one you did assuming \\(H_0\\) is true \\(\\rightarrow\\) p-value\nIf p-value \\(\\leq \\alpha\\), reject \\(H_0\\) and fail to reject otherwise. Make sure to interpret the p-value."
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#assumptions",
    "href": "notes/analytics/EDA/statistical-inference/index.html#assumptions",
    "title": "Introduction to Statistical Inference",
    "section": "5.1 Assumptions",
    "text": "5.1 Assumptions\nOne-sample t-tests need a large enough sample size for the Central Limit Theorem to hold. If you don’t have sample size, then the population distribution needs to be Normal."
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#example-in-r",
    "href": "notes/analytics/EDA/statistical-inference/index.html#example-in-r",
    "title": "Introduction to Statistical Inference",
    "section": "5.2 Example in R",
    "text": "5.2 Example in R\n\nWe want to know if the true Sales Price is different then $178,000.\nThe null hypothesis is \\(H_0\\): \\(\\mu = 178000\\) and the alternative is \\(H_a\\): \\(\\mu \\neq 178000\\). \\(\\alpha = 0.05\\)\n\n\n\nCode\nlibrary(AmesHousing)\n\names &lt;- make_ordinal_ames()\n\nt.test(ames$Sale_Price, mu = 178000)\n\n\n\n    One Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.05825\nalternative hypothesis: true mean is not equal to 178000\n95 percent confidence interval:\n 177902.3 183689.9\nsample estimates:\nmean of x \n 180796.1 \n\n\nDo not reject the null hypothesis as p-value \\(&gt; \\alpha\\)\nTo conduct a directional t-test:\n\n\nCode\nt.test(ames$Sale_Price, mu = 178000, alternative = \"greater\")\n\n\n\n    One Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.02913\nalternative hypothesis: true mean is greater than 178000\n95 percent confidence interval:\n 178367.7      Inf\nsample estimates:\nmean of x \n 180796.1 \n\n\nCode\nt.test(ames$Sale_Price, mu = 178000, alternative = \"less\")\n\n\n\n    One Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.9709\nalternative hypothesis: true mean is less than 178000\n95 percent confidence interval:\n     -Inf 183224.4\nsample estimates:\nmean of x \n 180796.1"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#assumptions-1",
    "href": "notes/analytics/EDA/statistical-inference/index.html#assumptions-1",
    "title": "Introduction to Statistical Inference",
    "section": "6.1 Assumptions",
    "text": "6.1 Assumptions\n\nIndependent observations\nNormally distributed data for each group\nEqual variances for each group\n\nTested formally with F-test to determine which t-test to use"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#f-test-for-equality-of-variances",
    "href": "notes/analytics/EDA/statistical-inference/index.html#f-test-for-equality-of-variances",
    "title": "Introduction to Statistical Inference",
    "section": "6.2 F-Test for Equality of Variances",
    "text": "6.2 F-Test for Equality of Variances\n\n\\(H_0: \\sigma_1^2 = \\sigma_2^2\\)\n\\(H_a: \\sigma_1^2 \\neq \\sigma_2^2\\)\n\\(F = \\frac{\\max(s_1^2, s_2^2)}{\\min(s_1^2, s_2^2)}\\)"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#two-sample-t-test-in-r",
    "href": "notes/analytics/EDA/statistical-inference/index.html#two-sample-t-test-in-r",
    "title": "Introduction to Statistical Inference",
    "section": "6.3 Two-Sample t-test in R",
    "text": "6.3 Two-Sample t-test in R\nWe first need to verify the normality condition:\n\n\nCode\nggplot(ames, aes(sample = Sale_Price, color = Central_Air)) +\n    stat_qq() +\n    stat_qq_line()\n\n\n\n\n\nNormality seems to fail with houses that have central air conditioning. However, for illustration we will still conduct the two-sample t-test.\nNote that in practice if normality fails then some groups consider not even conducting a t-test when variances are equal–just go straight to variances are not equal.\n\n\nCode\nvar.test(Sale_Price ~ Central_Air, data = ames)\n\n\n\n    F test to compare two variances\n\ndata:  Sale_Price by Central_Air\nF = 0.2258, num df = 195, denom df = 2733, p-value &lt; 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.1854873 0.2800271\nsample estimates:\nratio of variances \n         0.2257977 \n\n\nReject \\(H_0\\) based on the p-value so we conclude that the variances are not equal.\n\n\nCode\nt.test(Sale_Price ~ Central_Air, data = ames, var.equal = FALSE)\n\n\n\n    Welch Two Sample t-test\n\ndata:  Sale_Price by Central_Air\nt = -27.433, df = 336.06, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group N and group Y is not equal to 0\n95 percent confidence interval:\n -90625.69 -78498.92\nsample estimates:\nmean in group N mean in group Y \n       101890.5        186452.8 \n\n\nWith a regular two-sample t-test we reject the null hypothesis that the means are equal.\nHowever, our normality assumption wasn’t satisfied so we should use a nonparametric test that does not rely on normality."
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/index.html#wilcoxon-rank",
    "href": "notes/analytics/EDA/statistical-inference/index.html#wilcoxon-rank",
    "title": "Introduction to Statistical Inference",
    "section": "6.4 Wilcoxon Rank",
    "text": "6.4 Wilcoxon Rank\nThe question we are answering with this test is, “Are the median sale prices of houses with and without central air the same?”\n\n\nCode\nwilcox.test(Sale_Price ~ Central_Air, data = ames)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  Sale_Price by Central_Air\nW = 63164, p-value &lt; 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n6.4.1 Interpretations of Wilcoxon\n\n\n\n\n\n\n\nConditions\nInterpretation of Significant Mann-Whiteney-Wilcoxon Test\n\n\n\n\nGroup distributions are identical in shape, variance and symmetric\nDifference in means\n\n\nGroup distributions are identical in shape, variance, but not symmetric\nDifference in medians\n\n\nGroup distributions are not identical in shape, variance, and are not symmetric\nDifference in location (distributional dominance)"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/lab_12.html",
    "href": "notes/analytics/EDA/categorical-data/lab_12.html",
    "title": "Lab 12",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nsafety &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/safety.csv\")"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/lab_12.html#a",
    "href": "notes/analytics/EDA/categorical-data/lab_12.html#a",
    "title": "Lab 12",
    "section": "1.1 a",
    "text": "1.1 a\n\nUnsafe is ordinal\nType is nominal\nRegion is ordinal\nWeight is ordinal\nSize is ordinal"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/lab_12.html#b",
    "href": "notes/analytics/EDA/categorical-data/lab_12.html#b",
    "title": "Lab 12",
    "section": "1.2 b",
    "text": "1.2 b\n\n\nCode\ntable(safety$Region, safety$Unsafe)\n\n\n           \n             0  1\n  Asia      20 15\n  N America 46 15\n\n\nCode\nggplot(safety) +\n    geom_bar(aes(x = Region, fill = factor(Unsafe)))\n\n\n\n\n\n\nPercentage of cars classified as unsafe in the Asia region is 42.9 percent\nPercentage of cars classified as safe in the North America region is 75.4 percent\nAppropriate test to use is Mantel-Haenszel\n\n\\(H_0:\\) No linear association, \\(H_a:\\) Linear association exists\n\n\n\n\nCode\nlibrary(vcdExtra)\n\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n\nCode\nlibrary(DescTools)\n\nCMHtest(table(safety$Region, safety$Unsafe))$table[1, ]\n\n\n     Chisq         Df       Prob \n3.41813924 1.00000000 0.06448366 \n\n\nCode\nOddsRatio(table(safety$Region, safety$Unsafe))\n\n\n[1] 0.4347826\n\n\nCode\ntable(safety$Region, safety$Unsafe)\n\n\n           \n             0  1\n  Asia      20 15\n  N America 46 15\n\n\n\nAt an \\(\\alpha\\) level of 0.05, we do not reject the null hypothesis that there is no linear association between Region and Safety\nThere are 0.435 times the odds to be safe if it’s in the Asia region"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/lab_12.html#c",
    "href": "notes/analytics/EDA/categorical-data/lab_12.html#c",
    "title": "Lab 12",
    "section": "1.3 c",
    "text": "1.3 c\nAppropriate test is Mantel-Haenszel\n\n\nCode\nCMHtest(table(safety$Size, safety$Unsafe))$table[1, ]\n\n\n       Chisq           Df         Prob \n2.770978e+01 1.000000e+00 1.409484e-07 \n\n\n\nWe reject the null hypothesis that there is no linear association between the two variables\n\n\n\nCode\ncor.test(\n    x = as.numeric(ordered(safety$Size)), \n    y = as.numeric(ordered(safety$Unsafe)), \n    method = \"spearman\"\n)\n\n\nWarning in cor.test.default(x = as.numeric(ordered(safety$Size)), y =\nas.numeric(ordered(safety$Unsafe)), : Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  as.numeric(ordered(safety$Size)) and as.numeric(ordered(safety$Unsafe))\nS = 227423, p-value = 1.136e-08\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.5424769 \n\n\n\nIt’s not too strong at \\(\\rho = -0.542\\)"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/breakout_1.html",
    "href": "notes/analytics/EDA/introduction/breakout_1.html",
    "title": "Breakout 1",
    "section": "",
    "text": "1 Loading the Data\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\nstr(bike)\n\n\n'data.frame':   17379 obs. of  16 variables:\n $ dteday    : int  14975 14975 14975 14975 14975 14975 14975 14975 14975 14975 ...\n $ season    : int  1 1 1 1 1 1 1 1 1 1 ...\n $ yr        : int  0 0 0 0 0 0 0 0 0 0 ...\n $ mnth      : int  1 1 1 1 1 1 1 1 1 1 ...\n $ hr        : int  0 1 2 3 4 5 6 7 8 9 ...\n $ holiday   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ weekday   : int  6 6 6 6 6 6 6 6 6 6 ...\n $ workingday: int  0 0 0 0 0 0 0 0 0 0 ...\n $ weathersit: int  1 1 1 1 1 2 1 1 1 1 ...\n $ temp      : num  0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.2 0.24 0.32 ...\n $ atemp     : num  0.288 0.273 0.273 0.288 0.288 ...\n $ hum       : num  0.81 0.8 0.8 0.75 0.75 0.75 0.8 0.86 0.75 0.76 ...\n $ windspeed : num  0 0 0 0 0 0.0896 0 0 0 0 ...\n $ casual    : int  3 8 5 3 0 0 2 1 1 8 ...\n $ registered: int  13 32 27 10 1 1 0 2 7 6 ...\n $ cnt       : int  16 40 32 13 1 1 2 3 8 14 ...\n\n\nCode\ntable(bike$season)\n\n\n\n   1    2    3    4 \n4242 4409 4496 4232 \n\n\n\n\n2 Plotting the Distribution\n\n\nCode\nlibrary(ggplot2)\nggplot(bike, aes(x = cnt)) +\n    geom_histogram(fill = \"blue\") +\n    labs(x = \"Bike Rentals\", title = \"Histogram of Bike Rentals\", y = \"Frequency\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n3 Getting Summary Statistics\n\n\nCode\nsummary(bike$cnt)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    1.0    40.0   142.0   189.5   281.0   977.0 \n\n\nCode\nsd(bike$cnt)\n\n\n[1] 181.3876\n\n\nCode\nquantile(bike$cnt, probs = c(0.10, 0.40, 0.80))\n\n\n10% 40% 80% \n  9  98 321 \n\n\n\n\nCode\nggplot(bike, aes(x = cnt)) +\n    geom_histogram(fill = \"red\", binwidth = 50) +\n    labs(x = \"Bike Rentals\", title = \"Histogram of Bike Rentals\", y = \"Frequency\")\n\n\n\n\n\nCode\nggplot(bike, aes(x = cnt)) +\n    geom_histogram(fill = \"purple\", binwidth = 100) +\n    labs(x = \"Bike Rentals\", title = \"Histogram of Bike Rentals\", y = \"Frequency\")\n\n\n\n\n\nCode\nggplot(bike, aes(x = cnt)) +\n    geom_histogram(fill = \"pink\", binwidth = 250) +\n    labs(x = \"Bike Rentals\", title = \"Histogram of Bike Rentals\", y = \"Frequency\")\n\n\n\n\n\nWe can overlay a density estimator on our histogram:\n\n\nCode\nggplot(bike, aes(x = cnt)) +\n    geom_histogram(aes(y = after_stat(!!str2lang(\"density\"))), alpha = 0.2) +\n    geom_density() +\n    labs(x = \"Bike Rentals\", title = \"Histogram of Bike Rentals\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nUsing a QQ-Plot we can also see that the distribution is right-skewed:\n\n\nCode\nggplot(bike, aes(sample = cnt)) +\n    stat_qq(shape = 2) +\n    stat_qq_line()\n\n\n\n\n\n\n\n4 Associations\n\n\nCode\nggplot(bike, aes(x = factor(season), y = cnt, fill = factor(season))) +\n    geom_boxplot() +\n    scale_x_discrete(labels = c(\"Spring\", \"Summer\", \"Fall\", \"Winter\")) +\n    labs(x = \"Season\", y = \"Bike Rentals\", fill = \"Season\")\n\n\n\n\n\nSpring seems to have the fewest number of bike rentals. There are anomalous observations for count within each season."
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html",
    "href": "notes/analytics/EDA/introduction/index.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Our variables are quantities or qualities of interest. These are also called:\n\nAttributes\nFeatures\nPredictors/Targets\nFactors\nInputs/Outputs\nCovariates\n\n\n\nQuantitative variables have a quantity value associated with them. These are intervals, numerics or ratios.\n\nTime\nTemperature\nPrice\n\n\n\n\nCategorical variables are inherently described by categories instead of quantities.\nThere are two types of categorical variables:\n\nNominal\n\nSoda, Milk, Tea\n\nOrdinal\n\nHave logical orderings associated with them\nSmall, Medium, Large\nBinary IS ordinal regardless of the labels\n\n\nWith ordinal variables, you can treat them as either nominal or quantitative. You have to make the decision.\nCategorical Dummy Variables:\n\n\n\nSmall\nMedium\nLarge\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\nThe table shows an example of one-hot encoding. We can achieve this in R using the onehot package:\n\n\n\n\nCode\nlibrary(onehot)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\nset.seed(41)\ndat &lt;- data.frame(\n    y = c(rnorm(10, 2), rnorm(10, 1), rnorm(10, 0)),\n    x1 = factor(rep(c(\"A\", \"B\", \"C\"), each = 10)),\n    x2 = factor(rep(c(\"Z\", \"X\", \"Y\", \"W\", \"V\", \"U\"), each = 5))\n)\n\nencoder &lt;- onehot(dat)\ndummies &lt;- predict(encoder, dat)\nhead(dummies)\n\n\n            y x1=A x1=B x1=C x2=U x2=V x2=W x2=X x2=Y x2=Z\n[1,] 1.205632    1    0    0    0    0    0    0    0    1\n[2,] 2.197258    1    0    0    0    0    0    0    0    1\n[3,] 3.001704    1    0    0    0    0    0    0    0    1\n[4,] 3.288825    1    0    0    0    0    0    0    0    1\n[5,] 2.905753    1    0    0    0    0    0    0    0    1\n[6,] 2.493667    1    0    0    0    0    0    1    0    0\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom numpy import random\nimport pandas as pd\n\nx1 = np.repeat([\"A\", \"B\", \"C\"], 10)\nx2 = np.repeat([\"Z\", \"X\", \"Y\", \"W\", \"V\", \"U\"], 5)\n\nrandom.seed(41)\ny = np.concatenate(\n    [\n        random.normal(2.0, 1.0, 10),\n        random.normal(1.0, 1.0, 10),\n        random.normal(0.0, 1.0, 10),\n    ]\n)\narray = np.array([x1, x2, y])\narray2 = np.transpose(array)\n\ndf = pd.DataFrame(data=array2, columns=[\"x1\", \"x2\", \"y\"])\ndf.head()\n\n\n  x1 x2                   y\n0  A  Z  1.7292876769326795\n1  A  Z    2.10484805260974\n2  A  Z   2.250527815723572\n3  A  Z  1.0748000347219233\n4  A  Z   2.567143660285906\n\n\n\n\nCode\none_hot_encoded_data = pd.get_dummies(df, columns=[\"x1\", \"x2\"])\none_hot_encoded_data.head()\n\n\n                    y  x1_A   x1_B   x1_C  ...   x2_W   x2_X   x2_Y  x2_Z\n0  1.7292876769326795  True  False  False  ...  False  False  False  True\n1    2.10484805260974  True  False  False  ...  False  False  False  True\n2   2.250527815723572  True  False  False  ...  False  False  False  True\n3  1.0748000347219233  True  False  False  ...  False  False  False  True\n4   2.567143660285906  True  False  False  ...  False  False  False  True\n\n[5 rows x 10 columns]\n\n\nThe levels are given values if treated quantitatively:\n\n\n\nSize\nSize\n\n\n\n\nS\n1\n\n\nM\n2\n\n\nL\n3\n\n\n\nIn addition, we also could do optimal scaling to represent the scale of the ordinal variables. This requires a careful definition of a “1-unit” change in the variable.\n\n\n\nEducation\nEducation\n\n\n\n\nNo HS degree\n1\n\n\nGED\n2\n\n\nHS Diploma\n3\n\n\nBachelors\n10\n\n\nMasters\n16\n\n\nPhD\n20"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#quantitative-variables",
    "href": "notes/analytics/EDA/introduction/index.html#quantitative-variables",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Quantitative variables have a quantity value associated with them. These are intervals, numerics or ratios.\n\nTime\nTemperature\nPrice"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#categorical-variables",
    "href": "notes/analytics/EDA/introduction/index.html#categorical-variables",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Categorical variables are inherently described by categories instead of quantities.\nThere are two types of categorical variables:\n\nNominal\n\nSoda, Milk, Tea\n\nOrdinal\n\nHave logical orderings associated with them\nSmall, Medium, Large\nBinary IS ordinal regardless of the labels\n\n\nWith ordinal variables, you can treat them as either nominal or quantitative. You have to make the decision.\nCategorical Dummy Variables:\n\n\n\nSmall\nMedium\nLarge\n\n\n\n\n1\n0\n0\n\n\n0\n1\n0\n\n\n0\n0\n1\n\n\n\nThe table shows an example of one-hot encoding. We can achieve this in R using the onehot package:\n\n\n\n\nCode\nlibrary(onehot)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\nset.seed(41)\ndat &lt;- data.frame(\n    y = c(rnorm(10, 2), rnorm(10, 1), rnorm(10, 0)),\n    x1 = factor(rep(c(\"A\", \"B\", \"C\"), each = 10)),\n    x2 = factor(rep(c(\"Z\", \"X\", \"Y\", \"W\", \"V\", \"U\"), each = 5))\n)\n\nencoder &lt;- onehot(dat)\ndummies &lt;- predict(encoder, dat)\nhead(dummies)\n\n\n            y x1=A x1=B x1=C x2=U x2=V x2=W x2=X x2=Y x2=Z\n[1,] 1.205632    1    0    0    0    0    0    0    0    1\n[2,] 2.197258    1    0    0    0    0    0    0    0    1\n[3,] 3.001704    1    0    0    0    0    0    0    0    1\n[4,] 3.288825    1    0    0    0    0    0    0    0    1\n[5,] 2.905753    1    0    0    0    0    0    0    0    1\n[6,] 2.493667    1    0    0    0    0    0    1    0    0\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom numpy import random\nimport pandas as pd\n\nx1 = np.repeat([\"A\", \"B\", \"C\"], 10)\nx2 = np.repeat([\"Z\", \"X\", \"Y\", \"W\", \"V\", \"U\"], 5)\n\nrandom.seed(41)\ny = np.concatenate(\n    [\n        random.normal(2.0, 1.0, 10),\n        random.normal(1.0, 1.0, 10),\n        random.normal(0.0, 1.0, 10),\n    ]\n)\narray = np.array([x1, x2, y])\narray2 = np.transpose(array)\n\ndf = pd.DataFrame(data=array2, columns=[\"x1\", \"x2\", \"y\"])\ndf.head()\n\n\n  x1 x2                   y\n0  A  Z  1.7292876769326795\n1  A  Z    2.10484805260974\n2  A  Z   2.250527815723572\n3  A  Z  1.0748000347219233\n4  A  Z   2.567143660285906\n\n\n\n\nCode\none_hot_encoded_data = pd.get_dummies(df, columns=[\"x1\", \"x2\"])\none_hot_encoded_data.head()\n\n\n                    y  x1_A   x1_B   x1_C  ...   x2_W   x2_X   x2_Y  x2_Z\n0  1.7292876769326795  True  False  False  ...  False  False  False  True\n1    2.10484805260974  True  False  False  ...  False  False  False  True\n2   2.250527815723572  True  False  False  ...  False  False  False  True\n3  1.0748000347219233  True  False  False  ...  False  False  False  True\n4   2.567143660285906  True  False  False  ...  False  False  False  True\n\n[5 rows x 10 columns]\n\n\nThe levels are given values if treated quantitatively:\n\n\n\nSize\nSize\n\n\n\n\nS\n1\n\n\nM\n2\n\n\nL\n3\n\n\n\nIn addition, we also could do optimal scaling to represent the scale of the ordinal variables. This requires a careful definition of a “1-unit” change in the variable.\n\n\n\nEducation\nEducation\n\n\n\n\nNo HS degree\n1\n\n\nGED\n2\n\n\nHS Diploma\n3\n\n\nBachelors\n10\n\n\nMasters\n16\n\n\nPhD\n20"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#measures-of-central-tendency",
    "href": "notes/analytics/EDA/introduction/index.html#measures-of-central-tendency",
    "title": "Exploratory Data Analysis",
    "section": "2.1 Measures of Central Tendency",
    "text": "2.1 Measures of Central Tendency\n\n2.1.1 Mean\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\]\n\n\n2.1.2 Median\nMiddle value. 50th percentile. Unaffected by outliers. In a right-skew, median is lower than the mean. In a left-skew, median is higher than the mean.\n\n\n2.1.3 Mode\nMost frequent value. Typical for categorical data"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#measures-of-location",
    "href": "notes/analytics/EDA/introduction/index.html#measures-of-location",
    "title": "Exploratory Data Analysis",
    "section": "2.2 Measures of Location",
    "text": "2.2 Measures of Location\nPercentiles are a point, \\(x_p\\), in your data for which \\(p\\%\\) of the data is \\(\\leq x_p\\).\nQuantiles are the same thing as percentiles. The 10th percentile is the 0.10 quantile."
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#measures-of-spreaddispersion",
    "href": "notes/analytics/EDA/introduction/index.html#measures-of-spreaddispersion",
    "title": "Exploratory Data Analysis",
    "section": "2.3 Measures of Spread/Dispersion",
    "text": "2.3 Measures of Spread/Dispersion\nRange is \\(\\text{max}(data) - \\text{min}(data)\\)\n\n2.3.1 Interquartile Range\nIQR is the difference between third and first quartile. What is the range of the middle 50% of data.\n\n\n2.3.2 Variance \\(\\sigma^2\\) and Standard Deviation \nDispersion of the data around the mean. Average squared deviation from the mean.\n\\[\ns^2 = \\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\n\nThe \\(n - 1\\) comes from the degrees of freedom. In theory this will make this an unbiased estimator of the variance."
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#measure-of-shape",
    "href": "notes/analytics/EDA/introduction/index.html#measure-of-shape",
    "title": "Exploratory Data Analysis",
    "section": "2.4 Measure of Shape",
    "text": "2.4 Measure of Shape\n\n2.4.1 Modality\nModality is the number of humps a distribution has. A Normal distribution is unimodal.\n\n\n2.4.2 Skew\nIs the distribution symmetric? Or does it have a longer tail on one side?\n\n\n\nLeft-skew and Right-skew\n\n\n\n\n2.4.3 Kurtosis\nDoes the distribution have thicker/thinner tails than a Normal distribution with same mean and variance?\nA leptokurtic distribution has more data in the tails than a Normal distribution.\nA platykurtic distribution has less data in the tails than a Normal distribution.\nThis only makes sense if you have a symmetric distribution."
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#the-normal-distribution",
    "href": "notes/analytics/EDA/introduction/index.html#the-normal-distribution",
    "title": "Exploratory Data Analysis",
    "section": "2.5 The Normal Distribution",
    "text": "2.5 The Normal Distribution\nA Normal distribution is a distribution that is\n\nSymmetric\nFully defined by the mean and standard deviation\nBell-shaped / Unimodal\nMean = Median = Mode\nAsymptotic to the x-axis (bounds are \\(-\\infty\\) and \\(\\infty\\))\nKurtosis = 3 (kurtosis often reported as excess kurtosis = kurtosis - 3)\nSkew = 0 (there is no skew)\n\n\n\nCode\n# | code-fold: true\nfrom scipy.stats import norm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nx = np.linspace(-4, 4, 100)\n\nmean = 0\nsd = 1\n\ny = norm.pdf(x, mean, sd)\n\nplt.plot(x, y)"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#graphical-displays-of-distributions",
    "href": "notes/analytics/EDA/introduction/index.html#graphical-displays-of-distributions",
    "title": "Exploratory Data Analysis",
    "section": "3.1 Graphical Displays of Distributions",
    "text": "3.1 Graphical Displays of Distributions\n\n3.1.1 Histograms\nEach bar in the histogram represents a group of values (bin).\nThe height of the bar represents the frequency of percent of values in the bin. You can specify the number of width of the bins as desired.\n\n3.1.1.1 R Code\n\n\nCode\nlibrary(ggplot2)\n\nggplot(ames, aes(x = Sale_Price / 1000)) +\n    geom_histogram(mapping = aes(y = after_stat(density)), alpha = 0.5) +\n    geom_density(alpha = 0.2) +\n    labs(x = \"Sales Price (Thousands $)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n3.1.1.2 Python Code\n\n\nCode\nimport seaborn as sns\nfrom pathlib import Path\n\names = r.ames\nax = sns.histplot(x=ames[\"Sale_Price\"] / 1000, kde=True, data=ames, color=\"blue\")\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n  with pd.option_context('mode.use_inf_as_na', True):\n\n\nCode\nax.set(\n    xlabel=\"Sales Price (Thousands $)\",\n    ylabel=\"Frequency\",\n    title=\"Histogram of Sales Price in Thousands of Dollars\",\n)\n\n\n\n\n\nThe distribution is right-skewed so the mean is greater than the median housing price.\n\n\n\n3.1.2 Normal Probability Plots (QQ-Plots)\nUsed to compare two distributions, typically to verify that a variable is approx. Normal.\nCompare observed quantiles to theoretical quantiles of a Normal distribution with the same mean and variance.\nIf the points follow the line diagonal line, the distribution is Normal.\n\n\n\nQQ-Plot Problem Indicators\n\n\n\nQuadratic patterns indicate problems with skew\nCubic patterns indicate problems with kurtosis\n\n\n3.1.2.1 R Code\n\n\nCode\nggplot(ames, aes(sample = Sale_Price / 1000)) +\n    stat_qq() +\n    stat_qq_line() +\n    labs(x = \"theoretical\", y = \"observed\")\n\n\n\n\n\n\n\n3.1.2.2 Python Code\n\n\nCode\nimport statsmodels.api as sma\n\nsma.qqplot(ames[\"Sale_Price\"] / 1000, line=\"45\", fit=True)\n\n\n\n\n\n\n\n\n3.1.3 Box Plots\n\n\n\nBox Plot\n\n\n\n3.1.3.1 R Code\n\n\nCode\nggplot(ames, aes(y = Sale_Price / 1000, x = Central_Air, fill = Central_Air)) +\n    geom_boxplot() +\n    labs(y = \"Sales Price (Thousands $)\", x = \"Central Air\") +\n    scale_fill_brewer(palette = \"Accent\") +\n    theme_classic() +\n    coord_flip()\n\n\n\n\n\n\n\n3.1.3.2 Python Code\n\n\nCode\nax = sns.boxplot(ames, x=ames[\"Sale_Price\"] / 1000)\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\nCode\nax.set(\n    xlabel=\"Sales Price (Thousands $)\",\n    title=\"Boxplot of Sales Price in Thousands of Dollars\",\n)\n\n\n\n\n\n\n\nCode\nax = sns.catplot(ames, x=\"Central_Air\", y=\"Sale_Price\", kind=\"box\")\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/categorical.py:641: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  grouped_vals = vals.groupby(grouper)\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\nCode\nplt.show()"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/index.html#defining-anomalous-observations",
    "href": "notes/analytics/EDA/introduction/index.html#defining-anomalous-observations",
    "title": "Exploratory Data Analysis",
    "section": "3.2 Defining Anomalous Observations",
    "text": "3.2 Defining Anomalous Observations\n\n3.2.1 Standard Deviations from the Mean\nFor symmetric distributions and particularly for the Normal distribution, it’s common to consider observations more than 3 standard deviations from the mean as anomalous.\n\n\n3.2.2 Box-Plot Definition\nBox plots define outliers as points that are \\(1.5 \\times IQR\\) above the third quartile or less than \\(1.5 \\times IQR\\) below the first quartile.\nThere are more definitions but these are the first couple we are considering now."
  },
  {
    "objectID": "notes/analytics/logistic-regression/intro-review/index.html",
    "href": "notes/analytics/logistic-regression/intro-review/index.html",
    "title": "Introduction to Logistic Regression Review",
    "section": "",
    "text": "Binary classification is a supervised algorithm where we are trying to predict one of two target outcomes. Binary classification is one of the most common type of business problems that need solving.\nWe will use the Ames dataset again for binary logistic regression.\nWhat is regression actually doing?\nModeling the expected response conditional on the predictors. For a binary response, \\(y_i\\), the expected value is the probability of the event:\n\\[\nE(y_i) = P(y_i = 1) = p_i\n\\]\nWhy can’t we model the following:\n\\[\np_i = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n\\]\nProbabilities are bounded but linear functions can take on any value. Relationship between probabilities and X is usually nonlinear. Properties of OLS do not hold."
  },
  {
    "objectID": "notes/analytics/logistic-regression/intro-review/index.html#amount-to-double-the-odds",
    "href": "notes/analytics/logistic-regression/intro-review/index.html#amount-to-double-the-odds",
    "title": "Introduction to Logistic Regression Review",
    "section": "2.1 Amount to Double the Odds",
    "text": "2.1 Amount to Double the Odds\nWorking through math bckwards allows us to see what increase in square footage is needed for an expected doubling of odds:\n\\[\n\\text{Double Odds} = \\frac{\\log(2)}{\\beta}\n\\]\nAny odds ratio equal to 1 has no association. Lower than 1 means that the group in denominator has higher odds of the event. Greater than 1 means group in numerator has higher odds of the event."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html",
    "title": "Data Considerations",
    "section": "",
    "text": "The following considerations apply to any binary classification problem."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#oversampling",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#oversampling",
    "title": "Data Considerations",
    "section": "3.1 Oversampling",
    "text": "3.1 Oversampling\n\nDuplicate current event cases in training set to balance better with non-event cases\nKeep test set as original population proportion\n\nCan be a problem because we’re “repeating” the signal\n\nR\n\n\n\n\nCode\nlibrary(tidyverse)\n\nset.seed(12345)\n\ntrain_o &lt;- churn %&gt;%\n    sample_frac(0.7) %&gt;%\n    mutate(id = row_number())\n\n# Each positive observation is repeated 10 times\ntrain_o_T &lt;- train_o %&gt;%\n    filter(churn == TRUE) %&gt;%\n    slice(rep(1:n(), each = 10))\n\ntrain_o_F &lt;- train_o %&gt;%\n    filter(churn == FALSE)\n\ntrain_o  &lt;- rbind(train_o_F, train_o_T)\ntest_o &lt;- churn[-train_o$id,]\n\ntable(train_o$churn)\n\n\n\nFALSE  TRUE \n 1996  1070 \n\n\nCode\ntable(test_o$churn)\n\n\n\nFALSE  TRUE \n  747   154"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#adjusting-the-intercept",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#adjusting-the-intercept",
    "title": "Data Considerations",
    "section": "6.1 Adjusting the Intercept",
    "text": "6.1 Adjusting the Intercept\nNeed to correct for bias created by oversampling. Adjustment is only applied to intercept. This creates an unbiased estimate of our probabilities.\n\\[\n\\hat{p}_i = \\frac{\\hat{p}_i^*\\rho_0\\pi_1}{(1 - \\hat{p}_i^*)\\rho_1 + \\hat{p}_i^*\\rho_0\\pi_1}\n\\]\n\nPopulation proportion: \\(\\pi_1, \\pi_0\\)\nSample proportion: \\(\\rho_1, \\rho_0\\)\nUnadjusted predictions: \\(\\hat{p}_i^*\\)\n\n\nR\n\n\n\n\nCode\ntest_u_p_bias &lt;- predict(logit_model, newdata = test_u, type = \"response\")\ntest_u_p &lt;- (test_u_p_bias * (104 / 208) * (154 / 3004)) / ((1 - test_u_p_bias) * (104 / 208) * (2850 / 3004) + test_u_p_bias * (104 / 208) * (154 / 3004))\n\ntest_u &lt;- data.frame(test_u, \"Pred\" = test_u_p)\n\nhead(test_u_p)\n\n\n         1          2          3          4          5          6 \n0.04788873 0.00516951 0.03230002 0.91516214 0.56312205 0.29766875"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#weighted-observations",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#weighted-observations",
    "title": "Data Considerations",
    "section": "6.2 Weighted Observations",
    "text": "6.2 Weighted Observations\nWeighting observations adjusts while model is being built instead of after it is built.\nWe use weighted maximum likelihood estimation (MLE) so each observation has potentially different weights to the MLE calculation.\n\\[\n\\text{weight} = \\begin{cases}\n1, & y = 1 \\\\\n\\rho_1\\pi_0/\\rho_0\\pi_1, & y = 0\n\\end{cases}\n\\]\nWe can think of the 0 as gaining a multiplier of \\(\\frac{\\pi_0}{\\pi_1}\\) per observation in order to get back to the original population proportion. The weight expressed in the above formula is actually \\(\\frac{(\\pi_0 / \\pi_1)}{(\\rho_0 / \\rho_1)}\\)\n\nRPython\n\n\n\n\nCode\ntrain_u$weight &lt;- ifelse(train_u$churn == \"TRUE\", 1, 18.49)\nlogit_model_w &lt;- glm(churn ~ factor(international.plan) + factor(voice.mail.plan) + total.day.charge + customer.service.calls, data = train_u, family = binomial(), weights = weight)\nsummary(logit_model_w)\n\n\n\nCall:\nglm(formula = churn ~ factor(international.plan) + factor(voice.mail.plan) + \n    total.day.charge + customer.service.calls, family = binomial(), \n    data = train_u, weights = weight)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                   -9.76831    0.69648 -14.025  &lt; 2e-16 ***\nfactor(international.plan)yes  3.33560    0.32429  10.286  &lt; 2e-16 ***\nfactor(voice.mail.plan)yes    -1.07451    0.27107  -3.964 7.37e-05 ***\ntotal.day.charge               0.16320    0.01647   9.911  &lt; 2e-16 ***\ncustomer.service.calls         0.72693    0.08810   8.251  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 820.31  on 207  degrees of freedom\nResidual deviance: 585.33  on 203  degrees of freedom\nAIC: 590.92\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\n\n\n\nWe actually get difference significant variables between the adjusted intercept model and the weighted model. When do we use which technique? For the most part, we use weighted.\n\n\n\nWeighted vs. Adjust Intercept"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#deleting-variables",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#deleting-variables",
    "title": "Data Considerations",
    "section": "7.1 Deleting Variables",
    "text": "7.1 Deleting Variables\nIf a majority of your data is missing, then consider deleting the variable all together.\nMore than 50% missing is a good rule of thumb for removing the variable. We may not be confident that the data we do have is good."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#keeping-variables",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#keeping-variables",
    "title": "Data Considerations",
    "section": "7.2 Keeping Variables",
    "text": "7.2 Keeping Variables\nMissing values in predictor variables are not necessarily bad. For categorical variables we can add a missing category."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#replacing-variables",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#replacing-variables",
    "title": "Data Considerations",
    "section": "7.3 Replacing Variables",
    "text": "7.3 Replacing Variables\nWe can estimate a missing value with imputation. Not best to do with categorical variables as you can just add a missing category.\nApproaches:\n\nSimple mean/median replacement\nPredictive model using other variables (not empirically shown to add value)\n\nIf you impute missing continuous values then you need to create a missing value binary flag for each of the continuous variables you impute.\n\n\n\nImputing Continuous Variables"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#overall-considerations",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#overall-considerations",
    "title": "Data Considerations",
    "section": "7.4 Overall Considerations",
    "text": "7.4 Overall Considerations\nAny operations you apply to your training data need to be applied to your test data as well. Early on you should consider creating a pipeline that can apply the transforms you used on the test data when it comes to evaluation."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#linear-separation",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#linear-separation",
    "title": "Data Considerations",
    "section": "8.1 Linear Separation",
    "text": "8.1 Linear Separation\nProblems with linear separation have to be considered for categorical variables.\nComplete linear separation occurs when some combination of the predictors perfectly predict every outcome.\n\n\n\n\nYes\nNo\n\n\n\n\nGroup A\n100\n0\n\n\nGroup B\n0\n150\n\n\n\nQuasi-complete separation occurs when the outcome can be perfectly predicted for only a subset of the data. In this case, Group B is perfectly predicting the No category.\n\n\n\n\nYes\nNo\n\n\n\n\nGroup A\n77\n23\n\n\nGroup B\n0\n50\n\n\n\n\n\n\nConvergence Problems\n\n\n\nR\n\n\n\n\nCode\ntable(train_u$customer.service.calls, train_u$churn)\n\n\n   \n    FALSE TRUE\n  0    29   25\n  1    34   25\n  2    23   20\n  3    15   12\n  4     2   13\n  5     1    4\n  6     0    3\n  7     0    2\n\n\nCategories 6 and 7 are perfectly predicting churn. We have quasi-complete separation."
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#solutions",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#solutions",
    "title": "Data Considerations",
    "section": "8.2 Solutions",
    "text": "8.2 Solutions\n\nCollapse the categories of the predictor variable to eliminate the 0 cell count\nPenalized maximum likelihood\nEliminate the category altogether (may not be reasonable since the category seems important)\nCreate a dummy observation that adds to the cell counts"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#thresholding-ordinal-variables",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#thresholding-ordinal-variables",
    "title": "Data Considerations",
    "section": "8.3 Thresholding (Ordinal Variables)",
    "text": "8.3 Thresholding (Ordinal Variables)\n\n\n\nCustomer Service Calls\nSample Size\n0\n1\n\n\n\n\n0\n54\n29\n25\n\n\n1\n59\n34\n25\n\n\n2\n43\n23\n20\n\n\n3\n27\n15\n12\n\n\n4\n15\n2\n13\n\n\n5\n5\n1\n4\n\n\n6\n3\n0\n3\n\n\n7\n2\n0\n2\n\n\n\nGroup 4 - 7 categories together:\n\n\n\nCustomer Service Calls\nSample Size\n0\n1\n\n\n\n\n0\n54\n29\n25\n\n\n1\n59\n34\n25\n\n\n2\n43\n23\n20\n\n\n3\n27\n15\n12\n\n\n4+\n25\n3\n22"
  },
  {
    "objectID": "notes/analytics/logistic-regression/data-considerations/index.html#clustering-levels---greenacre-method-ordinal-variables",
    "href": "notes/analytics/logistic-regression/data-considerations/index.html#clustering-levels---greenacre-method-ordinal-variables",
    "title": "Data Considerations",
    "section": "8.4 Clustering Levels - Greenacre Method (Ordinal Variables)",
    "text": "8.4 Clustering Levels - Greenacre Method (Ordinal Variables)\n\n\n\nGreenacre Method\n\n\nWe select the clustering which results in the least amount of information lost from our original counts. In this case we group B and C together."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html",
    "href": "notes/analytics/logistic-regression/introduction/index.html",
    "title": "Introduction to Logistic Regression",
    "section": "",
    "text": "Code\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\nset.seed(123)\names &lt;- make_ordinal_ames()\n\names &lt;- ames %&gt;%\n    mutate(id = row_number())\ntrain &lt;- ames %&gt;% sample_frac(0.7)\ntrain &lt;- train %&gt;%\n    mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0))\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\ntrain = r.train\n\n\nLogistic regression is one of many classification models that can help us predict a specific class in a categorical variable.\nOne of the most common targets we will be trying to predict is a binary target variable (Yes / No, 1 / 0)."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#logit-link-function",
    "href": "notes/analytics/logistic-regression/introduction/index.html#logit-link-function",
    "title": "Introduction to Logistic Regression",
    "section": "3.1 Logit Link Function",
    "text": "3.1 Logit Link Function\nTo create linear model, a logit function is applied to the probabilities:\n\\[\n\\log(\\frac{p_i}{1 - p_i}) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i}\n\\]\n\n\\(\\frac{p_i}{1 - p_i}\\) is the odds of an outcome happening\nThe relationship between parameters and logits are linear\nLogits unbounded"
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#assumptions",
    "href": "notes/analytics/logistic-regression/introduction/index.html#assumptions",
    "title": "Introduction to Logistic Regression",
    "section": "3.2 Assumptions",
    "text": "3.2 Assumptions\n\nIndependence of observations\nLogit is linearly related to variables"
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#r-code",
    "href": "notes/analytics/logistic-regression/introduction/index.html#r-code",
    "title": "Introduction to Logistic Regression",
    "section": "3.3 R Code",
    "text": "3.3 R Code\n\n\nCode\names_logit &lt;- glm(Bonus ~ Gr_Liv_Area, data = train, family = binomial())\n\nsummary(ames_logit)\n\n\n\nCall:\nglm(formula = Bonus ~ Gr_Liv_Area, family = binomial(), data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -6.1348858  0.2757473  -22.25   &lt;2e-16 ***\nGr_Liv_Area  0.0038463  0.0001799   21.38   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1926.4  on 2049  degrees of freedom\nAIC: 1930.4\n\nNumber of Fisher Scoring iterations: 5"
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#python-code",
    "href": "notes/analytics/logistic-regression/introduction/index.html#python-code",
    "title": "Introduction to Logistic Regression",
    "section": "3.4 Python Code",
    "text": "3.4 Python Code\n\n\nCode\names_logit = smf.logit(\"Bonus ~ Gr_Liv_Area\", data=train).fit()\n\n\nOptimization terminated successfully.\n         Current function value: 0.469614\n         Iterations 7\n\n\nCode\names_logit.summary()\n\n\n\nLogit Regression Results\n\n\nDep. Variable:\nBonus\nNo. Observations:\n2051\n\n\nModel:\nLogit\nDf Residuals:\n2049\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nThu, 07 Sep 2023\nPseudo R-squ.:\n0.3060\n\n\nTime:\n10:54:06\nLog-Likelihood:\n-963.18\n\n\nconverged:\nTrue\nLL-Null:\n-1387.9\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n9.553e-187\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-6.1349\n0.276\n-22.248\n0.000\n-6.675\n-5.594\n\n\nGr_Liv_Area\n0.0038\n0.000\n21.375\n0.000\n0.003\n0.004"
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#r-code-1",
    "href": "notes/analytics/logistic-regression/introduction/index.html#r-code-1",
    "title": "Introduction to Logistic Regression",
    "section": "4.1 R Code",
    "text": "4.1 R Code\n\n\nCode\n100 * (exp(cbind(coef(ames_logit), confint(ames_logit))) - 1)\n\n\nWaiting for profiling to be done...\n\n\n                              2.5 %      97.5 %\n(Intercept) -99.7834027 -99.8755103 -99.6328865\nGr_Liv_Area   0.3853699   0.3508132   0.4216532\n\n\nThe reason we subtract by 1 is that 1 is the center point of odds. We subtract to help center back around 0.\nEvery additional square foot of greater living area increases the expected odds of being bonus eligible by 0.38%.\nIn practice, a client will care more about this interpretation of how an outcome odds changes rather than the pure technical details behind how a model was built."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#categorical-variable-interpretation",
    "href": "notes/analytics/logistic-regression/introduction/index.html#categorical-variable-interpretation",
    "title": "Introduction to Logistic Regression",
    "section": "4.2 Categorical Variable Interpretation",
    "text": "4.2 Categorical Variable Interpretation\n\n\nCode\names_logit2 &lt;- glm(Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), data = train, family = binomial())\n\n100 * (exp(cbind(coef(ames_logit2), confint(ames_logit2))) - 1)\n\n\nWaiting for profiling to be done...\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n                                         2.5 %        97.5 %\n(Intercept)         -9.999532e+01  -99.9988238   -99.9843742\nGr_Liv_Area          3.766413e-01    0.3376191     0.4175879\nCentral_AirY         3.429115e+03 1264.8396218 11177.2021479\nfactor(Fireplaces)1  1.670410e+02  108.9856565   241.6360910\nfactor(Fireplaces)2  9.607987e+01   22.4764491   214.9571145\nfactor(Fireplaces)3 -3.914247e+00  -81.9507819   497.4825425\nfactor(Fireplaces)4  8.308750e+05 -100.0000000            NA\n\n\nCode\nexp(cbind(coef(ames_logit2), confint(ames_logit2)))\n\n\nWaiting for profiling to be done...\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n                                        2.5 %       97.5 %\n(Intercept)         4.678435e-05 1.176215e-05 1.562580e-04\nGr_Liv_Area         1.003766e+00 1.003376e+00 1.004176e+00\nCentral_AirY        3.529115e+01 1.364840e+01 1.127720e+02\nfactor(Fireplaces)1 2.670410e+00 2.089857e+00 3.416361e+00\nfactor(Fireplaces)2 1.960799e+00 1.224764e+00 3.149571e+00\nfactor(Fireplaces)3 9.608575e-01 1.804922e-01 5.974825e+00\nfactor(Fireplaces)4 8.309750e+03 1.523733e-25           NA\n\n\nCode\ncoef(ames_logit2)\n\n\n        (Intercept)         Gr_Liv_Area        Central_AirY factor(Fireplaces)1 \n       -9.969961800         0.003759338         3.563632363         0.982231915 \nfactor(Fireplaces)2 factor(Fireplaces)3 factor(Fireplaces)4 \n        0.673351875        -0.039929137         9.025184862 \n\n\nHomes with central air increases the expected odds of being bonus eligible by 3416% compared to those without central air.\nA more believable way of saying this would be \\(e^{3.56} = 35.16\\) times more likely to be bonus eligible than compared to those without central air."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#concordant",
    "href": "notes/analytics/logistic-regression/introduction/index.html#concordant",
    "title": "Introduction to Logistic Regression",
    "section": "5.1 Concordant",
    "text": "5.1 Concordant\n0 and 1 pair where bonus eligible home (1) has a higher predicted probability than the non-bonus eligible home (0). The idea is that we calculate \\(p_i\\) for the 0 and 1 case for the pair and see which probability is higher.\nDoes not matter what the actual predicted probability values are as long as the bonus eligible home has a higher predicted probability than the non-bonus eligible home."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#discordant",
    "href": "notes/analytics/logistic-regression/introduction/index.html#discordant",
    "title": "Introduction to Logistic Regression",
    "section": "5.2 Discordant",
    "text": "5.2 Discordant\n0 and 1 pair where bonus eligible home (1) has a lower predicted probability than the non-bonus eligible home (0)\nModel unsuccessfully ordered the homes."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#tied-pair",
    "href": "notes/analytics/logistic-regression/introduction/index.html#tied-pair",
    "title": "Introduction to Logistic Regression",
    "section": "5.3 Tied Pair",
    "text": "5.3 Tied Pair\n0 and 1 pair where the bonus eligible home has the same predicted probability as the non-bonus eligible home."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/index.html#concordance",
    "href": "notes/analytics/logistic-regression/introduction/index.html#concordance",
    "title": "Introduction to Logistic Regression",
    "section": "5.4 Concordance",
    "text": "5.4 Concordance\nConcordance is the proportion of concordant pairs across all pairs considered in our data.\n\n\nCode\nlibrary(survival)\n\nconcordance(ames_logit)\n\n\nCall:\nconcordance.lm(object = ames_logit)\n\nn= 2051 \nConcordance= 0.8632 se= 0.007744\nconcordant discordant     tied.x     tied.y    tied.xy \n    877810     138829        601    1083029       2006 \n\n\nModel correctly ranks bonus eligible homes ahead of non-bonus eligible homes 86.3% of the time. This does not mean that our model is accurate 86.3% of the time."
  },
  {
    "objectID": "notes/communication/08032023/index.html",
    "href": "notes/communication/08032023/index.html",
    "title": "Individual Presentation Reminders",
    "section": "",
    "text": "flowchart LR \n    subgraph Context\n    bluf[BLUF] --- narrative[Narrative]\n    end\n    narrative --&gt; assumptions\n    subgraph Conflict\n    assumptions[Assumptions]---arg1[Argument 1] --- arg2[Argument 2] --- arg3[Argument 3]\n    end\n    arg3 --&gt; lf\n    subgraph Conclusion\n    lf[Looking Forward] --- sent[Sentiments]\n    end"
  },
  {
    "objectID": "notes/communication/index.html",
    "href": "notes/communication/index.html",
    "title": "Communication",
    "section": "",
    "text": "Individual Presentation Reminders\n\n\n\n\n\n\n\ncommunication\n\n\n\n\n\n\n\n\n\n\n\nAug 3, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nCommunications: 06/26/2023\n\n\n\n\n\n\n\ncommunication\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/programming/sql/Python+SQL - Lab Files/Pandas + SQL.html",
    "href": "notes/programming/sql/Python+SQL - Lab Files/Pandas + SQL.html",
    "title": "Yang MSA",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom pandasql import sqldf\nfrom pandasql import load_births\n\nbirths = load_births()\n\ntype(births)\n\n\npandas.core.frame.DataFrame\n\n\n\n\nCode\nprint(births.dtypes)\n\n\ndate      datetime64[ns]\nbirths             int64\ndtype: object\n\n\n\n\nCode\nprint(sqldf(\"SELECT * FROM births limit 5;\"))\n\n\n                         date  births\n0  1975-01-01 00:00:00.000000  265775\n1  1975-02-01 00:00:00.000000  241045\n2  1975-03-01 00:00:00.000000  268849\n3  1975-04-01 00:00:00.000000  247455\n4  1975-05-01 00:00:00.000000  254545\n\n\n\n\nCode\nprint(births.head())\n\n\n        date  births\n0 1975-01-01  265775\n1 1975-02-01  241045\n2 1975-03-01  268849\n3 1975-04-01  247455\n4 1975-05-01  254545\n\n\n\n\nCode\nq = \"\"\"\n      SELECT strftime('%Y', date) AS DOB, sum(births) as \"Total Births\"\n      FROM births\n      GROUP BY 1\n      LIMIT 10;  \n\"\"\"\n\nprint(sqldf(q))\n\n\n    DOB  Total Births\n0  1975       3136965\n1  1976       6304156\n2  1979       3333279\n3  1982       3612258\n4  1983       7333238\n5  1986       7308074\n6  1987       3760561\n7  1988       3756547\n8  1990       7718904\n9  1991      11714356\n\n\n\n\nCode\ndf1 = sqldf(q)\ntype(df1)\n\n\npandas.core.frame.DataFrame"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html",
    "href": "notes/programming/R/r4ds/intro/index.html",
    "title": "r4ds: Introduction",
    "section": "",
    "text": "Note\n\n\n\nThese notes are based on the second edition of the R for Data Science book by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund.\nAll quotes and examples are credited to the authors of this awesome book!"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html#import",
    "href": "notes/programming/R/r4ds/intro/index.html#import",
    "title": "r4ds: Introduction",
    "section": "1.1 Import",
    "text": "1.1 Import\nData comes from multiple sources:\n\nFiles\nDatabases\nWeb APIs\n\nIn the context of R, we are loading our data into a dataframe."
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html#tidy",
    "href": "notes/programming/R/r4ds/intro/index.html#tidy",
    "title": "r4ds: Introduction",
    "section": "1.2 Tidy",
    "text": "1.2 Tidy\nStore our data in a form that is consistent and allows us to focus on analyzing the problem rather than “fighting to get the data into the right form for different functions.”"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html#transform",
    "href": "notes/programming/R/r4ds/intro/index.html#transform",
    "title": "r4ds: Introduction",
    "section": "1.3 Transform",
    "text": "1.3 Transform\nIf we have a problem we are trying to analyze, then we use our problem space to guide observations of interest or create new variables from existing variables that relate to our problem.\n\n\n\n\nflowchart LR\n    Tidying --- C[ ]:::empty\n    Transforming --- C\n    C --&gt; Wrangling\n    classDef empty width:0px,height:0px;"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html#visualization-and-model",
    "href": "notes/programming/R/r4ds/intro/index.html#visualization-and-model",
    "title": "r4ds: Introduction",
    "section": "1.4 Visualization and Model",
    "text": "1.4 Visualization and Model\nWe represent our data in graphs, charts, and other displays to help us find or resolve questions about our data.\n“Models are complementary tools to visualization”. Models help us answer the questions we have.\n\nEvery model makes assumptions and they cannot answer their own assumptions"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro/index.html#communication",
    "href": "notes/programming/R/r4ds/intro/index.html#communication",
    "title": "r4ds: Introduction",
    "section": "1.5 Communication",
    "text": "1.5 Communication\nAfter the previous steps are at a satisfactory point, we have to communicate our results in a way that others can understand. This might mean in notebooks like this one or through presentations to a business client."
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "Notes",
    "section": "",
    "text": "Programming 🐍\nNotes on Python, R and fun little experiments using software!\n\n\nAnalytics 🔎\nNotes curated from my statistics and analytics courses taught by Dr. Aric LaBarr and Dr. Susan Simmons.\n\n\nCommunication 🙊\nAll about professional and technical communication taught by Dr. Sarah Egan Warren!\n\n\nPrimer 🐤\nMSA Summer Primer 2023 Notes"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html",
    "href": "notes/primer/categorical_data/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Type of Predictors | Type of Response\nCategorical\nContinuous\nContinuous and Categorical\n\n\n\n\nContinuous\nAnalysis of Variance\nOrdinary Least Squares Regression\nAnalysis of Covariance\n\n\nCategorical\nTests of Association\nLogistic Regression\nLogistic Regression"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#qualitative-data-types",
    "href": "notes/primer/categorical_data/index.html#qualitative-data-types",
    "title": "Categorical Data Analysis",
    "section": "2.1 Qualitative Data Types",
    "text": "2.1 Qualitative Data Types\nNominal\n\nCategories with no logical ordering\n\nOrdinal\n\nCategories with a logical order / only two ways to order the categories (binary is ordinal)"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#examining-categorical-variables",
    "href": "notes/primer/categorical_data/index.html#examining-categorical-variables",
    "title": "Categorical Data Analysis",
    "section": "2.2 Examining Categorical Variables",
    "text": "2.2 Examining Categorical Variables\nBy examining distributions of categorical variables we can\n\nDetermine the frequencies of data values\nRecognize possible associations among variables\n\nAssociation exists between two categorical variables if distribution of one variable changes when the level of the other variable changes.\nIf there is no association, distribution of first variable is the same regardless of the level of the other."
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#chi-square-tests",
    "href": "notes/primer/categorical_data/index.html#chi-square-tests",
    "title": "Categorical Data Analysis",
    "section": "3.1 Chi-Square Tests",
    "text": "3.1 Chi-Square Tests\n\n\\(H_0\\): No Association\nObserved freq \\(=\\) Expected freq.\n\\(H_a\\): Association\nObserved freq. \\(\\neq\\) Expected freq.\n\nExpected freq. are calculated by the formula\n\\[\n\\frac{\\text{Row Total} \\times \\text{Column Total}}{\\text{Sample Size}}\n\\]\n\n3.1.1 \\(\\chi^2\\) Distribution\n\nBounded below by zero\nRight skewed\nOne set of degrees of freedom\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2\n\nx = np.arange(0, 20, 0.001)\n\nplt.plot(x, chi2.pdf(x, df=4))\n\n\n\n\n\nFigure 1: Plot of a \\(\\chi^2\\) distribution with d.f. 4\n\n\n\n\n\n\n3.1.2 Pearson \\(\\chi^2\\) Test\n\\[\nQ_P = \\sum_{i=1}^{R} \\sum_{j=1}^{C} \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}\n\\]\n\\[\nd.f. = (\\#\\text{Rows} - 1)(\\#\\text{Columns} - 1)\n\\]\n\n\n3.1.3 Likelihood Ratio \\(\\chi^2\\) Test\n\\[\nQ_{LR} = 2 \\times \\sum_{i=1}^{R}\\sum_{j=1}^{C} Obs_{i,j} \\times \\log{(\\frac{Obs_{i,j}}{Exp_{i,j}})}\n\\]\n\\[\nd.f. = (\\#\\text{Rows} - 1)(\\#\\text{Columns} - 1)\n\\]"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#example",
    "href": "notes/primer/categorical_data/index.html#example",
    "title": "Categorical Data Analysis",
    "section": "3.2 Example",
    "text": "3.2 Example\n\nA manager of a major car dealership wants to determine if the membership of a client in the loyalty program is associated with the color of car that they buy. With this knowledge, it potentially could help the sales staff show different cars to different clients to help improve the likelihood of a sale. The manager pull information from the previous years sales.\n\n\n\nCalculate the expected counts in the right table\n\n\n\nRecall that expected frequency is given by the the product of row total and column total over sample size.\n\n\nCode\nd = {\n    'black': {'yes': 149, 'no': 101},\n    'white': {'yes': 101, 'no': 66},\n    'blue': {'yes': 72, 'no': 108},\n    'red': {'yes': 96, 'no': 161},\n    'green': {'yes': 39, 'no': 65}\n}\ndf_cars = pd.DataFrame(d).T\ndf_cars['total'] = df_cars['yes'] + df_cars['no']\ndf_cars['exp_y'] = df_cars['total'] * \\\n    df_cars['yes'].sum() / df_cars['total'].sum()\ndf_cars['exp_n'] = df_cars['total'] * \\\n    df_cars['no'].sum() / df_cars['total'].sum()\ndf_cars.head()\n\n\n\n\n\n\n\n\n\nyes\nno\ntotal\nexp_y\nexp_n\n\n\n\n\nblack\n149\n101\n250\n119.258873\n130.741127\n\n\nwhite\n101\n66\n167\n79.664927\n87.335073\n\n\nblue\n72\n108\n180\n85.866388\n94.133612\n\n\nred\n96\n161\n257\n122.598121\n134.401879\n\n\ngreen\n39\n65\n104\n49.611691\n54.388309\n\n\n\n\n\n\n\n\n\nCompute \\(Q_P\\) and \\(Q_{LR}\\) and summarize results.\n\n\n\n\nCode\ndef calculate_pearson(row):\n    return (row['yes'] - row['exp_y']) ** 2 / row['exp_y'] + (row['no'] - row['exp_n']) ** 2 / row['exp_n']\n\n\ndef calculate_likelihood(row):\n    return 2 * ((row['yes'] * np.log(row['yes'] / row['exp_y'])) + (row['no'] * np.log(row['no'] / row['exp_n'])))\n\n\nq_pearson = df_cars.apply(calculate_pearson, axis=1).sum()\nlikelihood = df_cars.apply(calculate_likelihood, axis=1).sum()\n\nprint(f'Q_p: {q_pearson}, Q_LR: {likelihood}')\n\n\nQ_p: 44.76457096344832, Q_LR: 45.07972866310165"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#ordinal-compared-to-nominal-tests",
    "href": "notes/primer/categorical_data/index.html#ordinal-compared-to-nominal-tests",
    "title": "Categorical Data Analysis",
    "section": "3.3 Ordinal Compared to Nominal Tests",
    "text": "3.3 Ordinal Compared to Nominal Tests\n\nPearson and Likelihood Ratio \\(\\chi^2\\) tests can handle any type of categorical variable\nOrdinal variables provide extra information since order of the categories matters compared to nominal\nCan test for even more with ordinal vars. against other ordinal vars.–whether two ordinal vars. have a linear relationship as compared to just a general one\n\nHypothesis Statements:\n\n\\(H_0\\): No Linear Association\n\\(H_a\\): Linear Association"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#mantel-haenszel-chi2-test",
    "href": "notes/primer/categorical_data/index.html#mantel-haenszel-chi2-test",
    "title": "Categorical Data Analysis",
    "section": "3.4 Mantel-Haenszel \\(\\chi^2\\) Test",
    "text": "3.4 Mantel-Haenszel \\(\\chi^2\\) Test\n\\[\nQ_{MH} = (n - 1)r^2\n\\]\n\n\\(r^2\\) is the Pearson correlation between row and column variables"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#odds-ratio",
    "href": "notes/primer/categorical_data/index.html#odds-ratio",
    "title": "Categorical Data Analysis",
    "section": "4.1 Odds Ratio",
    "text": "4.1 Odds Ratio\nOdds ratio measure how much more likely, with respect to odds, a certain event occurs in one group relative to its occurrence in another group.\nOdds of an event occurring is not the same as the probability that an event occurs.\n\\[\n\\text{Odds} = \\frac{p}{1 - p}\n\\]\n\n4.1.1 Probability vs. Odds of an Outcome\n\n\n\n\nYes\nNo\n\n\n\n\nLoyal\n20\n60\n\n\nNon-Loyal\n10\n90\n\n\n\n\n\nCode\nd = {'yes': [20, 10], 'no': [60, 90]}\ndf_loyalty = pd.DataFrame(d, index=['Loyal', 'Non-Loyal'])\n\ndf_loyalty['prob_y'] = df_loyalty['yes'] / df_loyalty.iloc[:, :2].sum(axis=1)\ndf_loyalty['prob_n'] = df_loyalty['no'] / df_loyalty.iloc[:, :2].sum(axis=1)\ndf_loyalty['odds_y'] = (df_loyalty['prob_y'] / df_loyalty['prob_n']).round(3)\ndf_loyalty['odds_n'] = df_loyalty['prob_n'] / df_loyalty['prob_y']\n\ndf_loyalty.head()\nprint(\n    f'Odds Ratio, Loyal to Non-Loyal: {df_loyalty.loc[\"Loyal\", \"odds_y\"] / df_loyalty.loc[\"Non-Loyal\", \"odds_y\"]}')\n\n\nOdds Ratio, Loyal to Non-Loyal: 3.0\n\n\n\nLoyal program customers have 3 times the odds of buying the product as compared to customers not in the loyalty program."
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#cramers-v",
    "href": "notes/primer/categorical_data/index.html#cramers-v",
    "title": "Categorical Data Analysis",
    "section": "4.2 Cramer’s V",
    "text": "4.2 Cramer’s V\nWhen you have more than &gt;2 categories in one or both variables we use Cramer’s V.\n\\[\nV = \\sqrt{\\frac{(\\frac{Q_P}{n})}{\\min(\\#\\text{Rows} - 1, \\#\\text{Columns} - 1)}}\n\\]\n\nBounded between 0 and 1 (-1 and 1 for 2x2 scenario) where closer to 0 the weaker the relationship"
  },
  {
    "objectID": "notes/primer/categorical_data/index.html#example-1",
    "href": "notes/primer/categorical_data/index.html#example-1",
    "title": "Categorical Data Analysis",
    "section": "4.3 Example",
    "text": "4.3 Example\n\nThe same manager as the previous example now wants to know the strength of the relationship between the color of car and loyalty program. Use the appropriate measure of association to calculate this.\n\n\n\nCode\nn = df_cars['total'].sum()\nrows, cols = df_cars.iloc[:, :2].shape\n\ncramer_v = np.sqrt((q_pearson / n) / np.min([rows - 1, cols - 1]))\nnp.round(cramer_v, 3)\n\n\n0.216"
  },
  {
    "objectID": "notes/primer/fundamental/index.html",
    "href": "notes/primer/fundamental/index.html",
    "title": "Fundamental Statistical Concepts",
    "section": "",
    "text": "There are three main pieces to statistics:"
  },
  {
    "objectID": "notes/primer/fundamental/index.html#common-types-of-bias",
    "href": "notes/primer/fundamental/index.html#common-types-of-bias",
    "title": "Fundamental Statistical Concepts",
    "section": "2.1 Common Types of Bias",
    "text": "2.1 Common Types of Bias\n\nSelection Bias\n\nUndercoverage: frame and population are not equal\nNonresponse: subject in sample cannot / will not respond or be measured\n\nSampling Bias\n\nConvenience Sampling: selecting subjects based on accessibility and ease\nVoluntary sampling: subjects volunteer themselves–may not be representative"
  },
  {
    "objectID": "notes/primer/fundamental/index.html#common-sampling-techniques",
    "href": "notes/primer/fundamental/index.html#common-sampling-techniques",
    "title": "Fundamental Statistical Concepts",
    "section": "2.2 Common Sampling Techniques",
    "text": "2.2 Common Sampling Techniques\n\n2.2.1 Simple Random Sampling (SRS)\nSample items from population such that every possible sample of specified sizes has an equal chance of being selected\n\n\n\nSimple Random Sampling\n\n\nAdvantages\n\nNo statistical bias\nNo prev. info about sample needed ahead of time\n\nDisadvantages\n\nExpensive\nHard to implement\nNeed list of population\n\n\n\n2.2.2 Stratified Random Sampling (STS)\nPopulation is divided into subgroups, called strata, so that each member in the population belongs to only one strata.\nSample items from every strata. The sample size between groups does not need to be the same (e.g. if we know groups in pop. are in a 20:80 ratio)\n\n\n\nStratified Random Sampling\n\n\nAdvantages\n\nSmaller sample sizes can achieve same accuracy as SRS\nMore info about parts of population\n\nDisadvantages\n\nNeed info about population ahead of time to split on\n\n\n\n2.2.3 Cluster Sampling\nSimilar to stratified where you group members of population into subgroups called clusters. You only talk to a sample of \\(m\\) clusters selected randomly.\nIn cluster sampling, you don’t necessarily believe there are differences between clusters.\n\n\n\nCluster Sampling\n\n\nAdvantages\n\nOvercome issues with travel, time, expense\nEasier to implement than SRS or STS\n\nDisadvantages\n\nNeed info about population ahead of time to split on–but not total list.\nMay have slight bias if random clusters aren’t representative\n\n\n\n2.2.4 Systematic Sampling\nSelect every \\(k^{th}\\) item in the populatino after randomly selecting a starting point between 1 and \\(k\\).\n\\(k\\) is determined as a ratio of population size over desired sample size.\n\n\n\nSystematic Sampling Starting Point Selection\n\n\n\n\n\nSystematic Sampling\n\n\nAdvantages\n\nVery easy to get sample\n\nDisadvantages\n\nMay be biased especialy if order of list of population matters"
  },
  {
    "objectID": "notes/primer/fundamental/index.html#example",
    "href": "notes/primer/fundamental/index.html#example",
    "title": "Fundamental Statistical Concepts",
    "section": "2.3 Example",
    "text": "2.3 Example\n\nA large worldwide financial company wnats to develop a new retirement plan for the company. They want to survey different managers of branches around the world to find out the most important strategies the new retirement plan should contain. They have 5000 branches worldwide and want to personally interview these branch managers. They have information about the banch size (small, medium, large) and the state/province location of the branch. They want to talk to 50 branch managers.\nDevelop four separate strategies to sample these branch managers based on the four different statistical sampling techniques discussed previously.\n\n\nSRS: Randomly sample 50 branches to interview the managers of\nSTS: Stratify by size and select random samples from every strata\nCluster: Randomly select sample of states/provinces, then select branches at random from those states/provinces\nSystematic: Select every 100th branch in list of branches starting from a random starting point between 1 and 100\n\n\n\nCode\n# Example of systematic sampling in Python\n\nimport numpy as np\nindexes = np.arange(5000)\nsample_size = 50\nk = len(indexes) // sample_size\n\nprint(f'k: {k}')\nprint(f'Sample Size: {sample_size}')\n\nstart = np.random.randint(k + 1)\nselected = indexes[start::k]\n\nprint(f'Length of selected: {len(selected)}')\nindexes[start::k]\n\n\nk: 100\nSample Size: 50\nLength of selected: 50\n\n\narray([  77,  177,  277,  377,  477,  577,  677,  777,  877,  977, 1077,\n       1177, 1277, 1377, 1477, 1577, 1677, 1777, 1877, 1977, 2077, 2177,\n       2277, 2377, 2477, 2577, 2677, 2777, 2877, 2977, 3077, 3177, 3277,\n       3377, 3477, 3577, 3677, 3777, 3877, 3977, 4077, 4177, 4277, 4377,\n       4477, 4577, 4677, 4777, 4877, 4977])"
  },
  {
    "objectID": "notes/primer/fundamental/index.html#qualitative-vs.-quantitative",
    "href": "notes/primer/fundamental/index.html#qualitative-vs.-quantitative",
    "title": "Fundamental Statistical Concepts",
    "section": "3.1 Qualitative vs. Quantitative",
    "text": "3.1 Qualitative vs. Quantitative\nQuantitative data are numeric data that define value of quantity.\nQualitative data are data whose measurement scale is inherently categorical.\n\nNominal data are categories with no logical ordering\nOrdinal data are categories with a logical order / only two ways to order the categories (binary is ordinal)"
  },
  {
    "objectID": "notes/primer/fundamental/index.html#time-series-vs.-cross-sectional",
    "href": "notes/primer/fundamental/index.html#time-series-vs.-cross-sectional",
    "title": "Fundamental Statistical Concepts",
    "section": "3.2 Time Series vs. Cross-sectional",
    "text": "3.2 Time Series vs. Cross-sectional\nTime series is a set of ordered data values observed at successive point in times. Each row might be indexed by time referring to a dependence in time.\nCross-sectional is a set of data values observed at a fixed point in time or where time is of no significance (could have time as a variable just that the response does not depend on it)."
  },
  {
    "objectID": "notes/primer/index.html",
    "href": "notes/primer/index.html",
    "title": "Primer",
    "section": "",
    "text": "Categorical Data Analysis\n\n\n\n\n\n\n\nprimer\n\n\n\n\n\n\n\n\n\n\n\nJun 14, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of Variance\n\n\n\n\n\n\n\nprimer\n\n\n\n\n\n\n\n\n\n\n\nJun 12, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\n\nprimer\n\n\nhypothesis testing\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\n\nprimer\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 30, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nProbability\n\n\n\n\n\n\n\nprimer\n\n\nprobability\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nFundamental Statistical Concepts\n\n\n\n\n\n\n\nprimer\n\n\n\n\n\n\n\n\n\n\n\nMay 22, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/first_weeks/index.html",
    "href": "blog/first_weeks/index.html",
    "title": "First Weeks: Initial Thoughts",
    "section": "",
    "text": "How would I describe the Institute for Advanced Analytics? It’s fast, it’s scary, and it’s absolutely thrilling.\nComing in as a Master’s student, I’m not sure I knew what to expect entirely. Just a few months ago I had a vastly different vision of trekking out to California to be a full-time art student. But becoming a data science professional?\nIt seemed unlikely.\nTo be honest, I often feel like formal education has been a barrier rather than a support for practical learning. I’m sure many students have shared a similar opinion.\nHere, “education” takes on a vastly different meaning than compared to an undergraduate program. Team work is crucial at the Institute and your success in any assignment, project, and interaction is defined by the success of your team. From day one, it was crystal clear that the faculty were incredibly passionate in both data science and teaching. Genuine smiles, genuine knowledge, genuine experience–it’s hard to beat.\nI love the other students as well even if we might not know each other at all. Everybody brings in a wildly different background. We’re a medley of primary school teachers, engineers, therapists, PhD students and more coming together to bash our brains and personalities together. Despite the differences between all of us, we share a similar drive to solve difficult problems and be the best data professionals we can be. We’re not all friends and we might not have to be, but we all stand to learn something new every day from each other.\nI realize before coming here, I tended to avoid teamwork when I could. Now I actually look forward to it. I find myself using a more analytical mindset to validate approaches, project scope, and data design. Now that I’m working for longer periods of time with a group, I’m appreciating the push-and-pull dynamic that is produced when we are discussing how to move a project forward.\nIt’s not so simple as a “hero” figure pulling ahead to complete the assignment for everyone. Instead, each of us is learning to work with the friction or flow that is produced when trying to manage everyone’s expectations, emotions, work methods, and energies. There’s an ongoing process that we all have a part to play in; we can’t just hear, we need to listen to the thoughts and experiences of our teammates.\nYou also can’t beat the adrenaline rush that comes when we all figured out or won at something.\nIf there’s one key takeaway I have from the Institute it’s that data science can be a new art for me. I often find myself reflecting on what data science means to me. I’ve heard it being compared to art in many ways, but I never felt a coherent connection between the two disciplines until recently.\nI don’t think typing characters on a keyboard elicits the same feeling of dexterity as putting pencil to paper with broad, sweeping strokes. However, data science does have this wonderful ebb and flow between unstructured data and diverse analytics methods to understand the data. No one method is completely correct, but some can resolve the large shapes and lines in the dataset to a coherent story which can be important to me or any one who views the story. I absolutely believe this notion of data agrees with how I feel towards making art."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yang Chen",
    "section": "",
    "text": "Hiya, I’m Yang!\nI’m a data science Master’s student who’s passionate about AI, deep learning, and explainable data science. This is an open repository of notes, projects, and musings that I am gathering through my Master’s at the Institute for Advanced Analytics.\nI’m excited to share what I learn and hope you can join me for the ride! You can find me on LinkedIn, Github or through my main website"
  },
  {
    "objectID": "projects/summer_r/index.html",
    "href": "projects/summer_r/index.html",
    "title": "Getting to Know Your Classmates Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(stringr)\nlibrary(countrycode)\nlibrary(ggthemes)\n\n# setwd(\"summer_r\")\nclassmates &lt;- read.csv(\"data/Get_to_know_survey_2023.csv\")\nglimpse(classmates)\n\nRows: 96\nColumns: 7\n$ Birth_Month_Year        &lt;chr&gt; \"December, 1999\", \"7, 1998\", \"April, 1999\", \"N…\n$ State                   &lt;chr&gt; \"North Carolina\", \"Virginia\", \"North Carolina\"…\n$ Country                 &lt;chr&gt; \"United States\", \"United States\", \"United Stat…\n$ Languages               &lt;chr&gt; \"English\", \"English, Spanish\", \"English\", \"Eng…\n$ When_Hear_About_Program &lt;chr&gt; \"1-2 years\", \"1-2 years\", \"3-5 years\", \"Less t…\n$ How_Hear_About_Program  &lt;chr&gt; \"1. Recommended by professors\", \"2. Recommende…\n$ Hobbies                 &lt;chr&gt; \"Video Games, Sports\", \"Movies, Television Ser…"
  },
  {
    "objectID": "projects/summer_r/index.html#top-5-languages-visualized",
    "href": "projects/summer_r/index.html#top-5-languages-visualized",
    "title": "Getting to Know Your Classmates Data",
    "section": "Top 5 Languages Visualized",
    "text": "Top 5 Languages Visualized\n\nggplot(languages_top_5, aes(x = reorder(Languages, desc(n)), y = n, fill = Languages)) +\n    geom_col() +\n    labs(x = \"Languages\", y = \"Counts\", title = \"MSA Class of 2024: Top 5 Languages\") +\n    theme_economist()"
  },
  {
    "objectID": "projects/summer_r/index.html#wide-to-long-hobbies-column",
    "href": "projects/summer_r/index.html#wide-to-long-hobbies-column",
    "title": "Getting to Know Your Classmates Data",
    "section": "Wide to Long Hobbies Column",
    "text": "Wide to Long Hobbies Column\n\nclassmates &lt;- classmates %&gt;%\n    mutate(Hobbies = str_squish(Hobbies)) %&gt;%\n    separate_longer_delim(Hobbies, \", \")\n\nclassmates\n\n# A tibble: 502 × 9\n   Birth_Month_Year State          Country      Languages When_Hear_About_Prog…¹\n   &lt;chr&gt;            &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;     &lt;chr&gt;                 \n 1 December, 1999   North Carolina United Stat… English   1-2 years             \n 2 December, 1999   North Carolina United Stat… English   1-2 years             \n 3 7, 1998          Virginia       United Stat… English   1-2 years             \n 4 7, 1998          Virginia       United Stat… English   1-2 years             \n 5 7, 1998          Virginia       United Stat… English   1-2 years             \n 6 7, 1998          Virginia       United Stat… English   1-2 years             \n 7 7, 1998          Virginia       United Stat… Spanish   1-2 years             \n 8 7, 1998          Virginia       United Stat… Spanish   1-2 years             \n 9 7, 1998          Virginia       United Stat… Spanish   1-2 years             \n10 7, 1998          Virginia       United Stat… Spanish   1-2 years             \n# ℹ 492 more rows\n# ℹ abbreviated name: ¹​When_Hear_About_Program\n# ℹ 4 more variables: How_Hear_About_Program &lt;chr&gt;, Hobbies &lt;chr&gt;,\n#   Birth_Month &lt;chr&gt;, Birth_Year &lt;chr&gt;\n\nclassmates_copy &lt;- classmates_copy %&gt;%\n    mutate(Hobbies = str_squish(Hobbies)) %&gt;%\n    separate_longer_delim(Hobbies, \", \")\n\nhobbies_top_5 &lt;- classmates_copy %&gt;%\n    filter(!is.na(Hobbies)) %&gt;%\n    count(Hobbies) %&gt;%\n    arrange(desc(n)) %&gt;%\n    head(n = 5)\n\nhobbies_top_5\n\n                          Hobbies  n\n1 Outside Recreational Activities 59\n2                          Sports 50\n3               Television Series 48\n4                          Movies 47\n5                           Music 34"
  },
  {
    "objectID": "projects/summer_r/index.html#top-5-hobbies-visualized",
    "href": "projects/summer_r/index.html#top-5-hobbies-visualized",
    "title": "Getting to Know Your Classmates Data",
    "section": "Top 5 Hobbies Visualized",
    "text": "Top 5 Hobbies Visualized\nNote: Using copy of older classmates data frame since we don’t want to “double-count” student hobbies after making the language column long. Maybe there’s a better way to do this through grouping.\n\nggplot(hobbies_top_5, aes(x = reorder(Hobbies, desc(n)), y = n, fill = Hobbies)) +\n    geom_col() +\n    labs(x = \"Hobbies\", y = \"Counts\", title = \"MSA Class of 2024: Top 5 Hobbies\") +\n    theme_economist()"
  },
  {
    "objectID": "projects/summer_r/index.html#standardizing-birth_month_year",
    "href": "projects/summer_r/index.html#standardizing-birth_month_year",
    "title": "Getting to Know Your Classmates Data",
    "section": "Standardizing Birth_Month_Year",
    "text": "Standardizing Birth_Month_Year\n\nclassmates &lt;- classmates %&gt;%\n    mutate(Birth_Month_Year = paste(Birth_Month, Birth_Year, sep = \", \"))\n\nclassmates %&gt;%\n    select(Birth_Month_Year)\n\n# A tibble: 502 × 1\n   Birth_Month_Year\n   &lt;chr&gt;           \n 1 December, 1999  \n 2 December, 1999  \n 3 July, 1998      \n 4 July, 1998      \n 5 July, 1998      \n 6 July, 1998      \n 7 July, 1998      \n 8 July, 1998      \n 9 July, 1998      \n10 July, 1998      \n# ℹ 492 more rows"
  },
  {
    "objectID": "projects/summer_r/index.html#student-countries-heatmap",
    "href": "projects/summer_r/index.html#student-countries-heatmap",
    "title": "Getting to Know Your Classmates Data",
    "section": "Student Countries Heatmap",
    "text": "Student Countries Heatmap\n\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n\nselected_countries &lt;- classmates %&gt;%\n    select(Country) %&gt;%\n    distinct(Country) %&gt;%\n    arrange(Country)\nworld_data &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nselected_countries_data &lt;- world_data %&gt;%\n    inner_join(selected_countries, by = c(\"name\" = \"Country\"))\n\nggplot() +\n    geom_sf(data = world_data, color = \"grey50\", fill = \"lightblue\") +\n    geom_sf(data = selected_countries_data, fill = \"red\", alpha = 0.7) +\n    labs(\n        title = \"Countries of Origin\",\n        subtitle = \"MSA 2024\",\n        caption = \"Library: Natural Earth\"\n    ) +\n    theme_minimal()"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "First Weeks: Initial Thoughts\n\n\n\n\n\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/primer/probability/index.html",
    "href": "notes/primer/probability/index.html",
    "title": "Probability",
    "section": "",
    "text": "Probability is a numerical measure of the likelihood of that event’s occurrence. It takes on a value in the interval \\([0, 1]\\).\nA sample space is a collection of all possible outcomes in a random process. The sum of all probabilities in the sample space must sum to 1.\nAn event is a collection of one or more outcomes from a process whose result cannot be predicted. The probability of event \\(X\\) is expressed as \\(P(X)\\)\n\n\n\n\nEvent consisting of all sample points that are not in \\(A\\).\n\\[\nP(\\bar{A}) = 1 - P(A)\n\\]\n\n\n\nThe union of an event \\(A\\) and event \\(B\\) is the event containing all sample points in \\(A\\) or \\(B\\) or both.\n\\[\nA \\cup B\n\\]\n\n\n\nAll sample points that are in both \\(A\\) and \\(B\\).\n\\[\nA \\cap B\n\\]\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\n\n\n\nMutually exclusive meants that events have no sample points in common–they do not intersect.\nThe events cannot both occur which turns the addition law into\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nProbability of an event given that another event has occurred.\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\n\nWe derive this from the conditional property which expresses the intersection as a multiplication:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(A | B) \\cdot P(B) \\\\\n&= P(B | A) \\cdot P(A)\n\\end{align*}\n\\]\nIf events are independent then this becomes\n\\[\nP(A \\cap B) = P(A) \\cdot P(B)\n\\]"
  },
  {
    "objectID": "notes/primer/probability/index.html#basic-relationships",
    "href": "notes/primer/probability/index.html#basic-relationships",
    "title": "Probability",
    "section": "",
    "text": "Event consisting of all sample points that are not in \\(A\\).\n\\[\nP(\\bar{A}) = 1 - P(A)\n\\]\n\n\n\nThe union of an event \\(A\\) and event \\(B\\) is the event containing all sample points in \\(A\\) or \\(B\\) or both.\n\\[\nA \\cup B\n\\]\n\n\n\nAll sample points that are in both \\(A\\) and \\(B\\).\n\\[\nA \\cap B\n\\]\n\n\n\n\\[\nP(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n\\]\n\n\n\nMutually exclusive meants that events have no sample points in common–they do not intersect.\nThe events cannot both occur which turns the addition law into\n\\[\nP(A \\cup B) = P(A) + P(B)\n\\]\n\n\n\nProbability of an event given that another event has occurred.\n\\[\nP(A | B) = \\frac{P(A \\cap B)}{P(B)}\n\\]\n\n\n\nWe derive this from the conditional property which expresses the intersection as a multiplication:\n\\[\n\\begin{align*}\nP(A \\cap B) &= P(A | B) \\cdot P(B) \\\\\n&= P(B | A) \\cdot P(A)\n\\end{align*}\n\\]\nIf events are independent then this becomes\n\\[\nP(A \\cap B) = P(A) \\cdot P(B)\n\\]"
  },
  {
    "objectID": "notes/primer/probability/index.html#conditional-vs.-marginal-probabilities",
    "href": "notes/primer/probability/index.html#conditional-vs.-marginal-probabilities",
    "title": "Probability",
    "section": "2.1 Conditional vs. Marginal Probabilities",
    "text": "2.1 Conditional vs. Marginal Probabilities\nMarginal probabilities are considered unconditional probabilities–they are probabilities of events without any condition.\n\n2.1.1 Example\n\nLet’s take a look at promotion of people at a company with advanced degrees vs. those who don’t have them.\n\n\n\nCode\nimport pandas as pd\n\nd = {'yes': {'promoted': 288, 'not_promoted': 672},\n     'no': {'promoted': 36, 'not_promoted': 204}}\n\ndf = pd.DataFrame(d)\ndf\n\n\n\n\n\n\n\n\n\nyes\nno\n\n\n\n\npromoted\n288\n36\n\n\nnot_promoted\n672\n204\n\n\n\n\n\n\n\nOne marginal probability is the probability of an advanced degree:\n\n\nCode\nprob_adv_deg = df['yes'].sum() / (df['yes'].sum() + df['no'].sum())\nprob_adv_deg\n\n\n0.8\n\n\nOn the other hand, a conditional probability might be the probability of a promotion given that the employee has no advanced degree:\n\n\nCode\ncond_prob = df.loc['promoted', 'no'] / df['no'].sum()\ncond_prob\n\n\n0.15"
  },
  {
    "objectID": "notes/primer/confidence_intervals/index.html",
    "href": "notes/primer/confidence_intervals/index.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Point estimators cannot be expected to provide exact values of population parameters. Intervals provide information about how close the point estimate is to the value of the parameter.\nConfidence intervals are interval estimates where we have a certain level of confidence in the interval.\nWhat does it mean when we are 95% confidence that the population mean is between 20 and 30?"
  },
  {
    "objectID": "notes/primer/confidence_intervals/index.html#student-t-distribution",
    "href": "notes/primer/confidence_intervals/index.html#student-t-distribution",
    "title": "Confidence Intervals",
    "section": "3.1 Student t Distribution",
    "text": "3.1 Student t Distribution\nThe t distribution is also symmetric, but has thicker tails than the Normal distribution.\nIt has \\(n - 1\\) degrees of freedom where the degrees of freedom are the number of independent pieces of information that go into the computation of \\(s\\).\nFor larger samples, the t distribution is approximately the standard Normal distribution.\n\n\nCode\nx = np.linspace(-4, 4, 100)\n\npdf = t.pdf(x, df=19)\nplt.plot(x, pdf)\n\n\n\n\n\nPlot of a t distribution with d.f. 19\n\n\n\n\nThe confidence interval for \\(\\bar{x}\\) is calculated as:\n\\[\n\\bar{x} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}}\n\\]\nFor large samples (\\(n \\geq 50\\)) we can calculate the confidence interval for the mean from any population.\nFor small samples (\\(n &lt; 50\\)) we need to assume the population follows a Normal distribution."
  },
  {
    "objectID": "notes/primer/hypothesis_testing/index.html",
    "href": "notes/primer/hypothesis_testing/index.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "According to the CLT, the the mean has a sampling distribution that follows a Normal distribution as long as the sample size is large enough.\nLet’s take the average age of our customers as an example. Initially, we believe that the average age of our customers is \\(\\mu = 25\\) years old with standard deviation \\(\\sigma = 10\\).\nWe sample 100 customers and collect their age.\n\n\nCode\nfrom scipy.stats import t\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\nmean = 25\nsd = 1\nalpha = 0.05\n\nx = np.linspace(20, 35, 1000)\n\ncrit_value = norm.ppf(1 - alpha / 2, loc=mean, scale=sd)\npdf = norm.pdf(x, loc=mean, scale=sd)\nplt.axvline(31, color='r')\n\nplt.plot(x, pdf)\n\n\n\n\n\nWhat is the probability we see a sample mean \\(\\bar{x} = 31\\) under our original assumption that the mean is 25?\nIt’s very low! \\(&lt; 0.0001\\) so we have to strongly question our original hypothesis. We may no longer think it is true given the data."
  },
  {
    "objectID": "notes/primer/hypothesis_testing/index.html#type-i-error",
    "href": "notes/primer/hypothesis_testing/index.html#type-i-error",
    "title": "Hypothesis Testing",
    "section": "7.1 Type I Error",
    "text": "7.1 Type I Error\n\nReject the null hypothesis when the null hypothesis was actually true (False rejection)\nProbability of making a Type I error in a hypothesis test is called the significance level"
  },
  {
    "objectID": "notes/primer/hypothesis_testing/index.html#type-ii-error",
    "href": "notes/primer/hypothesis_testing/index.html#type-ii-error",
    "title": "Hypothesis Testing",
    "section": "7.2 Type II Error",
    "text": "7.2 Type II Error\n\nAccepting the null hypothesis when the null hypothesis was actually false (False acceptance)\nProbability of not making a Type II error in a hypothesis test is called the power\nDifficult to control Type II error\nCan only control for Type I or Type II at a time"
  },
  {
    "objectID": "notes/primer/hypothesis_testing/index.html#conditions",
    "href": "notes/primer/hypothesis_testing/index.html#conditions",
    "title": "Hypothesis Testing",
    "section": "9.1 Conditions",
    "text": "9.1 Conditions\n\nThe hypothesis test is two-sided\n\\(C = 1 - \\alpha\\) where \\(C\\) is the confidence level and \\(\\alpha\\) is the significance level"
  },
  {
    "objectID": "notes/primer/anova/index.html",
    "href": "notes/primer/anova/index.html",
    "title": "Analysis of Variance",
    "section": "",
    "text": "One sample hypothesis tests are focused on one population parameter. However, sometimes we would like to compare multiple parameters against each other. This is the foundation of an analysis called analysis of variance (ANOVA).\nRecall the one-sample case:\n\\[\nH_0: \\mu\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ \\ \\mu_0\n\\]\n\\[\nH_a: \\mu\n\\begin{cases}\n&lt; \\\\\n\\neq \\\\\n&gt;\n\\end{cases}\n\\mu_0\n\\]\nIn the two-sample case, there are two parameters we are calculating so now we have an expression:\n\\[\nH_0: \\mu_1 - \\mu_2\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ 0\n\\]\n\\[\nH_a: \\mu_1 - \\mu_2\n\\begin{cases}\n&lt; \\\\\n\\neq \\\\\n&gt;\n\\end{cases}\n\\ 0\n\\]\n\n\n\nAssume two samples are independent of each other\nWe have to take into account whether the variances are equal or not\n\nDifferent hypothesis test structures depends on whether or not variances are equal\n\n\nRecall that our general test statistic is calculated as\n\\[\n\\begin{align*}\n\\text{Test Statistic} &= \\frac{\\text{Statistic} - \\text{Null Value}}{\\text{Standard Error}} \\\\\n\\ \\\\\n&= \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\end{align*}\n\\]\n\n\\(s_p\\) is the pooled standard deviation\n\nWe then calculate our p-value based on the t-distribution with \\(d.f. = n_1 - 1 + n_2 - 1 = n_1 + n_2 - 2\\)\n\n\n\nUnder assumption of equal variances we have two estimates of population variance–\\(s_1^2\\) and \\(s_2^2\\). We should combine both to get our estimate:\n\\[\n\\begin{align*}\ns_p &= \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}} \\\\\n\\ \\\\\n&= \\sqrt{\\frac{\\sum (x_{1,i} - \\bar{x}_1)^2 + \\sum (x_{2,i} - \\bar{x}_2)^2}{n_1 + n_2 - 2}}\n\\end{align*}\n\\]\n\n\n\nEach population has an approximate Normal distribution\nVariances of two groups are equal\n\n\n\n\n\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2} \\times s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]\n\n\n\nHypothesis Statements:\n\nSame as the prior tests\n\nOur Test Statistic:\n\\[\n\\text{Test Statistic} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\nStandard error changes since we need to test estimates of our two separate population variances separately \\(\\rightarrow\\) cannot “pool” them\n\nThe degrees of freedom on our t-test are a more complicated expression:\n\\[\nd.f. = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1 - 1} + \\frac{(\\frac{s_2^2}{n_2})^2}{n_2 - 1}}\n\\]\n\n\n\nFor different variances, we don’t use the pooled variance\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2}^* \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\n\nStandard error changes using separate population variances\n\n\n\n\n\nA human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager assumes the variability of salaries between genders is different, but wants to test if makes have higher average salary than females. The manager samples 62 males and 77 females. The sample of males had an average salary of $87,547 with a s.d. of $5,910. The sample of females had an average salary of $78,289 with a s.d. of $6,276. Run a hypothesis test.\n\n\n\nCode\n# H_a: mu_1 &gt; mu_2 with mu_1 representing mean of males\n\nt &lt;- ((87547 - 78289) - 0) / sqrt(5910^2 / 62 + 6276^2 / 77)\nsprintf(\"Test statistic equals %.3f\", t)\n\n\n[1] \"Test statistic equals 8.930\"\n\n\nCode\ndf &lt;- (5910^2 / 62 + 6276^2 / 77)^2 / ((5910^2 / 62)^2 / 61 + (6276^2 / 77)^2 / 76)\nsprintf(\"Df: %d\", floor(df))\n\n\n[1] \"Df: 133\"\n\n\nCode\npt(t, df, lower.tail = FALSE)\n\n\n[1] 1.468658e-15"
  },
  {
    "objectID": "notes/primer/anova/index.html#assumptions",
    "href": "notes/primer/anova/index.html#assumptions",
    "title": "Analysis of Variance",
    "section": "",
    "text": "Assume two samples are independent of each other\nWe have to take into account whether the variances are equal or not\n\nDifferent hypothesis test structures depends on whether or not variances are equal\n\n\nRecall that our general test statistic is calculated as\n\\[\n\\begin{align*}\n\\text{Test Statistic} &= \\frac{\\text{Statistic} - \\text{Null Value}}{\\text{Standard Error}} \\\\\n\\ \\\\\n&= \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\end{align*}\n\\]\n\n\\(s_p\\) is the pooled standard deviation\n\nWe then calculate our p-value based on the t-distribution with \\(d.f. = n_1 - 1 + n_2 - 1 = n_1 + n_2 - 2\\)"
  },
  {
    "objectID": "notes/primer/anova/index.html#pooled-standard-deviation",
    "href": "notes/primer/anova/index.html#pooled-standard-deviation",
    "title": "Analysis of Variance",
    "section": "",
    "text": "Under assumption of equal variances we have two estimates of population variance–\\(s_1^2\\) and \\(s_2^2\\). We should combine both to get our estimate:\n\\[\n\\begin{align*}\ns_p &= \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}} \\\\\n\\ \\\\\n&= \\sqrt{\\frac{\\sum (x_{1,i} - \\bar{x}_1)^2 + \\sum (x_{2,i} - \\bar{x}_2)^2}{n_1 + n_2 - 2}}\n\\end{align*}\n\\]\n\n\n\nEach population has an approximate Normal distribution\nVariances of two groups are equal"
  },
  {
    "objectID": "notes/primer/anova/index.html#confidence-interval",
    "href": "notes/primer/anova/index.html#confidence-interval",
    "title": "Analysis of Variance",
    "section": "",
    "text": "\\[\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2} \\times s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n\\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#testing-difference-in-means---unequal-variances",
    "href": "notes/primer/anova/index.html#testing-difference-in-means---unequal-variances",
    "title": "Analysis of Variance",
    "section": "",
    "text": "Hypothesis Statements:\n\nSame as the prior tests\n\nOur Test Statistic:\n\\[\n\\text{Test Statistic} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\n\nStandard error changes since we need to test estimates of our two separate population variances separately \\(\\rightarrow\\) cannot “pool” them\n\nThe degrees of freedom on our t-test are a more complicated expression:\n\\[\nd.f. = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1 - 1} + \\frac{(\\frac{s_2^2}{n_2})^2}{n_2 - 1}}\n\\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#confidence-interval-1",
    "href": "notes/primer/anova/index.html#confidence-interval-1",
    "title": "Analysis of Variance",
    "section": "",
    "text": "For different variances, we don’t use the pooled variance\n\\[\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2}^* \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n\\]\n\nStandard error changes using separate population variances"
  },
  {
    "objectID": "notes/primer/anova/index.html#example---comparing-two-means",
    "href": "notes/primer/anova/index.html#example---comparing-two-means",
    "title": "Analysis of Variance",
    "section": "",
    "text": "A human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager assumes the variability of salaries between genders is different, but wants to test if makes have higher average salary than females. The manager samples 62 males and 77 females. The sample of males had an average salary of $87,547 with a s.d. of $5,910. The sample of females had an average salary of $78,289 with a s.d. of $6,276. Run a hypothesis test.\n\n\n\nCode\n# H_a: mu_1 &gt; mu_2 with mu_1 representing mean of males\n\nt &lt;- ((87547 - 78289) - 0) / sqrt(5910^2 / 62 + 6276^2 / 77)\nsprintf(\"Test statistic equals %.3f\", t)\n\n\n[1] \"Test statistic equals 8.930\"\n\n\nCode\ndf &lt;- (5910^2 / 62 + 6276^2 / 77)^2 / ((5910^2 / 62)^2 / 61 + (6276^2 / 77)^2 / 76)\nsprintf(\"Df: %d\", floor(df))\n\n\n[1] \"Df: 133\"\n\n\nCode\npt(t, df, lower.tail = FALSE)\n\n\n[1] 1.468658e-15"
  },
  {
    "objectID": "notes/primer/anova/index.html#example---comparing-two-variances",
    "href": "notes/primer/anova/index.html#example---comparing-two-variances",
    "title": "Analysis of Variance",
    "section": "2.1 Example - Comparing Two Variances",
    "text": "2.1 Example - Comparing Two Variances\n\nA human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager has no assumption about the variability of salaries between genders, but wants to test if makes have higher average salary than females. The manager samples 62 males and 77 females. The sample of males had an average salary of $87,547 with a s.d. of $5,910. The sample of females had an average salary of $78,289 with a s.d. of $6,276. Run a hypothesis test.\nNeed to first test if variances are equal or not before running test of means\n\n\n\nCode\nf &lt;- 6276^2 / 5910^2\nsprintf(\"F Statistic: %.3f\", f)\n\n\n[1] \"F Statistic: 1.128\"\n\n\nCode\ndf1 &lt;- 76\ndf2 &lt;- 61\n\npf(f, df1, df2, lower.tail = FALSE)\n\n\n[1] 0.3147624\n\n\nAt a significance level of 0.05 we would not reject the null hypothesis that our variances are different."
  },
  {
    "objectID": "notes/primer/anova/index.html#example---sources-of-variation",
    "href": "notes/primer/anova/index.html#example---sources-of-variation",
    "title": "Analysis of Variance",
    "section": "3.1 Example - Sources of Variation",
    "text": "3.1 Example - Sources of Variation\n\nYou have SAT scores for both boys and girls from a local school\nYou believe that the boys and girls have the same avg. test score, but want to test otherwise\nOf the 39 females, 32 of them are part of the accelerated math and language arts program\nOf the 39 males, 11 of them are part of the accelerated math and language arts program\n\nMatched samples are samples selected such that each data value from one sample is related (or matched / paired) with a corresponding data value from a second sample\nIn the previous example, we would match boys and girls who were in the accelerated program and ones who were not.\nOur focus turns from individual values in the populations and to the values of the differences in the populations. All assumptions and calculations are done on the differences, not individual samples.\n\\[\nH_0: \\mu_d\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ \\ D_0\n\\]\n\\[\nH_a: \\mu_d\n\\begin{cases}\n&lt; \\\\\n\\neq \\\\\n&gt;\n\\end{cases}\n\\ \\ D_0\n\\]\n\nAssumptions for matched pairs hypothesis test are same as for regular hypothesis test for means\nLarge sample (\\(n &gt; 50\\)) of differences\nSmall samples with differences having Normal distribution\n\nHypothesis Statements:\n\\[\nH_0: \\mu_d\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ \\ D_0\n\\hspace{2cm}\nH_a: \\mu_d\n\\begin{cases}\ntest\n\\end{cases}\n\\]\nTest Statistic:\n\\[\nt = \\frac{\\bar{x}_d - D_0}{\\frac{s_d}{\\sqrt{n_d}}}\n\\]\n\\[ d.f. = n_d - 1 \\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#confidence-interval-2",
    "href": "notes/primer/anova/index.html#confidence-interval-2",
    "title": "Analysis of Variance",
    "section": "3.2 Confidence Interval",
    "text": "3.2 Confidence Interval\n\\[\n\\bar{x}_d \\pm t_{\\alpha/2}^* \\times \\frac{s_d}{\\sqrt{n_d}}\n\\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#example---paired-samples",
    "href": "notes/primer/anova/index.html#example---paired-samples",
    "title": "Analysis of Variance",
    "section": "3.3 Example - Paired Samples",
    "text": "3.3 Example - Paired Samples\n\nA human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager samples 51 pairs of male and female employees where the pair has the same job title and experience at the company. The average difference in salaries is $2,131 with a s.d. of differences of $7,898. Run a hypothesis test.\n\n\n\nCode\nt &lt;- (2131 - 0) / (7898 / sqrt(51))\nsprintf(\"Test statistic: %.3f\", t)\n\n\n[1] \"Test statistic: 1.927\"\n\n\nCode\ndf &lt;- 50\n\npt(t, df, lower.tail = FALSE)\n\n\n[1] 0.02984447\n\n\nAt a significant level of 0.05 we reject the null hypothesis that there is no gender bias in the pay scale of employees at the company."
  },
  {
    "objectID": "notes/primer/anova/index.html#confidence-interval-3",
    "href": "notes/primer/anova/index.html#confidence-interval-3",
    "title": "Analysis of Variance",
    "section": "4.1 Confidence Interval",
    "text": "4.1 Confidence Interval\n\\[\n(p_1 - p_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 - p_2)}{n_2}}\n\\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#comprehensive-example",
    "href": "notes/primer/anova/index.html#comprehensive-example",
    "title": "Analysis of Variance",
    "section": "4.2 Comprehensive Example",
    "text": "4.2 Comprehensive Example\n\nA researcher at a large university on the west coast is interested in comparing some factors between upperclassmen (juniors and seniors) and underclassmen (freshmen and sophomores) in the undergraduate school. The researcher believes that more experience in college may help students perform better in the classroom. The researcher is interested in testing if the average GPA of upperclassmen is greater than the average GPA of underclassmen. The researcher sampled 89 underclassmen with an average GPA of 2.75 with a s.d. of 0.91 and 102 upperclassmen with an average GPA of 3.07 and a s.d. of 1.02.\n\n\n\nThe researcher did not use matched sampling. Do you agree with their decision?\n\n\nNo. There are other factors that can influence GPA like major that we could match on for equal comparison\n\n\nConduct a hypothesis test on the variances to see if they are equal.\n\n\n\\[\nH_0: \\frac{\\sigma_1^2}{\\sigma_2^2} \\leq 1\n\\hspace{2cm}\nH_a: \\frac{\\sigma_1^2}{\\sigma_2^2} &gt; 1\n\\]\n\n\nCode\nf &lt;- 1.02^2 / 0.91^2\nsprintf(\"F Statistic: %0.3f\", f)\n\n\n[1] \"F Statistic: 1.256\"\n\n\nCode\ndf1 &lt;- 101\ndf2 &lt;- 88\n\npf(f, df1, df2, lower.tail = FALSE)\n\n\n[1] 0.1367661\n\n\nBased on a significance level of 0.05 we do not reject the null hypothesis. We do not have enough evidence to say that the variances between the two populations is different.\n\n\nConduct the appropriate hypothesis test on the means to see if they are equal.\n\n\n\n\nCode\ns_p &lt;- sqrt(((101 * 1.02^2) + (88 * 0.91^2)) / (102 + 89 - 2))\nsprintf(\"Pooled std. dev: %0.3f\", s_p)\n\n\n[1] \"Pooled std. dev: 0.970\"\n\n\nCode\nt &lt;- ((3.07 - 2.75) - 0) / (s_p * sqrt(1 / 102 + 1 / 89))\nsprintf(\"Test Statistic: %0.3f\", t)\n\n\n[1] \"Test Statistic: 2.274\"\n\n\nCode\npt(t, 89 + 102 - 2, lower.tail = FALSE)\n\n\n[1] 0.01205843\n\n\nAt a significance level of 0.05 we reject our null hypothesis that more experience in college does not lead to higher GPA performance."
  },
  {
    "objectID": "notes/primer/anova/index.html#example-continued",
    "href": "notes/primer/anova/index.html#example-continued",
    "title": "Analysis of Variance",
    "section": "4.3 Example Continued",
    "text": "4.3 Example Continued\n\nSame researcher as before also believes that a higher proportion of upperclassmen live off campus compared to the proportion of underclassmen. While sampling the students in the previous sample, the researcher also asked whether the student lived off campus. Of the 89 underclassmen sampled, 27 lived off campus. Of the 102 upperclassmen sampled, 65 lived off campus.\n\n\nConstruct a 95% confidence interval for the difference between the proportion of upperclassmen living off campus to the proportion of underclassmen living off campus.\n\n\\[\nH_0: p_1 - p_2 \\leq 0 \\hspace{1cm} H_a: p_1 - p_2 &gt; 0\n\\]\n\n\nCode\np1 &lt;- 65 / 102\np2 &lt;- 27 / 89\nsprintf(\"Upper p: %0.3f, Under p: %0.3f\", p1, p2)\n\n\n[1] \"Upper p: 0.637, Under p: 0.303\"\n\n\nCode\np_mean &lt;- (102 * p1 + 89 * p2) / (102 + 89)\nsprintf(\"p_mean: %0.3f\", p_mean)\n\n\n[1] \"p_mean: 0.482\"\n\n\nCode\nz &lt;- ((p1 - p2) - 0) / sqrt(p_mean * (1 - p_mean) * (1 / 102 + 1 / 89))\nsprintf(\"Z Statistic: %0.3f\", z)\n\n\n[1] \"Z Statistic: 4.607\"\n\n\nCode\nz_crit &lt;- qnorm(0.05 / 2, lower.tail = FALSE)\nz_crit\n\n\n[1] 1.959964\n\n\nCode\nmargin &lt;- z_crit * sqrt(p1 * (1 - p1) / 102 + p2 * (1 - p2) / 89)\nmargin\n\n\n[1] 0.1335203\n\n\nCode\nsprintf(\"95 perc. CI: %0.3f plus-minus %0.3f\", p1 - p2, margin)\n\n\n[1] \"95 perc. CI: 0.334 plus-minus 0.134\"\n\n\n\nConduct the appropriate hypothesis test to test the researcher’s claim.\n\n\\[\nH_0: p_1 - p_2 \\leq 0 \\hspace{1cm} H_a: p_1 - p_2 &gt; 0\n\\]\n\n\nCode\npnorm(z, lower.tail = FALSE)\n\n\n[1] 2.044913e-06\n\n\nAt a significance level of 0.0005 we reject the null hypothesis that the proportion of upperclassmen living off campus is not greater than the proportion of underclassmen living off campus.\n\nCan you compare the confidence interval and the hypothesis test?\n\nNo as the hypothesis test is one-sided while the confidence interval is two-sided."
  },
  {
    "objectID": "notes/primer/anova/index.html#one-way-anova",
    "href": "notes/primer/anova/index.html#one-way-anova",
    "title": "Analysis of Variance",
    "section": "5.1 One-Way ANOVA",
    "text": "5.1 One-Way ANOVA\nSimplest form of ANOVA is the one-way model.\n\nIndependent samples are obtained from \\(k\\) levels (categories) of a single factor (explanatory variable), then testing whether the \\(k\\) levels have equal means.\nSimilar to regression analysis in that we have one categorical variable predicting continuous response\n\n\\[\nH_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\n\\]\n\\[\nH_a: \\text{At least one mean different than another}\n\\]\n\n5.1.1 Assumptions\n\nNormally distributed categories\nEquality of variances between categories\nIndependence\n\nTest Statistic:\n\\[\nF = \\frac{s_{max}^2}{s_{min}^2}\n\\]\nThe p-value is calculated from Hartley-s F-max distribution which isn’t covered here.\n\n\n5.1.2 Sources of Variation\n\nWithin-Sample Variability\n\nVariability in response that exists within category of a variable\n\nWhat you categories cannot explain (like SSE)\n\n\nBetween-Sample Variability\n\nVariability in response that exists between categories of a variable\n\nWhat you categories can explain (like SSR)\n\n\n\n\n\n5.1.3 Sum of Squares Within\nWithin sample is variability that you cannot explain by just knowing which category your observation falls into\n\\[\nSSW = \\sum_{i=1}^k\\sum_{j=1}^{n_i} (x_{i,j} - \\bar{x}_i)^2\n\\]\n\n\n5.1.4 Sum of Squares Between\nBetween sample is variability that you can explain by just knowing which category your observation falls into\n\\[\nSSB = \\sum_{i=1}^k n_i(\\bar{x}_i - \\bar{\\bar{x}})^2\n\\]"
  },
  {
    "objectID": "notes/primer/anova/index.html#partitioning-variability-in-anova",
    "href": "notes/primer/anova/index.html#partitioning-variability-in-anova",
    "title": "Analysis of Variance",
    "section": "5.2 Partitioning Variability in ANOVA",
    "text": "5.2 Partitioning Variability in ANOVA\n\n\n\n\nflowchart LR\nA(Total Variability) --&gt; B(SSR + SSE)\nB --&gt; C[Variability Between Groups]\nB --&gt; D[Variability Within Groups]"
  },
  {
    "objectID": "notes/primer/anova/index.html#anova-f-test",
    "href": "notes/primer/anova/index.html#anova-f-test",
    "title": "Analysis of Variance",
    "section": "5.3 ANOVA F-test",
    "text": "5.3 ANOVA F-test\n\\[\n\\begin{align*}\nH_0&: \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_a&: \\text{At least one mean different than another}\n\\end{align*}\n\\]\nTest follows an F-distribution and is calculated as\n\\[\nF = \\frac{(\\frac{SSB}{k - 1})}{(\\frac{SSW}{N - k})}\n\\]\n\n\\(k\\) categories would be \\(k - 1\\) variables in a regression model\n\\(N\\) is the total sample size across all categories"
  },
  {
    "objectID": "notes/primer/anova/index.html#one-way-anova-table",
    "href": "notes/primer/anova/index.html#one-way-anova-table",
    "title": "Analysis of Variance",
    "section": "5.4 One-Way ANOVA Table",
    "text": "5.4 One-Way ANOVA Table\n\n\n\nOne-Way ANOVA Table"
  },
  {
    "objectID": "notes/primer/anova/index.html#one-way-anova-example",
    "href": "notes/primer/anova/index.html#one-way-anova-example",
    "title": "Analysis of Variance",
    "section": "5.5 One-Way ANOVA Example",
    "text": "5.5 One-Way ANOVA Example\n\nA marketing analyst is interested in testing the effectiveness of 4 different commercials describing their company’s new product. The marketing analyst randomly assigns a commercial to each of 32 cities across the country and measures the average increase in sales of their new product at their stores. The marketing analyst wants to test if there is a difference in sales between the commercials.\n\n\n\nFill in the blanks on the ANOVA table.\n\n\n\n\n\nSource\nDF\nSS\nMS\nF-Value\nP-Value\n\n\n\n\nBetween\n3\n2.3236\n0.775\n22.794\n\n\n\nWithin\n28\n0.9587\n0.034\n\n\n\n\nTotal\n31\n3.2823\n\n\n\n\n\n\n\n\nCode\npf(22.794, 3, 28, lower.tail = FALSE)\n\n\n[1] 1.128339e-07"
  },
  {
    "objectID": "notes/primer/anova/index.html#multiple-comparisons-problem",
    "href": "notes/primer/anova/index.html#multiple-comparisons-problem",
    "title": "Analysis of Variance",
    "section": "6.1 Multiple Comparisons Problem",
    "text": "6.1 Multiple Comparisons Problem\n\nYou have a test which makes an error 5% of the time when performed.\n\n\nWhat is the probability of making an error on your first test?\n\n5%\n\nWhat is the probability of making an error on your second test?\n\n5%\n\nWhat is the probability of making at least one error in two tests?\n\n9.75%\n\n\n\n6.1.1 Different Types of Error\nComparison-wise error rate\n\nError rate for each individual test or comparison\n\nExperiment-wise error rate\n\nError rate across all comparisons–proportion of experiments/comparisons in which at least one error occurs\n\nTests and confidence intervals usually control for comparison-wise, \\(\\alpha\\), but ideally want to control for experiment-wise."
  },
  {
    "objectID": "notes/primer/anova/index.html#multiple-comparison-methods",
    "href": "notes/primer/anova/index.html#multiple-comparison-methods",
    "title": "Analysis of Variance",
    "section": "6.2 Multiple Comparison Methods",
    "text": "6.2 Multiple Comparison Methods\n\n\n\n\n\n\n\n\nNumber of Groups Compared\nNumber of Comparisons\nExperimentwise Error Rate\n\n\n\n\n2\n1\n0.05\n\n\n3\n3\n0.14\n\n\n4\n6\n0.26\n\n\n5\n10\n0.40\n\n\n\n\n\\(EER \\leq 1 - (1 - \\alpha)^{nc}\\) where \\(nc\\) is the number of comparisons\n\n\n\n\n\nflowchart LR\nA(Control Comparisonwise Error Rate) --&gt; B(Pairwise t-tests)\nC(Control Experimentwise Error Rate) --&gt; D[Compare All Pairs Tukey]"
  },
  {
    "objectID": "notes/primer/anova/index.html#tukeys-hsd-test",
    "href": "notes/primer/anova/index.html#tukeys-hsd-test",
    "title": "Analysis of Variance",
    "section": "6.3 Tukey’s HSD Test",
    "text": "6.3 Tukey’s HSD Test\nHSD represents the Honest Significant Difference or Critical Range\nWe use Tukey’s when we consider pairwise comparisons\n\nExperimentwise error rate is equal to \\(\\alpha\\) when all pairwise comparisons are considered\nExperimentwise error rate is less than \\(\\alpha\\) when fewer than all pairwise comparisons are considered\nReplaces margin of error calculation for a typical confidence interval for a difference in means with an adjusted margin of error\n\n\\[\n\\text{Critical Range (Margin of Error)} = q_a \\times \\sqrt{\\frac{MSW}{2} \\times (\\frac{1}{n_i} + \\frac{1}{n_j})}\n\\]\n\n\\(q_a\\) is from studentized range distribution"
  },
  {
    "objectID": "notes/primer/anova/index.html#randomized-blocking",
    "href": "notes/primer/anova/index.html#randomized-blocking",
    "title": "Analysis of Variance",
    "section": "7.1 Randomized Blocking",
    "text": "7.1 Randomized Blocking\n\n7.1.1 Sources of Variation\nGenerally, comparing many population means works well in certain situations\nThere are some instances where blocking is used to control for sources of variation that might distort conclusions"
  },
  {
    "objectID": "notes/primer/anova/index.html#example",
    "href": "notes/primer/anova/index.html#example",
    "title": "Analysis of Variance",
    "section": "7.2 Example",
    "text": "7.2 Example\n\nThe same marketing analyst as before is interested in testing the effectiveness of 4 different commercials describing their company’s new product. The marketing analyst randomly assigns a commercial to each of 32 cities across the country and measures the average increase in sales of their new product at their stores. The four commercial average sales were $1.2M for commercial A, $1.8M for B, $0.76M for C, and $1.3M for D. Where are the differences in sales?\n\n\nWhat if the new product is a warm coat and a majority of the cities seeing C were warm weather cities?\nThe same marketing analyst as before is interested in testing the effectiveness of 4 different commercials describing their company’s new product. Split (block) country into 8 regions. Show each commercial to one city in each region. Sample size still 32."
  },
  {
    "objectID": "notes/primer/anova/index.html#assumptions-2",
    "href": "notes/primer/anova/index.html#assumptions-2",
    "title": "Analysis of Variance",
    "section": "7.3 Assumptions",
    "text": "7.3 Assumptions\nSame as One-Way ANOVA:\n\nNormally distributed categories\nEquality of variances between categories\nIndependence\n\nBlocking can come from collection of data as well as the analysis of the data as a variable being added to the model.\nWhen a new variable is added, we get a new source of variation–sum of squares of blocking."
  },
  {
    "objectID": "notes/primer/anova/index.html#sum-of-squares-blocks",
    "href": "notes/primer/anova/index.html#sum-of-squares-blocks",
    "title": "Analysis of Variance",
    "section": "7.4 Sum of Squares Blocks",
    "text": "7.4 Sum of Squares Blocks\n\\[\nSSBL = \\sum_{j=1}^b k(\\bar{x}_j - \\bar{\\bar{x}})^2\n\\]\nThe sum of squares comes out of the error sum of squares and gets brought into the model–the SSW shrinks even more."
  },
  {
    "objectID": "notes/primer/anova/index.html#blocking-anova-table",
    "href": "notes/primer/anova/index.html#blocking-anova-table",
    "title": "Analysis of Variance",
    "section": "7.5 Blocking ANOVA Table",
    "text": "7.5 Blocking ANOVA Table\n\n\n\nBlocking ANOVA Table\n\n\n\nThe F-Value in the Blocking row is the F-test with \\(H_a\\) at least one block mean not equal"
  },
  {
    "objectID": "notes/primer/anova/index.html#post-hoc-analysis-for-blocking",
    "href": "notes/primer/anova/index.html#post-hoc-analysis-for-blocking",
    "title": "Analysis of Variance",
    "section": "7.6 Post-hoc Analysis for blocking",
    "text": "7.6 Post-hoc Analysis for blocking\nTukey-Kramer ANOVA comparisons do not work for blocking.\nInstead, we have Fisher’s Least Significant Difference. Fisher’s LSD is a recalculation of the margin of error for the difference in means confidence interval just like Tukey’s critical range.\n\\[\nLSD = t^* \\times \\sqrt{MSW} \\times \\sqrt{\\frac{2}{b}}\n\\]"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html",
    "href": "notes/programming/python/practicum-prep/index.html",
    "title": "Pandas Primer 1",
    "section": "",
    "text": "All Python Data Scientists Currently"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#indexing-into-series",
    "href": "notes/programming/python/practicum-prep/index.html#indexing-into-series",
    "title": "Pandas Primer 1",
    "section": "2.1 Indexing into Series",
    "text": "2.1 Indexing into Series\nNotice how by default each entry in the series is labeled from 0 to \\(n - 1\\). This is the index. We can set our own index for the Series and retrieve values using the index:\n\n\nCode\nseries = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\nprint(series)\nprint(series['b'])\nprint(series[['a', 'b']])\n\n\na    1\nb    2\nc    3\ndtype: int64\n2\na    1\nb    2\ndtype: int64\n\n\nThe index is ordered according to the data, so we can also slice using the index. Unlike Python lists, both ends are inclusive:\n\n\nCode\nprint(series['a':'b'])\n\n# This is similar to saying list[2:0]\nprint(series['b':'a'])\n\n\na    1\nb    2\ndtype: int64\nSeries([], dtype: int64)"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#series-are-similar-to-numpy-arrays",
    "href": "notes/programming/python/practicum-prep/index.html#series-are-similar-to-numpy-arrays",
    "title": "Pandas Primer 1",
    "section": "2.2 Series Are Similar to Numpy Arrays!",
    "text": "2.2 Series Are Similar to Numpy Arrays!\nYou can use Boolean arrays and do vectorized operations like scalar multiplication just like with numpy arrays.\n\n\nCode\nseries = pd.Series(np.arange(9))\n\nprint(series[series &gt; 5])\nprint(series * 5)\nprint(np.square(series))\nprint(series.argmax())\n\n\n6    6\n7    7\n8    8\ndtype: int64\n0     0\n1     5\n2    10\n3    15\n4    20\n5    25\n6    30\n7    35\n8    40\ndtype: int64\n0     0\n1     1\n2     4\n3     9\n4    16\n5    25\n6    36\n7    49\n8    64\ndtype: int64\n8"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#converting-python-dictionaries-to-series",
    "href": "notes/programming/python/practicum-prep/index.html#converting-python-dictionaries-to-series",
    "title": "Pandas Primer 1",
    "section": "2.3 Converting Python Dictionaries to Series",
    "text": "2.3 Converting Python Dictionaries to Series\nWe can provide a dictionary as a Series which automatically converts the dictionary keys into the index:\n\n\nCode\nd = {\"Apples\": 2, \"Bananas\": 5, \"Oranges\": 18}\nseries = pd.Series(d)\n\n\nAnd then back into a dictionary with the to_dict method:\n\n\nCode\nseries.to_dict()\n\n\n{'Apples': 2, 'Bananas': 5, 'Oranges': 18}"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#series-in-data-analysis",
    "href": "notes/programming/python/practicum-prep/index.html#series-in-data-analysis",
    "title": "Pandas Primer 1",
    "section": "2.4 Series in Data Analysis",
    "text": "2.4 Series in Data Analysis\nChecking for missing values:\n\n\nCode\nseries = pd.Series(d, index = [\"Apples\", \"Bananas\", \"Mangoes\"])\nprint(series.isna())\nprint(series.notna())\n\n\nApples     False\nBananas    False\nMangoes     True\ndtype: bool\nApples      True\nBananas     True\nMangoes    False\ndtype: bool\n\n\nWhen you add Series together, it automatically aligns the labels together and NAs the values where the indexes are not the same.\n\n\nCode\nd = {\"Apples\": 60, \"Bananas\": 20, \"Durian\": 10, \"Avocados\": 10}\nseries2 = pd.Series(d)\n\nseries + series2\n\n\nApples      62.0\nAvocados     NaN\nBananas     25.0\nDurian       NaN\nMangoes      NaN\ndtype: float64"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#series-name",
    "href": "notes/programming/python/practicum-prep/index.html#series-name",
    "title": "Pandas Primer 1",
    "section": "2.5 Series Name",
    "text": "2.5 Series Name\nA Series index and its data values have a special name attribute which integrates with other functionality in pandas:\n\n\nCode\nseries.name = \"count\"\nseries.index.name = \"fruit\"\nseries\n\n\nfruit\nApples     2.0\nBananas    5.0\nMangoes    NaN\nName: count, dtype: float64"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#viewing-the-first-n-rows-last-n-rows",
    "href": "notes/programming/python/practicum-prep/index.html#viewing-the-first-n-rows-last-n-rows",
    "title": "Pandas Primer 1",
    "section": "3.1 Viewing the First N Rows, Last N Rows",
    "text": "3.1 Viewing the First N Rows, Last N Rows\n\n\nCode\nn = 2\nprint(df.head(n))\nprint(df.tail(n))\n\n\n   is_student  name  score\n0        True  Bill     10\n1       False  Jill     20\n   is_student                  name  score\n3        True  Cythulianmixseth III     20\n4        True                 Jonny     10"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#specifying-columns",
    "href": "notes/programming/python/practicum-prep/index.html#specifying-columns",
    "title": "Pandas Primer 1",
    "section": "3.2 Specifying Columns",
    "text": "3.2 Specifying Columns\nJust like Series, you can specify the names of columns. If the column names exist, then the DataFrame will be ordered by the order you specified them. Otherwise, the column will be inserted with NA values.\n\n\nCode\ndf2 = pd.DataFrame(data, columns = [\"name\", \"score\", \"is_student\"])\ndf2\n\n\n\n\n\n\n\n\n\nname\nscore\nis_student\n\n\n\n\n0\nBill\n10\nTrue\n\n\n1\nJill\n20\nFalse\n\n\n2\nDavis\n30\nTrue\n\n\n3\nCythulianmixseth III\n20\nTrue\n\n\n4\nJonny\n10\nTrue\n\n\n\n\n\n\n\n\n\nCode\ndf2 = pd.DataFrame(data, columns=[\"a\", \"b\", \"is_student\"])\ndf2\n\n\n\n\n\n\n\n\n\na\nb\nis_student\n\n\n\n\n0\nNaN\nNaN\nTrue\n\n\n1\nNaN\nNaN\nFalse\n\n\n2\nNaN\nNaN\nTrue\n\n\n3\nNaN\nNaN\nTrue\n\n\n4\nNaN\nNaN\nTrue"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#retrieving-columns",
    "href": "notes/programming/python/practicum-prep/index.html#retrieving-columns",
    "title": "Pandas Primer 1",
    "section": "3.3 Retrieving Columns",
    "text": "3.3 Retrieving Columns\nTo retrieve columns, use dictionary-like notation or use the dot notation.\n\n\nCode\nprint(df[\"is_student\"])\nprint(df.is_student)\n\n\n0     True\n1    False\n2     True\n3     True\n4     True\nName: is_student, dtype: bool\n0     True\n1    False\n2     True\n3     True\n4     True\nName: is_student, dtype: bool\n\n\n\n\n\n\n\n\nDot Notation\n\n\n\nThe main downside of dot notation is that you can’t use it if it’s not a valid Python variable name, conflicts with method names in DataFrame, or has whitespaces. Learn to not rely on dot notation."
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#modifying-columns-adding-columns",
    "href": "notes/programming/python/practicum-prep/index.html#modifying-columns-adding-columns",
    "title": "Pandas Primer 1",
    "section": "3.4 Modifying Columns, Adding Columns",
    "text": "3.4 Modifying Columns, Adding Columns\nYou can change column values by assigning a new value. If you assign a Series, then the labels will be aligned to the DataFrame’s index and insert NA where index values are not present:\n\n\nCode\ndf[\"score\"] = 1.\ndf\n\n\n\n\n\n\n\n\n\nis_student\nname\nscore\n\n\n\n\n0\nTrue\nBill\n1.0\n\n\n1\nFalse\nJill\n1.0\n\n\n2\nTrue\nDavis\n1.0\n\n\n3\nTrue\nCythulianmixseth III\n1.0\n\n\n4\nTrue\nJonny\n1.0\n\n\n\n\n\n\n\n\n\nCode\nval = pd.Series([2, 2], index=[2, 4])\ndf[\"score\"] = val\ndf\n\n\n\n\n\n\n\n\n\nis_student\nname\nscore\n\n\n\n\n0\nTrue\nBill\nNaN\n\n\n1\nFalse\nJill\nNaN\n\n\n2\nTrue\nDavis\n2.0\n\n\n3\nTrue\nCythulianmixseth III\nNaN\n\n\n4\nTrue\nJonny\n2.0\n\n\n\n\n\n\n\nNote that any column retrieved from a DataFrame is a view on the data, not a copy. Any modifications to this Series will be reflected in the DataFrame.\n\n\nCode\nscore = df[\"score\"]\nscore[0] = 1.\n\n# DataFrame is modified as a result of changing the view\ndf\n\n\n/var/folders/7s/plv7qn294jg_4c9nxj_69_380000gn/T/ipykernel_94512/2353297564.py:2: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nis_student\nname\nscore\n\n\n\n\n0\nTrue\nBill\n1.0\n\n\n1\nFalse\nJill\nNaN\n\n\n2\nTrue\nDavis\n2.0\n\n\n3\nTrue\nCythulianmixseth III\nNaN\n\n\n4\nTrue\nJonny\n2.0"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#transpose",
    "href": "notes/programming/python/practicum-prep/index.html#transpose",
    "title": "Pandas Primer 1",
    "section": "3.5 Transpose",
    "text": "3.5 Transpose\nTo transpose the data:\n\n\nCode\ndf.T\n\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nis_student\nTrue\nFalse\nTrue\nTrue\nTrue\n\n\nname\nBill\nJill\nDavis\nCythulianmixseth III\nJonny\n\n\nscore\n1.0\nNaN\n2.0\nNaN\n2.0\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nTransposing discards column data types of columns do not all have the same type. Transposing back may lose previous type information."
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#dataframe-column-and-index-names",
    "href": "notes/programming/python/practicum-prep/index.html#dataframe-column-and-index-names",
    "title": "Pandas Primer 1",
    "section": "3.6 DataFrame Column and Index Names",
    "text": "3.6 DataFrame Column and Index Names\nJust like Series, you can name the index and columns of the DataFrame:\n\n\nCode\ndf.index.name = \"student\"\ndf.columns.name = \"information\"\ndf\n\n\n\n\n\n\n\n\ninformation\nis_student\nname\nscore\n\n\nstudent\n\n\n\n\n\n\n\n0\nTrue\nBill\n1.0\n\n\n1\nFalse\nJill\nNaN\n\n\n2\nTrue\nDavis\n2.0\n\n\n3\nTrue\nCythulianmixseth III\nNaN\n\n\n4\nTrue\nJonny\n2.0"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#reindex",
    "href": "notes/programming/python/practicum-prep/index.html#reindex",
    "title": "Pandas Primer 1",
    "section": "5.1 Reindex",
    "text": "5.1 Reindex\nWe can modify the Index of a Series or DataFrame using reindex:\n\n\nCode\nseries2 = series.reindex(['a', 'b', 'c', 'd'])\nseries2\n\n\na   NaN\nb   NaN\nc   NaN\nd   NaN\ndtype: float64\n\n\nWe can also fill interpolate values when reindexing with a small subset of labels:\n\n\nCode\nseries = pd.Series([\"blue\", \"purple\", \"yellow\"], index=[0, 2, 4])\nseries.reindex(np.arange(6), method=\"ffill\")\n\n\n0      blue\n1      blue\n2    purple\n3    purple\n4    yellow\n5    yellow\ndtype: object"
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#drop-values",
    "href": "notes/programming/python/practicum-prep/index.html#drop-values",
    "title": "Pandas Primer 1",
    "section": "5.2 Drop Values",
    "text": "5.2 Drop Values\n\n\nCode\nseries = pd.Series(np.arange(5.0), index=[\"a\", \"b\", \"c\", \"d\", \"e\"])\nseries_drop = series.drop(\"c\")\n\nseries_drop\n\n\na    0.0\nb    1.0\nd    3.0\ne    4.0\ndtype: float64\n\n\nWithin a DataFrame, we can drop from the rows or the columns using the axis argument:\n\n\nCode\ndf = pd.DataFrame(\n    np.arange(16).reshape((4, 4)),\n    index=[\"Ohio\", \"Colorado\", \"Utah\", \"New York\"],\n    columns=[\"one\", \"two\", \"three\", \"four\"],\n)\n\ndf\n\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nOhio\n0\n1\n2\n3\n\n\nColorado\n4\n5\n6\n7\n\n\nUtah\n8\n9\n10\n11\n\n\nNew York\n12\n13\n14\n15\n\n\n\n\n\n\n\nTo drop from rows:\n\n\nCode\ndf.drop(['Colorado', 'Ohio'])\n\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nUtah\n8\n9\n10\n11\n\n\nNew York\n12\n13\n14\n15\n\n\n\n\n\n\n\nTo drop from columns:\n\n\nCode\ndf.drop(['two'], axis=1)\n\n\n\n\n\n\n\n\n\none\nthree\nfour\n\n\n\n\nOhio\n0\n2\n3\n\n\nColorado\n4\n6\n7\n\n\nUtah\n8\n10\n11\n\n\nNew York\n12\n14\n15\n\n\n\n\n\n\n\nKeep in mind these return a copy of a DataFrame by default. If you need to save these results then assign them to a variable."
  },
  {
    "objectID": "notes/programming/python/practicum-prep/index.html#indexing-selection-filtering",
    "href": "notes/programming/python/practicum-prep/index.html#indexing-selection-filtering",
    "title": "Pandas Primer 1",
    "section": "5.3 Indexing, Selection, Filtering",
    "text": "5.3 Indexing, Selection, Filtering\n\n5.3.1 Indexing\nThe preferred way to select data by Index labels is by using the special loc operator:\n\n\nCode\nseries = pd.Series(np.arange(4.0), index=[\"a\", \"b\", \"c\", \"d\"])\n\nseries.loc[[\"b\", \"a\", \"d\"]]\n\n\nb    1.0\na    0.0\nd    3.0\ndtype: float64\n\n\n\n\n\n\n\n\nBook Note\n\n\n\nThe reason to prefer loc is because of the different treatment of integers when indexing with []. Regular []-based indexing will treat integers as labels if the index contains integers, so the behavior differs depending on the data type of the index. - Wes McKinney\n\n\n\n\nCode\nseries = pd.Series([1, 2, 3], index=[2, 0, 1])\nseries2 = pd.Series([1, 2, 3], index=['a', 'b', 'c'])\n\nprint(series[[0, 1, 2]])\nprint(series2[[0, 1, 2]])\n\n\n0    2\n1    3\n2    1\ndtype: int64\na    1\nb    2\nc    3\ndtype: int64\n\n\n/var/folders/7s/plv7qn294jg_4c9nxj_69_380000gn/T/ipykernel_94512/4284678342.py:5: FutureWarning:\n\nSeries.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n\n\n\nNote how in the first indexing call the result is executed according to the actual value of the labels, not the integer position. In the second call, the result is executed according to the integer position because the Index is not integer-based. loc removes this ambiguity by throwing an error if the Index does not contain integers.\nWe can also do integer-based indexing using iloc:\n\n\nCode\nseries2.iloc[0:2]\n\n\na    1\nb    2\ndtype: int64\n\n\n\n\n\n\n\n\nBook Note\n\n\n\nIt can be a common newbie error to try to call loc or iloc like functions rather than “indexing into” them with square brackets. The square bracket notation is used to enable slice operations and to allow for indexing on multiple axes with DataFrame objects. - Wes McKinney\n\n\n\n\n5.3.2 Filtering\nThe most common and convenient way to do filtering in pandas is to supply a Boolean array. We can also rows that meet a certain condition using Boolean arrays:\n\n\nCode\ndf[df &lt; 5] = 9999\n\ndf\n\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nOhio\n9999\n9999\n9999\n9999\n\n\nColorado\n9999\n5\n6\n7\n\n\nUtah\n8\n9\n10\n11\n\n\nNew York\n12\n13\n14\n15\n\n\n\n\n\n\n\n\n\n5.3.3 Selection with loc and iloc\nNow here’s the juicy part. Remember our SQL lab with Python? Remember having to write several sets of brackets to get the rows and then the columns. loc and iloc allow you to do both at the same time similar to how we select values in R dataframes:\n\n\nCode\n# Selecting specific indexes\ndf.loc[['Colorado', 'New York']]\n\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nColorado\n9999\n5\n6\n7\n\n\nNew York\n12\n13\n14\n15\n\n\n\n\n\n\n\n\n\nCode\n# Selecting a specific index and two specific columns\ndf.loc[\"Colorado\", [\"two\", \"three\"]]\n\n\ntwo      5\nthree    6\nName: Colorado, dtype: int64\n\n\nAnd now with iloc:\n\n\nCode\n# Selecting indexes based on integer position\ndf.iloc[[2, 1]]\n\n\n\n\n\n\n\n\n\none\ntwo\nthree\nfour\n\n\n\n\nUtah\n8\n9\n10\n11\n\n\nColorado\n9999\n5\n6\n7\n\n\n\n\n\n\n\n\n\nCode\n# Selecting a specific index and three specific columns using integers\ndf.iloc[2, [3, 0, 1]]\n\n\nfour    11\none      8\ntwo      9\nName: Utah, dtype: int64\n\n\nWe can use Boolean arrays with loc but not iloc. Here is an example with our SQL lab:\n\n\nCode\nfrom pathlib import Path\n\npath = Path(\"data\")\ndf = pd.read_csv(path / \"AWProduct.csv\", low_memory=False)\n\n# Select ProductName, DealerPrice where DealerPrice &gt; 10\ndf.loc[df[\"DealerPrice\"] &gt; 10, [\"ProductName\", \"DealerPrice\"]].head(10)\n\n\n\n\n\n\n\n\n\nProductName\nDealerPrice\n\n\n\n\n211\nSport-100 Helmet, Red\n20.1865\n\n\n212\nSport-100 Helmet, Red\n20.1865\n\n\n213\nSport-100 Helmet, Red\n20.9940\n\n\n214\nSport-100 Helmet, Black\n20.1865\n\n\n215\nSport-100 Helmet, Black\n20.1865\n\n\n216\nSport-100 Helmet, Black\n20.9940\n\n\n219\nSport-100 Helmet, Blue\n20.1865\n\n\n220\nSport-100 Helmet, Blue\n20.1865\n\n\n221\nSport-100 Helmet, Blue\n20.9940\n\n\n225\nLong-Sleeve Logo Jersey, S\n28.8404\n\n\n\n\n\n\n\nIf we need to select a specific value by row and column label:\n\n\nCode\ndf.at[211, \"ProductName\"]\n\n\n'Sport-100 Helmet, Red'\n\n\nOr by integer position:\n\n\nCode\ndf.iat[211, 0]\n\n\n212"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro_visualization/index.html",
    "href": "notes/programming/R/r4ds/intro_visualization/index.html",
    "title": "r4ds: Visualization Introduction",
    "section": "",
    "text": "The book focuses on utilizing ggplot2 to build out data visualizations. The underlying system of the package is the grammar of graphics which builds up visuals through a layered approach of defined components.\nGrammar of graphics has a layered hierarchy of components:\nWe load the tidyverse package to have ggplot available to us in our workspace.\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nCode\n# Dataset and colorblind color palette\nlibrary(palmerpenguins)\nlibrary(ggthemes)"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro_visualization/index.html#terms",
    "href": "notes/programming/R/r4ds/intro_visualization/index.html#terms",
    "title": "r4ds: Visualization Introduction",
    "section": "1.1 Terms",
    "text": "1.1 Terms\n\nVariable is quantity, quality, property that can be measured.\nValue is the state of variable when it is measured. Value may change from measurement to measurement.\nObservations are measurements made under similar conditions. Observations contain several values for different variables. Sometimes called a data point.\nTabular data organizes values according to their variables and an observation. Considered tidy if every value is placed in its own cell, every variable in its own column, each observation on a row.\n\n\n\n\nIndex\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\n0\n1\nApple\n\n\n1\n2\nBanana\n\n\n\nTo view a dataframe / tibble:\n\n\nCode\npenguins\n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nUse glimpse for a transposed view of the data. This function helps us view the different variables we have in our dataset.\n\n\nCode\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro_visualization/index.html#initial-ggplot",
    "href": "notes/programming/R/r4ds/intro_visualization/index.html#initial-ggplot",
    "title": "r4ds: Visualization Introduction",
    "section": "2.1 Initial ggplot",
    "text": "2.1 Initial ggplot\nBegin a plot by calling the ggplot function along with the data you have.\n\n\nCode\nggplot(data = penguins)\n\n\n\n\n\nThe next layer in the hierarchy is aesthetics. We now add an additional layer that maps visual properties to our dataset variables. We can map x to flipper length and y to body mass.\n\n\nCode\nggplot(penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g))\n\n\n\n\n\nNext layer in the hierarchy is what actually plots the data. We define a geom to initialize a geometric object to present data. A point geom is created using geom_point.\n\n\nCode\nggplot(penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) +\n    geom_point()\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nRelationship appears to be positive between flipper length and body mass from the plot!"
  },
  {
    "objectID": "notes/programming/R/r4ds/intro_visualization/index.html#aesthetics-and-layers",
    "href": "notes/programming/R/r4ds/intro_visualization/index.html#aesthetics-and-layers",
    "title": "r4ds: Visualization Introduction",
    "section": "2.2 Aesthetics and Layers",
    "text": "2.2 Aesthetics and Layers\nWe can assign a variable to different parameters in the aes function to have ggplot automatically assign unique values of an aesthetic to a unique level of the variable.\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n    geom_point()\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "notes/programming/index.html",
    "href": "notes/programming/index.html",
    "title": "Programming",
    "section": "",
    "text": "Pandas Primer 1\n\n\n\n\n\n\n\nprogramming\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nr4ds: Introduction\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nYang Chen\n\n\n\n\n\n\n  \n\n\n\n\nr4ds: Visualization Introduction\n\n\n\n\n\n\n\nprogramming\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nYang Chen\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "notes/programming/sql/Python+SQL - Lab Files/Pandas + SQL - Exercise.html",
    "href": "notes/programming/sql/Python+SQL - Lab Files/Pandas + SQL - Exercise.html",
    "title": "Load the AWProduct.csv file into a Pandas Dataframe. Name the Dataframe df",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom pandasql import sqldf\nimport numpy as np\n\n\n\n\nCode\ndf = pd.read_csv(\"AWProduct.csv\", low_memory=False)\n\n\n\n0.0.1 Using SQL: Display the first 10 rows of the Dataframe. Only show the following columns: ProductKey, ProductAlternateKey, ProductSubcategoryKey, ProductName\n\n\nCode\nsqldf(\"select ProductKey, ProductALternateKey, ProductSubcategoryKey, ProductName from df limit 10\")\n\n\n\n\n\n\n\n\n\nProductKey\nProductAlternateKey\nProductSubcategoryKey\nProductName\n\n\n\n\n0\n1\nAR-5381\nNone\nAdjustable Race\n\n\n1\n2\nBA-8327\nNone\nBearing Ball\n\n\n2\n3\nBE-2349\nNone\nBB Ball Bearing\n\n\n3\n4\nBE-2908\nNone\nHeadset Ball Bearings\n\n\n4\n5\nBL-2036\nNone\nBlade\n\n\n5\n6\nCA-5965\nNone\nLL Crankarm\n\n\n6\n7\nCA-6738\nNone\nML Crankarm\n\n\n7\n8\nCA-7457\nNone\nHL Crankarm\n\n\n8\n9\nCB-2903\nNone\nChainring Bolts\n\n\n9\n10\nCN-6137\nNone\nChainring Nut\n\n\n\n\n\n\n\n\n\n0.0.2 Using Pandas: Display the first 10 rows of the Dataframe. Only show the following columns: ProductKey, ProductAlternateKey, ProductSubcategoryKey, ProductName\n\n\nCode\ndf[['ProductKey', 'ProductAlternateKey', 'ProductSubcategoryKey', 'ProductName']].head(10)\n\n\n\n\n\n\n\n\n\nProductKey\nProductAlternateKey\nProductSubcategoryKey\nProductName\n\n\n\n\n0\n1\nAR-5381\nNaN\nAdjustable Race\n\n\n1\n2\nBA-8327\nNaN\nBearing Ball\n\n\n2\n3\nBE-2349\nNaN\nBB Ball Bearing\n\n\n3\n4\nBE-2908\nNaN\nHeadset Ball Bearings\n\n\n4\n5\nBL-2036\nNaN\nBlade\n\n\n5\n6\nCA-5965\nNaN\nLL Crankarm\n\n\n6\n7\nCA-6738\nNaN\nML Crankarm\n\n\n7\n8\nCA-7457\nNaN\nHL Crankarm\n\n\n8\n9\nCB-2903\nNaN\nChainring Bolts\n\n\n9\n10\nCN-6137\nNaN\nChainring Nut\n\n\n\n\n\n\n\n\n\n0.0.3 Using SQL: Display ProductName, DealerPrice only for those rows where the DealerPrice &gt; 10. Limit your output to only 10 rows\n\n\nCode\nsqldf(\"select ProductName, DealerPrice from df where DealerPrice &gt; 10 limit 10;\")\n\n\n\n\n\n\n\n\n\nProductName\nDealerPrice\n\n\n\n\n0\nSport-100 Helmet, Red\n20.1865\n\n\n1\nSport-100 Helmet, Red\n20.1865\n\n\n2\nSport-100 Helmet, Red\n20.9940\n\n\n3\nSport-100 Helmet, Black\n20.1865\n\n\n4\nSport-100 Helmet, Black\n20.1865\n\n\n5\nSport-100 Helmet, Black\n20.9940\n\n\n6\nSport-100 Helmet, Blue\n20.1865\n\n\n7\nSport-100 Helmet, Blue\n20.1865\n\n\n8\nSport-100 Helmet, Blue\n20.9940\n\n\n9\nLong-Sleeve Logo Jersey, S\n28.8404\n\n\n\n\n\n\n\n\n\n0.0.4 Using Pandas: Display ProductName, DealerPrice only for those rows where the DealerPrice &gt; 10. Limit your output to only 10 rows\n\n\nCode\ndf.loc[df['DealerPrice'] &gt; 10, ['ProductName', 'DealerPrice']].head(10)\n\n\n\n\n\n\n\n\n\nProductName\nDealerPrice\n\n\n\n\n211\nSport-100 Helmet, Red\n20.1865\n\n\n212\nSport-100 Helmet, Red\n20.1865\n\n\n213\nSport-100 Helmet, Red\n20.9940\n\n\n214\nSport-100 Helmet, Black\n20.1865\n\n\n215\nSport-100 Helmet, Black\n20.1865\n\n\n216\nSport-100 Helmet, Black\n20.9940\n\n\n219\nSport-100 Helmet, Blue\n20.1865\n\n\n220\nSport-100 Helmet, Blue\n20.1865\n\n\n221\nSport-100 Helmet, Blue\n20.9940\n\n\n225\nLong-Sleeve Logo Jersey, S\n28.8404\n\n\n\n\n\n\n\n\n\n0.0.5 Using SQL: Display ProductName, DealerPrice, Color only for those rows where the DealerPrice &gt; 10 AND Color = Silver. Limit your output to only 10 rows\n\n\nCode\nsqldf(\"select ProductName, DealerPrice, Color from df where DealerPrice &gt; 10 and Color = 'Silver' limit 10\")\n\n\n\n\n\n\n\n\n\nProductName\nDealerPrice\nColor\n\n\n\n\n0\nHL Mountain Frame - Silver, 42\n722.5949\nSilver\n\n\n1\nHL Mountain Frame - Silver, 42\n744.2727\nSilver\n\n\n2\nHL Mountain Frame - Silver, 42\n818.7000\nSilver\n\n\n3\nHL Mountain Frame - Silver, 44\n818.7000\nSilver\n\n\n4\nHL Mountain Frame - Silver, 48\n818.7000\nSilver\n\n\n5\nHL Mountain Frame - Silver, 46\n722.5949\nSilver\n\n\n6\nHL Mountain Frame - Silver, 46\n744.2727\nSilver\n\n\n7\nHL Mountain Frame - Silver, 46\n818.7000\nSilver\n\n\n8\nHL Mountain Frame - Silver, 38\n722.5949\nSilver\n\n\n9\nHL Mountain Frame - Silver, 38\n744.2727\nSilver\n\n\n\n\n\n\n\n\n\n0.0.6 Using Pandas: Display ProductName, DealerPrice, Color only for those rows where the DealerPrice &gt; 10 AND Color = Silver. Limit your output to only 10 rows\n\n\nCode\ndf.loc[(df['DealerPrice'] &gt; 10) & (df['Color'] == 'Silver'), ['ProductName', 'DealerPrice', 'Color']].head(10)\n\n\n\n\n\n\n\n\n\nProductName\nDealerPrice\nColor\n\n\n\n\n287\nHL Mountain Frame - Silver, 42\n722.5949\nSilver\n\n\n288\nHL Mountain Frame - Silver, 42\n744.2727\nSilver\n\n\n289\nHL Mountain Frame - Silver, 42\n818.7000\nSilver\n\n\n290\nHL Mountain Frame - Silver, 44\n818.7000\nSilver\n\n\n291\nHL Mountain Frame - Silver, 48\n818.7000\nSilver\n\n\n292\nHL Mountain Frame - Silver, 46\n722.5949\nSilver\n\n\n293\nHL Mountain Frame - Silver, 46\n744.2727\nSilver\n\n\n294\nHL Mountain Frame - Silver, 46\n818.7000\nSilver\n\n\n306\nHL Mountain Frame - Silver, 38\n722.5949\nSilver\n\n\n307\nHL Mountain Frame - Silver, 38\n744.2727\nSilver\n\n\n\n\n\n\n\n\n\n0.0.7 Using SQL: Group by color. Find the ListPrice AVERAGE per color.\n\n\nCode\nsqldf(\"select Color, avg(ListPrice) as ListPrice_Avg from df group by Color\")\n\n\n\n\n\n\n\n\n\nColor\nListPrice_Avg\n\n\n\n\n0\nNone\n56.470650\n\n\n1\nBlack\n791.372805\n\n\n2\nBlue\n860.105300\n\n\n3\nGrey\n125.000000\n\n\n4\nMulti\n48.930378\n\n\n5\nRed\n1176.605463\n\n\n6\nSilver\n1113.593927\n\n\n7\nSilver/Black\n64.018571\n\n\n8\nWhite\n9.245000\n\n\n9\nYellow\n918.114130\n\n\n\n\n\n\n\n\n\n0.0.8 Using Pandas: Group by color. Find the ListPrice AVERAGE per color.\n\n\nCode\ndf.groupby('Color', dropna=False).agg({\"ListPrice\": 'mean'})\n\n\n\n\n\n\n\n\n\nListPrice\n\n\nColor\n\n\n\n\n\nBlack\n791.372805\n\n\nBlue\n860.105300\n\n\nGrey\n125.000000\n\n\nMulti\n48.930378\n\n\nRed\n1176.605463\n\n\nSilver\n1113.593927\n\n\nSilver/Black\n64.018571\n\n\nWhite\n9.245000\n\n\nYellow\n918.114130\n\n\nNaN\n56.470650\n\n\n\n\n\n\n\n\n\n0.0.9 Load the AWProductSubcategory.csv file into a Pandas Dataframe. Name the Dataframe df_s\n\n\nCode\ndf_s = pd.read_csv(\"AWProductSubcategory.csv\", low_memory=False)\n\n\n\n\n0.0.10 Using SQL: Join the df and df_s Dataframes by the ProductSubcategoryKey column. Display ProductAlternateKey, ProductName, ProductSubcategoryKey, ProductCategory, ProductSubcategoryName. Limit your output to only 10 rows\n\n\nCode\nsqldf('select ProductAlternateKey, ProductName, df.ProductSubcategoryKey, ProductCategory, ProductSubcategoryName from df inner join df_s on df.ProductSubCategoryKey = df_s.ProductSubCategoryKey order by ProductAlternateKey limit 10')\n\n\n\n\n\n\n\n\n\nProductAlternateKey\nProductName\nProductSubcategoryKey\nProductCategory\nProductSubcategoryName\n\n\n\n\n0\nBB-7421\nLL Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n1\nBB-8107\nML Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n2\nBB-9108\nHL Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n3\nBC-M005\nMountain Bottle Cage\n28.0\nAccessories\nBottles and Cages\n\n\n4\nBC-R205\nRoad Bottle Cage\n28.0\nAccessories\nBottles and Cages\n\n\n5\nBK-M18B-40\nMountain-500 Black, 40\n1.0\nBikes\nMountain Bikes\n\n\n6\nBK-M18B-42\nMountain-500 Black, 42\n1.0\nBikes\nMountain Bikes\n\n\n7\nBK-M18B-44\nMountain-500 Black, 44\n1.0\nBikes\nMountain Bikes\n\n\n8\nBK-M18B-48\nMountain-500 Black, 48\n1.0\nBikes\nMountain Bikes\n\n\n9\nBK-M18B-52\nMountain-500 Black, 52\n1.0\nBikes\nMountain Bikes\n\n\n10\nBK-M18S-40\nMountain-500 Silver, 40\n1.0\nBikes\nMountain Bikes\n\n\n11\nBK-M18S-42\nMountain-500 Silver, 42\n1.0\nBikes\nMountain Bikes\n\n\n\n\n\n\n\n\n\n0.0.11 Using Pandas: Join the df and df_s Dataframes by the ProductSubcategoryKey column. Display ProductAlternateKey, ProductName, ProductSubcategoryKey, ProductCategory, ProductSubcategoryName. Limit your output to only 10 rows\n\n\nCode\ndf_join = pd.merge(df, df_s, on='ProductSubcategoryKey').sort_values('ProductAlternateKey').head(10)\ndf_join[['ProductAlternateKey', 'ProductName', 'ProductSubcategoryKey', 'ProductCategory', 'ProductSubcategoryName']]\n\n\n\n\n\n\n\n\n\nProductAlternateKey\nProductName\nProductSubcategoryKey\nProductCategory\nProductSubcategoryName\n\n\n\n\n394\nBB-7421\nLL Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n395\nBB-8107\nML Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n396\nBB-9108\nHL Bottom Bracket\n5.0\nComponents\nBottom Brackets\n\n\n312\nBC-M005\nMountain Bottle Cage\n28.0\nAccessories\nBottles and Cages\n\n\n313\nBC-R205\nRoad Bottle Cage\n28.0\nAccessories\nBottles and Cages\n\n\n240\nBK-M18B-40\nMountain-500 Black, 40\n1.0\nBikes\nMountain Bikes\n\n\n241\nBK-M18B-42\nMountain-500 Black, 42\n1.0\nBikes\nMountain Bikes\n\n\n242\nBK-M18B-44\nMountain-500 Black, 44\n1.0\nBikes\nMountain Bikes\n\n\n243\nBK-M18B-48\nMountain-500 Black, 48\n1.0\nBikes\nMountain Bikes\n\n\n244\nBK-M18B-52\nMountain-500 Black, 52\n1.0\nBikes\nMountain Bikes"
  },
  {
    "objectID": "notes/communication/062623/index.html",
    "href": "notes/communication/062623/index.html",
    "title": "Communications: 06/26/2023",
    "section": "",
    "text": "Make eye contact\n\nIf you’re uncomfortable making eye contact, you can look at the hairline or some other area near the face\n\nPracticing natural gestures\n\nYou can practice holding something heavy while you’re pointing in a presentation\nIf you fidget with your hands, press thumb into the joint of the middle finger\n\nAlways make sure to find something interesting to you about your presentation–when you are excited, you can get the audience excited.\n\nStrong speakers are prepared, show enthusiasm and demonstrate knowledge."
  },
  {
    "objectID": "notes/communication/062623/index.html#slide-order",
    "href": "notes/communication/062623/index.html#slide-order",
    "title": "Communications: 06/26/2023",
    "section": "2.1 Slide Order",
    "text": "2.1 Slide Order\n\n\n\n\nflowchart LR\n    A[Title Slide] --&gt; B[\"BLUF (Bottom Line Up Front)\"] --&gt; C[Agenda]\n    C --&gt; D[Section 1] --&gt; G[Appendix]\n    C --&gt; E[Section 2] --&gt; G\n    C --&gt; F[Questions] --&gt; G\n\n\n\n\n\n\nUse visual elements in the sections to highlight / call out specific details in graphs or tables\nUse a milestone bar to help the audience keep track of the overall location"
  },
  {
    "objectID": "notes/analytics/logistic-regression/estimation-methods/index.html",
    "href": "notes/analytics/logistic-regression/estimation-methods/index.html",
    "title": "Estimation Methods for Logistic Regression",
    "section": "",
    "text": "In logistic regression, the assumptions of residual Normality and constant variance are violated. OLS is not the best method for parameter estimation."
  },
  {
    "objectID": "notes/analytics/logistic-regression/estimation-methods/index.html#likelihood-ratio-tests",
    "href": "notes/analytics/logistic-regression/estimation-methods/index.html#likelihood-ratio-tests",
    "title": "Estimation Methods for Logistic Regression",
    "section": "1.1 Likelihood Ratio Tests",
    "text": "1.1 Likelihood Ratio Tests\nLikelihood estimation allows us to do hypothesis testing. If extra predictors don’t add information, then a model that includes them shouldn’t be substantially more likely than the moel that doesn’t include them.\nLikelihood Ratio Test (LRT) compares the full and reduced models.\n\n\n\nLikelihood Ratio Test\n\n\n\\(L_0\\) is the reduced model and \\(L_1\\) is the full model.\n\nRPython\n\n\n\n\nCode\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names &lt;- make_ordinal_ames()\names &lt;- ames |&gt;\n    mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0))\ntrain &lt;- sample_frac(ames, 0.7)\n\nlogit_model &lt;- glm(Bonus ~ Gr_Liv_Area + factor(Central_Air), data = train, family = binomial(link = \"logit\"))\n\nlogit_model_r &lt;- glm(Bonus ~ 1, data = train, family = binomial(link = \"logit\"))\nanova(logit_model, logit_model_r, test = \"LRT\")\n\n\nAnalysis of Deviance Table\n\nModel 1: Bonus ~ Gr_Liv_Area + factor(Central_Air)\nModel 2: Bonus ~ 1\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1      2048     1808.8                          \n2      2050     2775.8 -2  -966.96 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\nCode\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\nimport statsmodels.formula.api as smf\nimport scipy as sp\n\names = r.ames\ntrain = r.train\nlogit_model = GLM.from_formula(\n    \"Bonus ~ Gr_Liv_Area + C(Central_Air)\", data=train, family=Binomial()\n).fit()\n\nreduced_ll = logit_model.llf\nfull_ll = (\n    smf.logit(\"Bonus ~ Gr_Liv_Area + C(Central_Air) + C(Fireplaces)\", data=train)\n    .fit()\n    .llf\n)\n\n\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.425703\n         Iterations: 35\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n\n\nCode\nLR_stat = -2 * (reduced_ll - full_ll)\np_val = sp.stats.chi2.sf(LR_stat, 4)\np_val\n\n\n8.181135706933704e-13\n\n\n\n\n\nIn this example, you can think of LRT as comparing these two equations:\n\\[\n\\begin{align*}\nL_1 &= \\beta_0 + \\beta_1GLA + \\beta_2CA + \\beta_3F1 \\beta_4F_2 + \\cdots \\\\\nL_0 &= \\beta_0 + \\beta_1GLA + \\beta_2CA\n\\end{align*}\n\\]\nAlways check the difference in degrees of freedom to double-check if you are comparing the right number of variables (how many levels are in the additional variable you are including?)."
  },
  {
    "objectID": "notes/analytics/logistic-regression/estimation-methods/index.html#categorical-p-values",
    "href": "notes/analytics/logistic-regression/estimation-methods/index.html#categorical-p-values",
    "title": "Estimation Methods for Logistic Regression",
    "section": "1.2 Categorical P-Values",
    "text": "1.2 Categorical P-Values\nFor categorical variables with more than 2 levels we shouldn’t evaluate the significance of the entire variable with the individual p-values. Use LRT to compare model with and without the categorical variable since LRT compares the model with ALL the levels included against the model with ALL the levels not included.\n\n\nCode\nlogit_model_f &lt;- glm(Bonus ~ Gr_Liv_Area + factor(Central_Air) + factor(Fireplaces), data = train, family = binomial(link = \"logit\"))\ncar::Anova(logit_model_f, test = \"LR\", type = \"III\")\n\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Bonus\n                    LR Chisq Df Pr(&gt;Chisq)    \nGr_Liv_Area           565.89  1  &lt; 2.2e-16 ***\nfactor(Central_Air)    86.81  1  &lt; 2.2e-16 ***\nfactor(Fireplaces)     62.61  4  8.181e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "notes/analytics/logistic-regression/estimation-methods/index.html#general-additive-model-gam",
    "href": "notes/analytics/logistic-regression/estimation-methods/index.html#general-additive-model-gam",
    "title": "Estimation Methods for Logistic Regression",
    "section": "2.1 General Additive Model (GAM)",
    "text": "2.1 General Additive Model (GAM)\nThe idea: We want to fit the best curve for our target and then we run a statistical test to see if it our fitted curve is any better than just a straight line relationship. If it is better, then our assumption of linearity is not met.\nTraditional logistic regression model:\n\\[\n\\log(odds) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n\\]\nGAM logistic regression model:\n\\[\n\\log(odds) = \\beta_0 + f_1(x_{1,i}) + \\cdots + f_k(x_{k,i})\n\\]\n\nR\n\n\n\n\nCode\nlibrary(mgcv)\nfit_gam &lt;- gam(Bonus ~ s(Gr_Liv_Area) + factor(Central_Air), data = train, family = binomial(link = \"logit\"), method = \"REML\")\n\nsummary(fit_gam)\n\n\n\nFamily: binomial \nLink function: logit \n\nFormula:\nBonus ~ s(Gr_Liv_Area) + factor(Central_Air)\n\nParametric coefficients:\n                     Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)           -4.4616     0.5033  -8.864  &lt; 2e-16 ***\nfactor(Central_Air)Y   3.4882     0.4911   7.103 1.22e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df Chi.sq p-value    \ns(Gr_Liv_Area) 6.221  7.232  380.4  &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.43   Deviance explained =   39%\n-REML = 859.46  Scale est. = 1         n = 2051\n\n\nCode\nplot(fit_gam)\n\n\n\n\n\nThe spline p-value is not telling us whether our assumption is met or not. It tells us whether or not the splined variable is significant in the model.\nedf in the splined variable is the polynomial degree that the fit thinks we should have. In theory, if the relationship was close to a straight line then edf would be close to 1.\nHow do we actually test if our variable satisfies the linearity assumption?\n\n\nCode\nanova(logit_model, fit_gam, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: Bonus ~ Gr_Liv_Area + factor(Central_Air)\nModel 2: Bonus ~ s(Gr_Liv_Area) + factor(Central_Air)\n  Resid. Df Resid. Dev     Df Deviance  Pr(&gt;Chi)    \n1    2048.0     1808.8                              \n2    2042.8     1692.3 5.2212   116.58 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIf our p-value is significant, then it means our two models are significantly different. If our two models are significantly different, then our curve model is providing more information than a straight line. Assumption is not met.\nIn conclusion, high p-value means our assumption is met, else not met.\n\n\n\n\n2.1.1 Linearity Assumption Failed?\n\nUse GAM logistic model instead with more limited interpretation on variables that break assumption\nBin the continuous variables that break assumption (keeps interpretation)\n\n\n\n\nBinning Continuous Variable\n\n\n\n\nCode\ntrain &lt;- train %&gt;%\n    mutate(Gr_Liv_Area_BIN = cut(Gr_Liv_Area, breaks = c(-Inf, 1000, 1500, 3000, 4500, Inf)))\n\nlogit_model_bin &lt;- glm(Bonus ~ factor(Gr_Liv_Area_BIN) + factor(Central_Air), data = train, family = binomial())\nsummary(logit_model_bin)\n\n\n\nCall:\nglm(formula = Bonus ~ factor(Gr_Liv_Area_BIN) + factor(Central_Air), \n    family = binomial(), data = train)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                             -8.8210     1.1065  -7.972 1.56e-15 ***\nfactor(Gr_Liv_Area_BIN)(1e+03,1.5e+03]   4.5121     1.0052   4.489 7.16e-06 ***\nfactor(Gr_Liv_Area_BIN)(1.5e+03,3e+03]   6.6437     1.0049   6.611 3.81e-11 ***\nfactor(Gr_Liv_Area_BIN)(3e+03,4.5e+03]  21.1646   363.8508   0.058  0.95361    \nfactor(Gr_Liv_Area_BIN)(4.5e+03, Inf]    5.5986     1.7331   3.230  0.00124 ** \nfactor(Central_Air)Y                     3.2224     0.4734   6.807 9.95e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1892.0  on 2045  degrees of freedom\nAIC: 1904\n\nNumber of Fisher Scoring iterations: 14\n\n\nNote that binning a continuous variable results in an ordinal variable."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/lab_13.html",
    "href": "notes/analytics/logistic-regression/introduction/lab_13.html",
    "title": "Lab 13",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nsafety &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/safety.csv\")\n\nsafety &lt;- safety %&gt;%\n    mutate(across(c(Region, Size), as.factor))\n\n\n\n\nCode\nmodel &lt;- glm(Unsafe ~ Region + Weight + Size, family = binomial(), data = safety)\nsummary(model)\n\n\n\nCall:\nglm(formula = Unsafe ~ Region + Weight + Size, family = binomial(), \n    data = safety)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       2.7285     1.3949   1.956  0.05046 . \nRegionN America  -0.3775     0.5624  -0.671  0.50203   \nWeight           -0.6678     0.4589  -1.455  0.14559   \nSize2            -2.0200     0.6246  -3.234  0.00122 **\nSize3            -2.6785     0.8810  -3.040  0.00236 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 119.249  on 95  degrees of freedom\nResidual deviance:  84.004  on 91  degrees of freedom\nAIC: 94.004\n\nNumber of Fisher Scoring iterations: 5\n\n\nSignificant variables are Size\n\n\nCode\nlibrary(survival)\nconcordance(model)\n\n\nCall:\nconcordance.lm(object = model)\n\nn= 96 \nConcordance= 0.8482 se= 0.03897\nconcordant discordant     tied.x     tied.y    tied.xy \n      1622        243        115       2273        307 \n\n\nModel correctly ranks unsafe cars ahead of safe cars 84.8% of the time."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/lab_13.html#a",
    "href": "notes/analytics/logistic-regression/introduction/lab_13.html#a",
    "title": "Lab 13",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nsafety &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/safety.csv\")\n\nsafety &lt;- safety %&gt;%\n    mutate(across(c(Region, Size), as.factor))\n\n\n\n\nCode\nmodel &lt;- glm(Unsafe ~ Region + Weight + Size, family = binomial(), data = safety)\nsummary(model)\n\n\n\nCall:\nglm(formula = Unsafe ~ Region + Weight + Size, family = binomial(), \n    data = safety)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)       2.7285     1.3949   1.956  0.05046 . \nRegionN America  -0.3775     0.5624  -0.671  0.50203   \nWeight           -0.6678     0.4589  -1.455  0.14559   \nSize2            -2.0200     0.6246  -3.234  0.00122 **\nSize3            -2.6785     0.8810  -3.040  0.00236 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 119.249  on 95  degrees of freedom\nResidual deviance:  84.004  on 91  degrees of freedom\nAIC: 94.004\n\nNumber of Fisher Scoring iterations: 5\n\n\nSignificant variables are Size\n\n\nCode\nlibrary(survival)\nconcordance(model)\n\n\nCall:\nconcordance.lm(object = model)\n\nn= 96 \nConcordance= 0.8482 se= 0.03897\nconcordant discordant     tied.x     tied.y    tied.xy \n      1622        243        115       2273        307 \n\n\nModel correctly ranks unsafe cars ahead of safe cars 84.8% of the time."
  },
  {
    "objectID": "notes/analytics/logistic-regression/introduction/lab_13.html#b",
    "href": "notes/analytics/logistic-regression/introduction/lab_13.html#b",
    "title": "Lab 13",
    "section": "2 b",
    "text": "2 b\n\n\nCode\nmodel2 &lt;- glm(Unsafe ~ Weight + Size, data = safety, family = binomial())\nsummary(model2)\n\n\n\nCall:\nglm(formula = Unsafe ~ Weight + Size, family = binomial(), data = safety)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)   2.5272     1.3427   1.882  0.05981 . \nWeight       -0.6686     0.4553  -1.469  0.14195   \nSize2        -2.0142     0.6222  -3.237  0.00121 **\nSize3        -2.7911     0.8666  -3.221  0.00128 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 119.249  on 95  degrees of freedom\nResidual deviance:  84.455  on 92  degrees of freedom\nAIC: 92.455\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nCode\nmodel3 &lt;- glm(Unsafe ~ Size, data = safety, family = binomial())\nsummary(model3)\n\n\n\nCall:\nglm(formula = Unsafe ~ Size, family = binomial(), data = safety)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.6506     0.3561   1.827 0.067708 .  \nSize2        -2.2192     0.6070  -3.656 0.000256 ***\nSize3        -3.3586     0.8125  -4.134 3.57e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 119.249  on 95  degrees of freedom\nResidual deviance:  86.629  on 93  degrees of freedom\nAIC: 92.629\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\nconcordance(model3)\n\n\nCall:\nconcordance.lm(object = model3)\n\nn= 96 \nConcordance= 0.8182 se= 0.043\nconcordant discordant     tied.x     tied.y    tied.xy \n      1392        132        456       1539       1041 \n\n\nCode\nexp(cbind(coef(model3), confint(model3)))\n\n\nWaiting for profiling to be done...\n\n\n                             2.5 %    97.5 %\n(Intercept) 1.91666667 0.971535104 3.9816962\nSize2       0.10869565 0.030145216 0.3357070\nSize3       0.03478261 0.005058893 0.1414522\n\n\nOnly Size remained in the model. 0.8182 C-statistic\nThe interpretation of the Size variable in the model is that Size2 is 0.108 times more likely to be unsafe than compared to Size1. Size3 is 0.03 times more likely to be unsafe than compared to Size1."
  },
  {
    "objectID": "notes/analytics/logistic-regression/diagnostics/index.html",
    "href": "notes/analytics/logistic-regression/diagnostics/index.html",
    "title": "Diagnostics and Subset Selection",
    "section": "",
    "text": "RPython\n\n\n\n\nCode\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names &lt;- make_ordinal_ames()\names &lt;- ames |&gt;\n    mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0))\ntrain &lt;- sample_frac(ames, 0.7)\n\n\n\n\n\n\nCode\names = r.ames\ntrain = r.train"
  },
  {
    "objectID": "notes/analytics/logistic-regression/diagnostics/index.html#deviance",
    "href": "notes/analytics/logistic-regression/diagnostics/index.html#deviance",
    "title": "Diagnostics and Subset Selection",
    "section": "4.1 Deviance",
    "text": "4.1 Deviance\nA saturated model is a model that fits the data perfectly by essentially overfitting it. We create a variable for each unique combination of inputs. Deviance is a measure of how far the fitted model is from the saturated model–the error. Logistic regression minimizes the sum of squared deviances.\nDeviance residuals tell us how much each observation reduces the deviance."
  },
  {
    "objectID": "notes/analytics/logistic-regression/diagnostics/index.html#influence-statistics",
    "href": "notes/analytics/logistic-regression/diagnostics/index.html#influence-statistics",
    "title": "Diagnostics and Subset Selection",
    "section": "4.2 Influence Statistics",
    "text": "4.2 Influence Statistics\n\nCook’s D\n\nMeasures the overall impact to the coefficients in the model\n\nDFBETAS\n\nMeasures standardized change in each parameter estimate with deletion of observation\n\nDIFCHISQ\n\nMeasures change in Pearson Chi-square with deletion of observation\n\nDIFDEV\n\nMeasures change in deviance with deletion of the observation\n\n\n\nRPython\n\n\n\n\nCode\nlibrary(car)\n\nlogit_model &lt;- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())\n\n# influence.measures(logit_model)\n\n\nTo plot Cook’s D:\n\n\nCode\nplot(logit_model, 4)\n\n\n\n\n\nTo plot DFBETAS:\n\n\nCode\ndfbetasPlots(logit_model, terms = \"Gr_Liv_Area\", id.n = 5, col = ifelse(logit_model$y == 1, \"red\", \"blue\"))\n\n\n\n\n\n\n\n\n\nCode\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\nlog_model = GLM.from_formula(\n    \"Bonus ~ Gr_Liv_Area + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces\",\n    data=train,\n    family=Binomial(),\n).fit()\nlog_model.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nBonus\nNo. Observations:\n2051\n\n\nModel:\nGLM\nDf Residuals:\n2039\n\n\nModel Family:\nBinomial\nDf Model:\n11\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-655.10\n\n\nDate:\nThu, 07 Sep 2023\nDeviance:\n1310.2\n\n\nTime:\n10:54:18\nPearson chi2:\n5.34e+05\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.5106\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-9.7344\n1.441\n-6.753\n0.000\n-12.560\n-6.909\n\n\nC(Full_Bath)[T.1]\n-0.0390\n1.143\n-0.034\n0.973\n-2.279\n2.201\n\n\nC(Full_Bath)[T.2]\n2.3699\n1.143\n2.073\n0.038\n0.129\n4.610\n\n\nC(Full_Bath)[T.3]\n4.5001\n1.519\n2.963\n0.003\n1.523\n7.477\n\n\nC(Full_Bath)[T.4]\n-1.2645\n2.126\n-0.595\n0.552\n-5.431\n2.902\n\n\nC(Central_Air)[T.Y]\n2.2895\n0.567\n4.037\n0.000\n1.178\n3.401\n\n\nGr_Liv_Area\n0.0038\n0.000\n8.659\n0.000\n0.003\n0.005\n\n\nGarage_Area\n0.0046\n0.000\n9.520\n0.000\n0.004\n0.006\n\n\nFireplaces\n1.9427\n0.542\n3.582\n0.000\n0.880\n3.006\n\n\nLot_Area\n1.568e-05\n1.57e-05\n1.000\n0.317\n-1.51e-05\n4.64e-05\n\n\nTotRms_AbvGrd\n-0.5049\n0.080\n-6.342\n0.000\n-0.661\n-0.349\n\n\nGr_Liv_Area:Fireplaces\n-0.0006\n0.000\n-1.859\n0.063\n-0.001\n3.45e-05\n\n\n\n\n\n\n\nCode\nimport statsmodels.stats.tests.test_influence\n\nlog_diag = log_model.get_influence()\nlog_diag.summary_frame().head()\n\n\n   dfb_Intercept  dfb_C(Full_Bath)[T.1]  ...  hat_diag  dffits_internal\n0      -0.004889               0.001706  ...  0.003598         0.023667\n1      -0.013708               0.004351  ...  0.011756        -0.167066\n2       0.023485               0.001300  ...  0.006500        -0.135552\n3      -0.002182               0.000401  ...  0.002478         0.008381\n4      -0.000883              -0.000017  ...  0.000711        -0.002877\n\n[5 rows x 16 columns]\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\n\nlog_diag.plot_influence()\nplt.show()"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html",
    "title": "Model Assessment",
    "section": "",
    "text": "RPython\n\n\n\n\nCode\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names &lt;- make_ordinal_ames()\names &lt;- ames |&gt;\n    mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0))\ntrain &lt;- sample_frac(ames, 0.7)\n\nlogit_model &lt;- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())\n\n\n\n\n\n\nCode\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\names = r.ames\ntrain = r.train\nlog_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#generalized-nagelkerkge-r2",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#generalized-nagelkerkge-r2",
    "title": "Model Assessment",
    "section": "2.1 Generalized / Nagelkerkge \\(R^2\\)",
    "text": "2.1 Generalized / Nagelkerkge \\(R^2\\)\n\\[\nR_G^2 = 1 - (\\frac{L_0}{L_1})^{\\frac{2}{n}}\n\\]\n\nRPython\n\n\n\n\nCode\nlibrary(DescTools)\n\nAIC(logit_model)\n\n\n[1] 1287.964\n\n\nCode\nBIC(logit_model)\n\n\n[1] 1394.86\n\n\nCode\nPseudoR2(logit_model, which = \"Nagelkerke\")\n\n\nNagelkerke \n 0.7075796 \n\n\n\n\n\n\nCode\nprint(log_model.aic)\n\n\n1287.964395223345\n\n\nCode\nprint(log_model.bic_llf)\n\n\n1394.8599676267202\n\n\nCode\nprint(log_model.pseudo_rsquared())\n\n\n0.5247678540521266"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#discrimination-vs.-calibration",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#discrimination-vs.-calibration",
    "title": "Model Assessment",
    "section": "3.1 Discrimination vs. Calibration",
    "text": "3.1 Discrimination vs. Calibration\nDiscrimination is the ability to separate the events from the non-events. How good is a model at distinguishing the 1’s from the 0’s.\nCalibration is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.\nThese two metrics may not agree with each other."
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#coefficient-of-discrimination",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#coefficient-of-discrimination",
    "title": "Model Assessment",
    "section": "3.2 Coefficient of Discrimination",
    "text": "3.2 Coefficient of Discrimination\nCoefficient of determination is the difference in average predicted probability between 1’s and 0’s:\n\\[\nD = \\bar{\\hat{p}}_1 - \\bar{\\hat{p}}_0\n\\]\nThis is a comparison metric to see which model can separate the 1’s and 0’s better.\n\nRPython\n\n\n\n\nCode\ntrain$p_hat &lt;- predict(logit_model, type = \"response\")\np1 &lt;- train$p_hat[train$Bonus == 1]\np0 &lt;- train$p_hat[train$Bonus == 0]\n\ncoef_discrim &lt;- mean(p1) - mean(p0)\n\nggplot(train, aes(p_hat, fill = factor(Bonus))) +\n    geom_density(alpha = 0.7) +\n    scale_fill_grey() +\n    labs(x = \"Predicted Probability\", fill = \"Outcome\", title = paste(\"Coefficient of Discrimination = \", round(coef_discrim, 3), sep = \"\"))\n\n\n\n\n\n\n\n\n\nCode\ntrain[\"p_hat\"] = log_model.predict()\n\np1 = train.loc[train[\"Bonus\"] == 1, \"p_hat\"]\np0 = train.loc[train[\"Bonus\"] == 0, \"p_hat\"]\n\ncoef_discrim = p1.mean() - p0.mean()"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#concordance",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#concordance",
    "title": "Model Assessment",
    "section": "4.1 Concordance",
    "text": "4.1 Concordance\nInterpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event \\(Concordance\\%\\) of the time.\nCommon metrics based on concordance:\n\nC-Statistic: \\(c = Concordance\\% + \\frac{1}{2}Tied\\%\\)\nSomer’s D (Gini): \\(D_{xy} = 2c - 1\\)\nKendall’s \\(\\tau_\\alpha\\): \\(\\tau_\\alpha = \\frac{\\text{#concordant} - \\text{#discordant}}{\\frac{n(n - 1)}{2}}\\)\n\n\nRPython\n\n\n\n\nCode\nlibrary(Hmisc)\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:DescTools':\n\n    %nin%, Label, Mean, Quantile\n\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nsomers2(train$p_hat, train$Bonus)\n\n\n           C          Dxy            n      Missing \n   0.9428394    0.8856789 2051.0000000    0.0000000"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#youden-j-statistic",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#youden-j-statistic",
    "title": "Model Assessment",
    "section": "6.1 Youden J Statistic",
    "text": "6.1 Youden J Statistic\n\\[\nJ = \\text{Sensitivity} + \\text{Specificity} - 1\n\\]\nFalse positives and false negatives are weighed equally, so select cut-off that products highest Youden \\(J\\) statistic.\n\nRPython\n\n\n\n\nCode\ntrain &lt;- train %&gt;%\n    mutate(Bonus_hat = ifelse(p_hat &gt; 0.5, 1, 0))\n\ntable(train$Bonus_hat, train$Bonus)\n\n\n   \n       0    1\n  0 1062  127\n  1  149  713\n\n\n\n\nCode\nlibrary(ROCit)\n\nlogit_meas &lt;- measureit(train$p_hat, train$Bonus, measure = c(\"ACC\", \"SENS\", \"SPEC\"))"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#roc-curve",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#roc-curve",
    "title": "Model Assessment",
    "section": "6.2 ROC Curve",
    "text": "6.2 ROC Curve\nThe ROC curve plots TPR vs. FPR for a grid of thresholds. Area under the curve (AUC or AUROC) summarizes the overall quality of ROC curve. Equivalent to C-statistic.\nWe want high sensitivity and high specificity. \n\n\n\nROC Curve\n\n\n\nRPython\n\n\n\n\nCode\nlogit_roc &lt;- rocit(train$p_hat, train$Bonus)\nplot(logit_roc)\n\n\n\n\n\n\n\nCode\nplot(logit_roc)$optimal\n\n\n\n\n\n    value       FPR       TPR    cutoff \n0.7352326 0.1552436 0.8904762 0.4229724"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#ks-statistic-or-youden",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#ks-statistic-or-youden",
    "title": "Model Assessment",
    "section": "7.1 KS-Statistic or Youden?",
    "text": "7.1 KS-Statistic or Youden?\n\\(D\\) test statistic is used for model comparison. However, it is mathematically equivalent to Youden’s J statistic. The point at which we have the maximum \\(D\\) statistic is the optimal cutoff."
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#best-cut-off",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#best-cut-off",
    "title": "Model Assessment",
    "section": "8.1 Best Cut-off?",
    "text": "8.1 Best Cut-off?\nAlways consider the cost of false positives and false negatives when doing classification.\nWhen not considering costs, many different techniques to “optimize” cutoff.\n\\[\nF_1 = 2\\left(\\frac{PPV \\cdot TPR}{PPV + TPR}\\right)\n\\]\nPrecision and recall are weighted equally, so select cut-off that produces highest \\(F_1\\) score.\n\nRPython\n\n\n\n\nCode\nlogit_meas &lt;- measureit(train$p_hat, train$Bonus, measure = c(\"PREC\", \"REC\", \"FSCR\"))\n\nfscore_table &lt;- data.frame(Cutoff = logit_meas$Cutoff, FScore = logit_meas$FSCR)\n\nfscore_table %&gt;%\n    arrange(desc(FScore)) %&gt;%\n    head(1)\n\n\n     Cutoff    FScore\n1 0.4229724 0.8423423"
  },
  {
    "objectID": "notes/analytics/logistic-regression/model-assessment/index.html#precision-and-lift",
    "href": "notes/analytics/logistic-regression/model-assessment/index.html#precision-and-lift",
    "title": "Model Assessment",
    "section": "8.2 Precision and Lift",
    "text": "8.2 Precision and Lift\nCommon calculation in marketing. Great for interpretation around validity of model ranking / classifying observations correctly.\n\\[\n\\text{Lift} = \\frac{PPV}{\\pi_1}\n\\]\n\n\\(\\pi_1\\) is the proportion of 1’s in your original population\n\nThe top depth% of your customers, based on predicted probability, you get lift times as many responses compared to targeting a random sample of depth% of your customers.\n\nR\n\n\n\n\nCode\nlogit_lift &lt;- gainstable(logit_roc)\nlogit_lift\n\n\n   Bucket Obs CObs Depth Resp CResp RespRate CRespRate CCapRate  Lift CLift\n1       1 205  205   0.1  200   200    0.976     0.976    0.238 2.382 2.382\n2       2 205  410   0.2  190   390    0.927     0.951    0.464 2.263 2.323\n3       3 205  615   0.3  167   557    0.815     0.906    0.663 1.989 2.211\n4       4 205  820   0.4  134   691    0.654     0.843    0.823 1.596 2.058\n5       5 206 1026   0.5   92   783    0.447     0.763    0.932 1.090 1.863\n6       6 205 1231   0.6   42   825    0.205     0.670    0.982 0.500 1.636\n7       7 205 1436   0.7   12   837    0.059     0.583    0.996 0.143 1.423\n8       8 205 1641   0.8    1   838    0.005     0.511    0.998 0.012 1.247\n9       9 205 1846   0.9    2   840    0.010     0.455    1.000 0.024 1.111\n10     10 205 2051   1.0    0   840    0.000     0.410    1.000 0.000 1.000"
  },
  {
    "objectID": "notes/analytics/EDA/introduction/lab_1.html",
    "href": "notes/analytics/EDA/introduction/lab_1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Code\nlibrary(UsingR)\n\n\nLoading required package: MASS\n\n\nLoading required package: HistData\n\n\nLoading required package: Hmisc\n\n\n\nAttaching package: 'Hmisc'\n\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\nlibrary(ggplot2)\ndata(normtemp)\nstr(normtemp)\n\n\n'data.frame':   130 obs. of  3 variables:\n $ temperature: num  96.3 96.7 96.9 97 97.1 97.1 97.1 97.2 97.3 97.4 ...\n $ gender     : int  1 1 1 1 1 1 1 1 1 1 ...\n $ hr         : int  70 71 74 80 73 75 82 64 69 70 ...\n\n\n\n\nUse the normtemp dataset to answer the following:\n\nDetermine the following statistics for the variable temperature\n\n\n\nCode\nsummary(normtemp$temperature)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  96.30   97.80   98.30   98.25   98.70  100.80 \n\n\n\nDoes temperature appear to be normally distributed?\n\n\n\nCode\nggplot(normtemp, aes(sample = temperature)) +\n    stat_qq(col = \"blue\") +\n    stat_qq_line() +\n    labs(x = \"theoretical\", y = \"observed\")\n\n\n\n\n\nBased on the QQ-Plot, temperature appears to be approximately Normal. However, we should be wary that the distribution is tending towards a platykurtic distribution\n\nCreate box plots for temperature. Are there any outliers? Display a reference line at 98.6.\nFor horizontal line: geom_hline(yintercept=98.6)\nFor vertical line: geom_vline(xintercept=98.6)\n\n\n\nCode\nggplot(normtemp, aes(x = temperature)) +\n    geom_boxplot(outlier.color = \"red\") +\n    labs(x = \"Temperature\", title = \"Box-Plot of Temperature\") +\n    geom_vline(xintercept = 98.6, col = \"blue\")\n\n\n\n\n\nThree observations appear to be outliers (colored in red) for temperature. After plotting the reference line at 98.6 degrees, we can visually see that the median is actually lower than 98.6.\n\n\nUsing the Ameshousing dataset from our in-class examples, run some distributional analysis on Sale_Price, Log(Sale_Price), and Gr_Liv_Area.\n\nCreate histograms of these three variables.\n\nOverlay a kernel density estimator of the variables.\n\n\n\n\nCode\nlibrary(AmesHousing)\n\names &lt;- make_ordinal_ames()\n\n\n\n\nCode\nggplot(ames, aes(x = Sale_Price)) +\n    geom_histogram(aes(y = after_stat(!!str2lang(\"density\"))), fill = \"pink\", alpha = 0.4) +\n    geom_density() +\n    labs(x = \"Sales Price (USD)\", title = \"Histogram of Housing Sales Price\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(ames, aes(x = log(Sale_Price))) +\n    geom_histogram(aes(y = after_stat(!!str2lang(\"density\"))), fill = \"blue\", alpha = 0.4) +\n    geom_density() +\n    labs(x = \"Sales Price (USD)\", title = \"Histogram of Log(Sales Price)\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nCode\nggplot(ames, aes(x = Gr_Liv_Area)) +\n    geom_histogram(aes(y = after_stat(!!str2lang(\"density\"))), fill = \"purple\", alpha = 0.4) +\n    geom_density() +\n    labs(x = \"Sales Price (USD)\", title = \"Histogram of Living Area\")\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\nCreate a QQ Plot for both Sale_Price and Log(Sale_Price). Based on these exploratory procedures, which version of the price information would you say is closer to being normally distributed?\n\n\n\nCode\nggplot(ames, aes(sample = Sale_Price)) +\n    stat_qq(col = \"blue\", shape = 8, size = 1) +\n    stat_qq_line() +\n    labs(x = \"theoretical\", y = \"observed\", title = \"QQ-Plot of Sale Price\")\n\n\n\n\n\nCode\nggplot(ames, aes(sample = log(Sale_Price))) +\n    stat_qq(col = \"blue\", shape = 8, size = 1) +\n    stat_qq_line()\n\n\n\n\n\nCode\nlabs(x = \"theoretical\", y = \"observed\", title = \"QQ-Plot of Log(Sale Price)\")\n\n\n$x\n[1] \"theoretical\"\n\n$y\n[1] \"observed\"\n\n$title\n[1] \"QQ-Plot of Log(Sale Price)\"\n\nattr(,\"class\")\n[1] \"labels\"\n\n\nThe Log(Sale Price) QQ-Plot shows a much closer similarity to a Normal distribution than just Sale Price.\n\n\nUsing the Ameshousing dataset from our in-class examples, determine the following:\n\nWhat type of variables are each of these columns (Nominal, Ordinal, or Continuous/Quantitative)? Keep in mind that the way they are represented in the R dataset may not be appropriate, so you should make this determination using your own judgement based on the data you are looking at.\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter()    masks stats::filter()\n✖ dplyr::lag()       masks stats::lag()\n✖ dplyr::select()    masks MASS::select()\n✖ dplyr::src()       masks Hmisc::src()\n✖ dplyr::summarize() masks Hmisc::summarize()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nglimpse(ames)\n\n\nRows: 2,930\nColumns: 81\n$ MS_SubClass        &lt;fct&gt; One_Story_1946_and_Newer_All_Styles, One_Story_1946…\n$ MS_Zoning          &lt;fct&gt; Residential_Low_Density, Residential_High_Density, …\n$ Lot_Frontage       &lt;dbl&gt; 141, 80, 81, 93, 74, 78, 41, 43, 39, 60, 75, 0, 63,…\n$ Lot_Area           &lt;int&gt; 31770, 11622, 14267, 11160, 13830, 9978, 4920, 5005…\n$ Street             &lt;fct&gt; Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pave, Pav…\n$ Alley              &lt;fct&gt; No_Alley_Access, No_Alley_Access, No_Alley_Access, …\n$ Lot_Shape          &lt;ord&gt; Slightly_Irregular, Regular, Slightly_Irregular, Re…\n$ Land_Contour       &lt;ord&gt; Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, Lvl, HLS, Lvl, Lvl, L…\n$ Utilities          &lt;ord&gt; AllPub, AllPub, AllPub, AllPub, AllPub, AllPub, All…\n$ Lot_Config         &lt;fct&gt; Corner, Inside, Corner, Corner, Inside, Inside, Ins…\n$ Land_Slope         &lt;ord&gt; Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, Gtl, G…\n$ Neighborhood       &lt;fct&gt; North_Ames, North_Ames, North_Ames, North_Ames, Gil…\n$ Condition_1        &lt;fct&gt; Norm, Feedr, Norm, Norm, Norm, Norm, Norm, Norm, No…\n$ Condition_2        &lt;fct&gt; Norm, Norm, Norm, Norm, Norm, Norm, Norm, Norm, Nor…\n$ Bldg_Type          &lt;fct&gt; OneFam, OneFam, OneFam, OneFam, OneFam, OneFam, Twn…\n$ House_Style        &lt;fct&gt; One_Story, One_Story, One_Story, One_Story, Two_Sto…\n$ Overall_Qual       &lt;ord&gt; Above_Average, Average, Above_Average, Good, Averag…\n$ Overall_Cond       &lt;ord&gt; Average, Above_Average, Above_Average, Average, Ave…\n$ Year_Built         &lt;int&gt; 1960, 1961, 1958, 1968, 1997, 1998, 2001, 1992, 199…\n$ Year_Remod_Add     &lt;int&gt; 1960, 1961, 1958, 1968, 1998, 1998, 2001, 1992, 199…\n$ Roof_Style         &lt;fct&gt; Hip, Gable, Hip, Hip, Gable, Gable, Gable, Gable, G…\n$ Roof_Matl          &lt;fct&gt; CompShg, CompShg, CompShg, CompShg, CompShg, CompSh…\n$ Exterior_1st       &lt;fct&gt; BrkFace, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Exterior_2nd       &lt;fct&gt; Plywood, VinylSd, Wd Sdng, BrkFace, VinylSd, VinylS…\n$ Mas_Vnr_Type       &lt;fct&gt; Stone, None, BrkFace, None, None, BrkFace, None, No…\n$ Mas_Vnr_Area       &lt;dbl&gt; 112, 0, 108, 0, 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6…\n$ Exter_Qual         &lt;ord&gt; Typical, Typical, Typical, Good, Typical, Typical, …\n$ Exter_Cond         &lt;ord&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Foundation         &lt;fct&gt; CBlock, CBlock, CBlock, CBlock, PConc, PConc, PConc…\n$ Bsmt_Qual          &lt;ord&gt; Typical, Typical, Typical, Typical, Good, Typical, …\n$ Bsmt_Cond          &lt;ord&gt; Good, Typical, Typical, Typical, Typical, Typical, …\n$ Bsmt_Exposure      &lt;ord&gt; Gd, No, No, No, No, No, Mn, No, No, No, No, No, No,…\n$ BsmtFin_Type_1     &lt;ord&gt; BLQ, Rec, ALQ, ALQ, GLQ, GLQ, GLQ, ALQ, GLQ, Unf, U…\n$ BsmtFin_SF_1       &lt;dbl&gt; 2, 6, 1, 1, 3, 3, 3, 1, 3, 7, 7, 1, 7, 3, 3, 1, 3, …\n$ BsmtFin_Type_2     &lt;ord&gt; Unf, LwQ, Unf, Unf, Unf, Unf, Unf, Unf, Unf, Unf, U…\n$ BsmtFin_SF_2       &lt;dbl&gt; 0, 144, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1120, 0…\n$ Bsmt_Unf_SF        &lt;dbl&gt; 441, 270, 406, 1045, 137, 324, 722, 1017, 415, 994,…\n$ Total_Bsmt_SF      &lt;dbl&gt; 1080, 882, 1329, 2110, 928, 926, 1338, 1280, 1595, …\n$ Heating            &lt;fct&gt; GasA, GasA, GasA, GasA, GasA, GasA, GasA, GasA, Gas…\n$ Heating_QC         &lt;ord&gt; Fair, Typical, Typical, Excellent, Good, Excellent,…\n$ Central_Air        &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …\n$ Electrical         &lt;ord&gt; SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SBrkr, SB…\n$ First_Flr_SF       &lt;int&gt; 1656, 896, 1329, 2110, 928, 926, 1338, 1280, 1616, …\n$ Second_Flr_SF      &lt;int&gt; 0, 0, 0, 0, 701, 678, 0, 0, 0, 776, 892, 0, 676, 0,…\n$ Low_Qual_Fin_SF    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Gr_Liv_Area        &lt;int&gt; 1656, 896, 1329, 2110, 1629, 1604, 1338, 1280, 1616…\n$ Bsmt_Full_Bath     &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, …\n$ Bsmt_Half_Bath     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Full_Bath          &lt;int&gt; 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 3, 2, …\n$ Half_Bath          &lt;int&gt; 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, …\n$ Bedroom_AbvGr      &lt;int&gt; 3, 2, 3, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2, 1, 4, 4, …\n$ Kitchen_AbvGr      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ Kitchen_Qual       &lt;ord&gt; Typical, Typical, Good, Excellent, Typical, Good, G…\n$ TotRms_AbvGrd      &lt;int&gt; 7, 5, 6, 8, 6, 7, 6, 5, 5, 7, 7, 6, 7, 5, 4, 12, 8,…\n$ Functional         &lt;ord&gt; Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, Typ, T…\n$ Fireplaces         &lt;int&gt; 2, 0, 0, 2, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, …\n$ Fireplace_Qu       &lt;ord&gt; Good, No_Fireplace, No_Fireplace, Typical, Typical,…\n$ Garage_Type        &lt;fct&gt; Attchd, Attchd, Attchd, Attchd, Attchd, Attchd, Att…\n$ Garage_Finish      &lt;ord&gt; Fin, Unf, Unf, Fin, Fin, Fin, Fin, RFn, RFn, Fin, F…\n$ Garage_Cars        &lt;dbl&gt; 2, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, …\n$ Garage_Area        &lt;dbl&gt; 528, 730, 312, 522, 482, 470, 582, 506, 608, 442, 4…\n$ Garage_Qual        &lt;ord&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Garage_Cond        &lt;ord&gt; Typical, Typical, Typical, Typical, Typical, Typica…\n$ Paved_Drive        &lt;ord&gt; Partial_Pavement, Paved, Paved, Paved, Paved, Paved…\n$ Wood_Deck_SF       &lt;int&gt; 210, 140, 393, 0, 212, 360, 0, 0, 237, 140, 157, 48…\n$ Open_Porch_SF      &lt;int&gt; 62, 0, 36, 0, 34, 36, 0, 82, 152, 60, 84, 21, 75, 0…\n$ Enclosed_Porch     &lt;int&gt; 0, 0, 0, 0, 0, 0, 170, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ Three_season_porch &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Screen_Porch       &lt;int&gt; 0, 120, 0, 0, 0, 0, 0, 144, 0, 0, 0, 0, 0, 0, 140, …\n$ Pool_Area          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ Pool_QC            &lt;ord&gt; No_Pool, No_Pool, No_Pool, No_Pool, No_Pool, No_Poo…\n$ Fence              &lt;ord&gt; No_Fence, Minimum_Privacy, No_Fence, No_Fence, Mini…\n$ Misc_Feature       &lt;fct&gt; None, None, Gar2, None, None, None, None, None, Non…\n$ Misc_Val           &lt;int&gt; 0, 0, 12500, 0, 0, 0, 0, 0, 0, 0, 0, 500, 0, 0, 0, …\n$ Mo_Sold            &lt;int&gt; 5, 6, 6, 4, 3, 6, 4, 1, 3, 6, 4, 3, 5, 2, 6, 6, 6, …\n$ Year_Sold          &lt;int&gt; 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 201…\n$ Sale_Type          &lt;fct&gt; WD , WD , WD , WD , WD , WD , WD , WD , WD , WD , W…\n$ Sale_Condition     &lt;fct&gt; Normal, Normal, Normal, Normal, Normal, Normal, Nor…\n$ Sale_Price         &lt;int&gt; 215000, 105000, 172000, 244000, 189900, 195500, 213…\n$ Longitude          &lt;dbl&gt; -93.61975, -93.61976, -93.61939, -93.61732, -93.638…\n$ Latitude           &lt;dbl&gt; 42.05403, 42.05301, 42.05266, 42.05125, 42.06090, 4…\n\n\n\nOverall_Qual is an ordinal variable as it represents categories that could be ordered based on the rated quality of the house\nLot_Shape is an ordinal variable. It represents categories that could be ordered based on the rated lot shape. For example, Slightly_Irregular could be ordered after Irregular\nHeating_QC is an ordinal variable. It represents the categories of heating quality that could be ordered. For example, Excellent would represent a higher order than Good\nLot_Area is a quantitative variable as it represents a continuous quantity of the area of the lot"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html",
    "href": "notes/analytics/EDA/categorical-data/index.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Recall: categorical variables are data whose measurement scale is inherently categorical.\nTwo types: nominal and ordinal. Nominal has no logical ordering and ordianl has a logical ordering.\nCode\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\nuse_condaenv(\"msa\")\names &lt;- make_ordinal_ames()\n\names &lt;- ames %&gt;%\n    mutate(id = row_number())\n\ntrain &lt;- ames %&gt;%\n    sample_frac(0.7)\ntest &lt;- anti_join(ames, train, by = \"id\")\nCode\ntrain &lt;- train %&gt;%\n    mutate(Bonus = ifelse(Sale_Price &gt; 175000, 1, 0))"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#r-code",
    "href": "notes/analytics/EDA/categorical-data/index.html#r-code",
    "title": "Categorical Data Analysis",
    "section": "1.1 R Code",
    "text": "1.1 R Code\n\n\nCode\ntable(train$Central_Air)\n\n\n\n   N    Y \n 147 1904 \n\n\nCode\nggplot(train) +\n    geom_bar(mapping = aes(x = Central_Air))\n\n\n\n\n\nCode\ntable(train$Bonus)\n\n\n\n   0    1 \n1211  840 \n\n\nCode\nggplot(train) +\n    geom_bar(mapping = aes(x = Bonus))"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#python-code",
    "href": "notes/analytics/EDA/categorical-data/index.html#python-code",
    "title": "Categorical Data Analysis",
    "section": "1.2 Python Code",
    "text": "1.2 Python Code\n\n\nCode\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\ntrain = r.train\n\ntrain[\"Bonus\"] == np.where(train[\"Sale_Price\"] &gt; 175000, 1, 0)\n\n\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n2046    True\n2047    True\n2048    True\n2049    True\n2050    True\nName: Bonus, Length: 2051, dtype: bool\n\n\nCode\ntrain[\"Bonus\"].value_counts()\n\n\nBonus\n0.0    1211\n1.0     840\nName: count, dtype: int64\n\n\nCode\nax = sns.countplot(x=\"Bonus\", data=train, color=\"Blue\")\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n\n\nCode\nax.set(\n    xlabel=\"Bonus Eligible\", ylabel=\"Frequency\", title=\"Bar Graph of Bonus Eligibility\"\n)"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#chi2-distribution",
    "href": "notes/analytics/EDA/categorical-data/index.html#chi2-distribution",
    "title": "Categorical Data Analysis",
    "section": "2.1 \\(\\chi^2\\) Distribution",
    "text": "2.1 \\(\\chi^2\\) Distribution\nCharacteristics of the distribution:\n\nBounded below by 0\nRight skewed\nOne set of degrees of freedom\n\n\\[\n\\chi_P^2 = \\sum_{i=1}^{R}\\sum_{j=1}^{C} \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}\n\\]\n\nSumming over all the rows and columns and checking the difference between the observed frequencies and expected frequencies\n\\(d.f.\\) equals (# Rows - 1)(# Columns - 1)\n\n\nTo calculate expected cell counts, we take the proportion of column values and multiply them by the row totals to get that entry’s expected count."
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#pearson-chi-square-test",
    "href": "notes/analytics/EDA/categorical-data/index.html#pearson-chi-square-test",
    "title": "Categorical Data Analysis",
    "section": "2.2 Pearson Chi-Square Test",
    "text": "2.2 Pearson Chi-Square Test\nThink of Pearson Chi-Square Test as the categorical counterpart to Pearson correlation test with continuous variables.\n\n2.2.1 R Code\n\n\nCode\nchisq.test(table(train$Central_Air, train$Bonus))\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(train$Central_Air, train$Bonus)\nX-squared = 90.686, df = 1, p-value &lt; 2.2e-16\n\n\n\n\n2.2.2 Python Code\n\n\nCode\nfrom scipy.stats import chi2_contingency\n\nchi2_contingency(pd.crosstab(train[\"Central_Air\"], train[\"Bonus\"]))\n\n\nChi2ContingencyResult(statistic=90.6859058522855, pvalue=1.6838843977848939e-21, dof=1, expected_freq=array([[  86.79522184,   60.20477816],\n       [1124.20477816,  779.79522184]]))"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#likelihood-ratio-test",
    "href": "notes/analytics/EDA/categorical-data/index.html#likelihood-ratio-test",
    "title": "Categorical Data Analysis",
    "section": "2.3 Likelihood Ratio Test",
    "text": "2.3 Likelihood Ratio Test\n\\[\n\\chi_{LR}^2 = 2 \\cdot \\sum_{i=1}^{R}\\sum_{j=1}^{C} Obs_{i,j} \\cdot \\log(\\frac{Obs_{i,j}}{Exp_{i,j}})\n\\]\n\n\\(d.f.\\) equals (# Rows - 1)(# Columns - 1)"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#assumptions",
    "href": "notes/analytics/EDA/categorical-data/index.html#assumptions",
    "title": "Categorical Data Analysis",
    "section": "2.4 Assumptions",
    "text": "2.4 Assumptions\nBoth of the tests have a sample size requirement. The sample size requirement is 80% or more of the cells in the cross-tabulation table need expected count larger than 5."
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#fishers-exact-test",
    "href": "notes/analytics/EDA/categorical-data/index.html#fishers-exact-test",
    "title": "Categorical Data Analysis",
    "section": "2.5 Fisher’s Exact Test",
    "text": "2.5 Fisher’s Exact Test\nWhen we do not meet the assumption we use the Fisher’s exact test that calculates all possible permutations of data.\n\n2.5.1 R Code\n\n\nCode\nfisher.test(table(train$Central_Air, train$Bonus))\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  table(train$Central_Air, train$Bonus)\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  9.213525 69.646380\nsample estimates:\nodds ratio \n  22.16545 \n\n\n\n\n2.5.2 Python Code\n\n\nCode\nfrom scipy.stats import fisher_exact\n\nfisher_exact(pd.crosstab(train[\"Central_Air\"], train[\"Bonus\"]))\n\n\nSignificanceResult(statistic=22.18334892422825, pvalue=1.0365949500671678e-27)"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#ordinal-compared-to-nominal",
    "href": "notes/analytics/EDA/categorical-data/index.html#ordinal-compared-to-nominal",
    "title": "Categorical Data Analysis",
    "section": "2.6 Ordinal Compared to Nominal",
    "text": "2.6 Ordinal Compared to Nominal\nPearson and Likelihood Ratio tests can handle any type of categorical variable. However, ordinal variables provide extra information due to order mattering.\nWe can test for ordinal variables against other ordinal variables with Mantel-Haenszel Chi-Square Test. This test checks whether two ordinal variables have a linear relationship as compared to just a general one.\nHowever, if you are comparing nominal to ordinal, you have to stick to a general test of association with Pearson’s chi-square."
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#mantel-haenszel-chi-square-test",
    "href": "notes/analytics/EDA/categorical-data/index.html#mantel-haenszel-chi-square-test",
    "title": "Categorical Data Analysis",
    "section": "2.7 Mantel-Haenszel Chi-Square Test",
    "text": "2.7 Mantel-Haenszel Chi-Square Test\n\n\\(H_0:\\) There is no linear association\n\\(H_a:\\) There is a linear association\n\n\\[\n\\chi_{MH}^2 = (n - 1)r^2\n\\]\n\n\\(d.f.\\) equals 1\n\nJust because you fail to reject in Mantel-Haenszel it does not mean that there is no association. There is just no linear association\n\n\nCode\nlibrary(vcdExtra)\n\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n\nCode\nCMHtest(table(train$Central_Air, train$Bonus))$table[1, ]\n\n\n       Chisq           Df         Prob \n9.230619e+01 1.000000e+00 7.425180e-22 \n\n\nOne thing to keep in mind about this function is that it orders the variables alphanumerically. For more than two categories in a variable you need to make sure that the values reflect the correct order (e.g. encoding strings as ordered numerics)."
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#odds-ratios",
    "href": "notes/analytics/EDA/categorical-data/index.html#odds-ratios",
    "title": "Categorical Data Analysis",
    "section": "3.1 Odds Ratios",
    "text": "3.1 Odds Ratios\n\nOdds ratios indicates how much more likely, with respect to odds, a certain event occurs in one group relative to its occurrence in another group.\nOdds of an event occurring is not the same as probability.\n\\[\n\\text{Odds} = \\frac{p}{1 - p}\n\\]"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#cramers-v",
    "href": "notes/analytics/EDA/categorical-data/index.html#cramers-v",
    "title": "Categorical Data Analysis",
    "section": "3.2 Cramer’s V",
    "text": "3.2 Cramer’s V\n\\[\nV = \\sqrt{\\frac{(\\frac{\\chi_P^2}{n})}{\\min(\\text{Rows} - 1, \\text{Columns} - 1)}}\n\\]\n\nBounded between 0 and 1 (-1 and 1 for 2x2) where closer to 0 the weaker the relationship\n\n\n3.2.1 R Code\n\n\nCode\nlibrary(vcd)\nassocstats(table(train$Central_Air, train$Bonus))\n\n\n                     X^2 df P(&gt; X^2)\nLikelihood Ratio 121.499  1        0\nPearson           92.351  1        0\n\nPhi-Coefficient   : 0.212 \nContingency Coeff.: 0.208 \nCramer's V        : 0.212 \n\n\n\n\n3.2.2 Python Code\n\n\nCode\nfrom scipy.stats.contingency import association\n\nassociation(pd.crosstab(train['Central_Air'], train['Bonus']), method = 'cramer')\n\n\n0.21219662657018892"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/index.html#spearmans-correlation",
    "href": "notes/analytics/EDA/categorical-data/index.html#spearmans-correlation",
    "title": "Categorical Data Analysis",
    "section": "3.3 Spearman’s Correlation",
    "text": "3.3 Spearman’s Correlation\nMeasures the strength of association between two ordinal variables. Calculated with Pearson’s correlation on the ranks of the observations instead of the values of the observations.\n\n3.3.1 R Code\n\n\nCode\ncor.test(\n    x = as.numeric(ordered(train$Central_Air)),\n    y = as.numeric(ordered(train$Bonus)),\n    method = \"spearman\"\n)\n\n\nWarning in cor.test.default(x = as.numeric(ordered(train$Central_Air)), :\nCannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  as.numeric(ordered(train$Central_Air)) and as.numeric(ordered(train$Bonus))\nS = 1132826666, p-value &lt; 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2121966 \n\n\n\nAs you increase the number of Central_Air the number of Bonus increases\n\n\n\n3.3.2 Python Code\n\n\nCode\nfrom scipy.stats import spearmanr\n\nspearmanr(train['Central_Air'], train['Bonus'])\n\n\nSignificanceResult(statistic=0.21219662657018892, pvalue=2.604243816433701e-22)"
  },
  {
    "objectID": "notes/analytics/EDA/categorical-data/breakout_12.html",
    "href": "notes/analytics/EDA/categorical-data/breakout_12.html",
    "title": "Breakout 12",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\n\nset.seed(123)\nbike &lt;- bike %&gt;%\n    mutate(id = row_number())\ntrain &lt;- bike %&gt;% sample_frac(0.7)\ntest &lt;- anti_join(bike, train, by = \"id\")\n\n\n\n\nCode\ntrain$casual_high &lt;- train$casual &gt;= train$registered\n\n\n\n\nCode\nlibrary(vcdExtra)\n\n\nLoading required package: vcd\n\n\nLoading required package: grid\n\n\nLoading required package: gnm\n\n\n\nAttaching package: 'vcdExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    summarise\n\n\nCode\nchisq.test(table(train$casual_high, train$season))\n\n\n\n    Pearson's Chi-squared test\n\ndata:  table(train$casual_high, train$season)\nX-squared = 29.74, df = 3, p-value = 1.565e-06\n\n\nCode\nCMHtest(table(train$casual_high, train$season))$table[1, ]\n\n\n     Chisq         Df       Prob \n5.50559131 1.00000000 0.01895577 \n\n\n\n\nCode\nlibrary(DescTools)\n\nCMHtest(table(train$casual_high, train$holiday))$table[1, ]\n\n\n       Chisq           Df         Prob \n3.830649e+01 1.000000e+00 6.046125e-10 \n\n\nCode\nOddsRatio(table(train$casual_high, train$holiday))\n\n\n[1] 3.455253\n\n\nCode\ntable(train$casual_high, train$holiday)\n\n\n       \n            0     1\n  FALSE 11544   338\n  TRUE    257    26\n\n\nCode\n((11544 / (11544 + 338)) / (338 / (11544 + 338))) / ((257 / (257 + 26)) / (26 / (257 + 26)))\n\n\n[1] 3.455253\n\n\n\nTimes where casual users are higher than the registered users have 3.455 times the odds of being a holiday"
  },
  {
    "objectID": "notes/analytics/EDA/statistical-inference/lab_2.html",
    "href": "notes/analytics/EDA/statistical-inference/lab_2.html",
    "title": "Yang MSA",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(UsingR)\n\n\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\nLoading required package: HistData\nLoading required package: Hmisc\n\nAttaching package: 'Hmisc'\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n\nCode\ndata(normtemp)\nglimpse(normtemp)\n\n\nRows: 130\nColumns: 3\n$ temperature &lt;dbl&gt; 96.3, 96.7, 96.9, 97.0, 97.1, 97.1, 97.1, 97.2, 97.3, 97.4…\n$ gender      &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ hr          &lt;int&gt; 70, 71, 74, 80, 73, 75, 82, 64, 69, 70, 68, 72, 78, 70, 75…\n\n\nCode\n?normtemp\n\n\n\nRevisit the NormTemp dataset from Lab 1, where we examined the observed mean body temperature (temperature) in comparison to the well-known “average” of 98.6\n\n\n\nCode\nt.test(normtemp$temperature, mu = 98.6)\n\n\n\n    One Sample t-test\n\ndata:  normtemp$temperature\nt = -5.4548, df = 129, p-value = 2.411e-07\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.12200 98.37646\nsample estimates:\nmean of x \n 98.24923 \n\n\n\nThe p-value is 2.411e-07\nThe p-value is the probability that we observe our data given that the mean body temperature is 98.6\nGiven our low p-value at a significance level of \\(\\alpha = 0.05\\) we reject our null hypothesis. We believe the actual mean is significantly different from 98.6\nThe 95% confidence interval for temperature is [98.12200, 98.37646]\n\n\n\nCode\nfemales &lt;- normtemp[normtemp$gender == 2, ]\nt.test(females$temperature, mu = 98.6)\n\n\n\n    One Sample t-test\n\ndata:  females$temperature\nt = -2.2355, df = 64, p-value = 0.02888\nalternative hypothesis: true mean is not equal to 98.6\n95 percent confidence interval:\n 98.20962 98.57807\nsample estimates:\nmean of x \n 98.39385 \n\n\n\nGiven our p-value \\(&lt; \\alpha\\) our conclusion does not change\n\n\n\nCode\nggplot(normtemp, aes(sample = temperature, color = gender)) +\n    stat_qq() +\n    stat_qq_line()\n\n\nWarning: The following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\nThe following aesthetics were dropped during statistical transformation: colour\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n\n\n\n\n\n\nApproximately Normal\n\n\n\nCode\nvar.test(temperature ~ gender, data = normtemp)\n\n\n\n    F test to compare two variances\n\ndata:  temperature by gender\nF = 0.88329, num df = 64, denom df = 64, p-value = 0.6211\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.5387604 1.4481404\nsample estimates:\nratio of variances \n         0.8832897 \n\n\n\nWe believe variances are equal\n\n\n\nCode\nt.test(temperature ~ gender, data = normtemp)\n\n\n\n    Welch Two Sample t-test\n\ndata:  temperature by gender\nt = -2.2854, df = 127.51, p-value = 0.02394\nalternative hypothesis: true difference in means between group 1 and group 2 is not equal to 0\n95 percent confidence interval:\n -0.53964856 -0.03881298\nsample estimates:\nmean in group 1 mean in group 2 \n       98.10462        98.39385 \n\n\n\nAt a significance level of 0.05, we reject our null hypothesis that there is no difference in means between the two genders\n\n\nThe Airline dataset contains information regarding the number of international airline travelers (variable air) across different months of the year from 1949-1960. We are interested in knowing if during this time period there was a significant difference between air travel in the Summer months of June, July, and August vs. the remainder of the year? Use a statistical hypothesis test (alpha=0.05) to support your answer.\n\n\n\nCode\ndata(AirPassengers)\nlibrary(tseries)\n\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\nCode\nlibrary(forecast)\ncycle(AirPassengers)\n\n\n     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n1949   1   2   3   4   5   6   7   8   9  10  11  12\n1950   1   2   3   4   5   6   7   8   9  10  11  12\n1951   1   2   3   4   5   6   7   8   9  10  11  12\n1952   1   2   3   4   5   6   7   8   9  10  11  12\n1953   1   2   3   4   5   6   7   8   9  10  11  12\n1954   1   2   3   4   5   6   7   8   9  10  11  12\n1955   1   2   3   4   5   6   7   8   9  10  11  12\n1956   1   2   3   4   5   6   7   8   9  10  11  12\n1957   1   2   3   4   5   6   7   8   9  10  11  12\n1958   1   2   3   4   5   6   7   8   9  10  11  12\n1959   1   2   3   4   5   6   7   8   9  10  11  12\n1960   1   2   3   4   5   6   7   8   9  10  11  12\n\n\n\n\nCode\nair1 = data.frame(AirPassengers)\nair2 = air1 |&gt; mutate(summer = ifelse(cycle(AirPassengers) %in% 6:8, 1, 0))\n\n\n\n\nCode\nggplot(air2, aes(sample = AirPassengers, color = factor(summer))) +\n    stat_qq() +\n    stat_qq_line()\n\n\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous.\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous.\n\n\n\n\n\n\nNormality not met. We will use a nonparametric test\n\n\n\nCode\nggplot(air2, aes(x = AirPassengers, color = factor(summer))) +\n    geom_density()\n\n\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous.\n\n\n\n\n\n\nAfter plotting the distributions of both groups, we can see a similar shape but not a similar enough variation between the two groups.\nWhen we are conducting the Wilcoxon test we can’t necessarily claim anything about the mean or median but moreso about the distributional dominance\n\n\n\nCode\nwilcox.test(AirPassengers ~ summer, data = air2)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  AirPassengers by summer\nW = 1346.5, p-value = 0.00588\nalternative hypothesis: true location shift is not equal to 0\n\n\n\nAt a significance level of 0.05, we reject our null hypothesis that the true location shift is equal to 0"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html",
    "title": "ARIMA Models",
    "section": "",
    "text": ":::{.panel-tabset group = “language”} # R\n\n\nCode\nlibrary(tidyverse)\nlibrary(tseries)\nlibrary(forecast)\nlibrary(imputeTS)\n\nebay &lt;- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/ebay9899.csv\")\nhurricane &lt;- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/hurrican.csv\")"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#regression-line-trend",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#regression-line-trend",
    "title": "ARIMA Models",
    "section": "4.1 Regression Line Trend",
    "text": "4.1 Regression Line Trend\n\n\n\nTrend About a Regression Line\n\n\nOnce we fit the trend line then our residuals are stationary.\nA deterministic trend is a linear trend like we have done in regression:\n\\[\nY_t = \\beta_0 + \\beta_1 t + \\epsilon_t\n\\]\n\n\\(t\\) is time\nCan also fit quadratic, exponential, or any other form of time"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#random-walk-with-drift",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#random-walk-with-drift",
    "title": "ARIMA Models",
    "section": "4.2 Random Walk with Drift",
    "text": "4.2 Random Walk with Drift\n\\[\nY_t = \\omega + Y_{t-1} + e_t\n\\]\n\n\\(\\omega\\) controls the “drift” or trend. If positive, “drift” upward; if negative, “drift” downward.\n\nRandom Walk with drift is NOT stationary if you remove the trend line. We need to take differences."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#dickey-fuller-test-for-trend",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#dickey-fuller-test-for-trend",
    "title": "ARIMA Models",
    "section": "4.3 Dickey-Fuller Test for Trend",
    "text": "4.3 Dickey-Fuller Test for Trend\n\n\\(H_0: \\phi = 1\\). Random walk with drift\n\\(H_a: \\left| \\phi \\right| &lt; 1\\). Deterministic trend, not stochastic trend\n\nStochastic trend refers to a random walk. Even if we reject the null hypothesis we still have to take care of the trend in some way."
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#different-ways-to-interpolate",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#different-ways-to-interpolate",
    "title": "ARIMA Models",
    "section": "5.1 Different Ways to Interpolate",
    "text": "5.1 Different Ways to Interpolate\n\nFit a function between points\nLast observation carried forward\nWeighted moving average\nSummary statistics (e.g. mean or median of the series, but not always recommended)\nRandom sample\nAnd so on!\n\n:::{.panel-tabset group = “language”} # R\n\n\nCode\nlibrary(aTSA)\n\n\n\nAttaching package: 'aTSA'\n\n\nThe following object is masked from 'package:forecast':\n\n    forecast\n\n\nThe following objects are masked from 'package:tseries':\n\n    adf.test, kpss.test, pp.test\n\n\nThe following object is masked from 'package:graphics':\n\n    identify\n\n\nCode\ndaily_high &lt;- daily_high %&gt;%\n    na_interpolation(option = \"spline\")\n\nautoplot(daily_high) + labs(title = \"Daily High Stock Quotes\", x = \"Time\", y = \"Quotes\")\n\n\n\n\n\nCode\n# Now we can perform ADF test\nadf.test(daily_high)\n\n\nAugmented Dickey-Fuller Test \nalternative: stationary \n \nType 1: no drift no trend \n     lag   ADF p.value\n[1,]   0 0.414   0.763\n[2,]   1 0.148   0.687\n[3,]   2 0.266   0.720\n[4,]   3 0.330   0.739\n[5,]   4 0.385   0.755\n[6,]   5 0.434   0.769\nType 2: with drift no trend \n     lag   ADF p.value\n[1,]   0 -1.80   0.405\n[2,]   1 -2.01   0.324\n[3,]   2 -1.91   0.364\n[4,]   3 -1.90   0.367\n[5,]   4 -1.90   0.368\n[6,]   5 -1.92   0.361\nType 3: with drift and trend \n     lag   ADF p.value\n[1,]   0 -1.76   0.679\n[2,]   1 -2.13   0.520\n[3,]   2 -1.96   0.595\n[4,]   3 -1.89   0.622\n[5,]   4 -1.84   0.642\n[6,]   5 -1.82   0.653\n---- \nNote: in fact, p.value = 0.01 means p.value &lt;= 0.01 \n\n\n:::"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#finding-mixed-models-p-q",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#finding-mixed-models-p-q",
    "title": "ARIMA Models",
    "section": "5.2 Finding Mixed Models (p, q)",
    "text": "5.2 Finding Mixed Models (p, q)\nExplore correlation plots (ACF, PACF) to see what patterns there are.\nWhat to explore correlations on:\n\nIf series is stationary, just use series ggAcf(y.ts)\nIf series is a random walk we need to take differences: diff.y &lt;- diff(y.ts)\n\nAfter taking difference, run ADF test to see if stationary. If not stationary, keep taking additional differences\n\nIf series is a random walk with drift, use difference: diff.y &lt;- diff(y.ts)\nIf series is stationary about a line, use residuals: resid.y &lt;- lm(y ~ time)$resid"
  },
  {
    "objectID": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#automatic-searches",
    "href": "notes/analytics/timeseries/ARIMA/ARIMA/index.html#automatic-searches",
    "title": "ARIMA Models",
    "section": "6.1 Automatic Searches",
    "text": "6.1 Automatic Searches\n\nPlotting Patterns - ACF, PACF\nAutomatic Selection Techniques (R and Python)\n\nauto.arima function\n\n\n\n6.1.1 Recommendations\nIf there is a trend, test to see if it is a deterministic trend or random walk with drift.\nIf series has a deterministic trend, fit regression and then use automatic search on residuals. Otherwise, send series through automatic procedure (it will fit a difference if there is a trend).\nIf there is no trend, you can send series through automatic search.\n:::{.panel-tabset group = “language”} # R\n\n\nCode\nmax_velocity &lt;- hurricane$MeanVMax\nggplot_na_distribution(max_velocity) +\n    labs(y = \"Mean Max Velocity\")\n\n\n\n\n\n:::"
  },
  {
    "objectID": "notes/analytics/timeseries/introduction/index.html",
    "href": "notes/analytics/timeseries/introduction/index.html",
    "title": "Introduction to Forecasting & Time Series Structure",
    "section": "",
    "text": "Time series is an ordered sequence of observations, typically through equally spaced time intervals. If you have a missing interval, you need to impute that period (e.g. Jan, Feb, April; we need to impute March).\n\n\n\n\n\nField\nUse Case\n\n\n\n\nAgriculture\nCrop Production\n\n\nEconomics\nStock Prices\n\n\nEngineering\nElectric Signals\n\n\nMeteorology\nWind Speeds\n\n\nSocial Sciences\nCrime Rates\n\n\n\nTime Series Use Cases\n\n\nWe focus on univariate time series data for now.\n\n\n\nGoogle Stock 2018 - 2023"
  },
  {
    "objectID": "notes/analytics/timeseries/introduction/index.html#seasonally-adjusted-data",
    "href": "notes/analytics/timeseries/introduction/index.html#seasonally-adjusted-data",
    "title": "Introduction to Forecasting & Time Series Structure",
    "section": "3.1 Seasonally Adjusted Data",
    "text": "3.1 Seasonally Adjusted Data\nAn advantage of time series decompositive is that we can create seasonally adjusted data (i.e. remove the “effect of seasonality”).\n\\[\nY_t - S_t = T_t + R_t\n\\]\n\\[\n\\frac{Y_t}{S_t} = T_t \\times R_t\n\\]\nSeasonal length of the time series is the length of one season–how long until the series repeats the “pattern.”"
  },
  {
    "objectID": "notes/analytics/timeseries/introduction/index.html#stl-decomposition",
    "href": "notes/analytics/timeseries/introduction/index.html#stl-decomposition",
    "title": "Introduction to Forecasting & Time Series Structure",
    "section": "4.1 STL Decomposition",
    "text": "4.1 STL Decomposition\nSeasonal and Trend Decomposition using Loess. Smoothness of the trend is decided by the length of the seasonal window that the moving average uses.\n\n\nCode\nstl = STL(usair['Passengers'], period=12)\nres = stl.fit()\nfig = res.plot()\nplt.show()\n\n\n\n\n\nTo pull out the different decomposition components we can concatenate the different decomposition objects returned by the fit:\n\n\nCode\npd.concat([res.seasonal, res.trend, res.resid], axis=1)\n\n\n\n\n\n\n\n\n\nseason\ntrend\nresid\n\n\n\n\n1990-01-01\n-4209.074208\n38999.134669\n-442.060461\n\n\n1990-02-01\n-5433.547341\n38869.036724\n100.510617\n\n\n1990-03-01\n559.028725\n38740.856067\n1278.115209\n\n\n1990-04-01\n-602.850726\n38614.992784\n254.857942\n\n\n1990-05-01\n-71.241100\n38491.161768\n-170.920668\n\n\n...\n...\n...\n...\n\n\n2007-11-01\n-2674.853169\n64848.872472\n-207.019303\n\n\n2007-12-01\n-3718.499734\n64990.641956\n-365.142222\n\n\n2008-01-01\n-6755.005707\n65129.993257\n-574.987550\n\n\n2008-02-01\n-8822.280962\n65266.832075\n664.448887\n\n\n2008-03-01\n3061.324052\n65401.303910\n-644.627961\n\n\n\n\n219 rows × 3 columns\n\n\n\nWe can also overlay the trend (blue) and seasonally adjusted data (orange) on our time series data:\n\n\nCode\nplt.plot(usair['Passengers'])\nplt.plot(res.trend, color='blue')\n\nseas_adj = usair['Passengers'] - res.seasonal\n\nplt.plot(seas_adj, color='orange')\nplt.xlabel('Time')\nplt.ylabel('Passengers')\n\n\nText(0, 0.5, 'Passengers')"
  },
  {
    "objectID": "notes/analytics/timeseries/introduction/index.html#cautions",
    "href": "notes/analytics/timeseries/introduction/index.html#cautions",
    "title": "Introduction to Forecasting & Time Series Structure",
    "section": "5.1 Cautions",
    "text": "5.1 Cautions\nDecomposition will not tell you if have seasonal data nor length of seasonality."
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "",
    "text": "Single Exponential Smoothing can’t adequately handle data that is trending up or down.\nMultiple ways to incorporate a trend in the ESM:\n\nLinear / Holt Exponential Smoothing\nDamped Trend Exponential Smoothing\n\n\\[\n\\begin{align*}\n\\hat{Y}_{t + h} &= L_t + hT_t \\\\\nL_t &= \\theta Y_t + (1 - \\theta)(L_{t - 1} + T_{t - 1}) \\\\\nT_t &= \\gamma(L_t - L_{t - 1}) + (1 - \\gamma)T_{t - 1}\n\\end{align*}\n\\]\nAdding a component means we add a new smoothing parameter, \\(\\gamma\\). This second component incorporates trending into the model. \\(h\\) is the amount of time we are forecasting ahead.\n\n\nWe have a new dampening parameter incorporated into our model, \\(\\phi\\).\n\\[\n\\hat{Y}_{t+h} = L_t + \\sum_{i=1}^{h} \\phi^i T_t\n\\]\n\n\\(0 &lt; \\phi &lt; 1\\)\n\n\nPython\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import Holt\n\nsteel = pd.read_csv(\n    \"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/steel.csv\"\n)\ndf = pd.date_range(\"1/1/1984\", \"12/1/1991\", freq=\"MS\")\nsteel.set_index(pd.to_datetime(df), inplace=True)\n\nfit = Holt(steel[\"steelshp\"]).fit()\nfit.summary()\n\n\n\nHolt Model Results\n\n\nDep. Variable:\nsteelshp\nNo. Observations:\n96\n\n\nModel:\nHolt\nSSE\n22317588.149\n\n\nOptimized:\nTrue\nAIC\n1194.228\n\n\nTrend:\nAdditive\nBIC\n1204.485\n\n\nSeasonal:\nNone\nAICC\n1195.171\n\n\nSeasonal Periods:\nNone\nDate:\nThu, 07 Sep 2023\n\n\nBox-Cox:\nFalse\nTime:\n11:01:41\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.5454469\nalpha\nTrue\n\n\nsmoothing_trend\n0.0545996\nbeta\nTrue\n\n\ninitial_level\n5980.0000\nl.0\nFalse\n\n\ninitial_trend\n170.00000\nb.0\nFalse\n\n\n\n\n\n\n\nCode\nfit.forecast(24)\n\n\n1992-01-01    6394.260299\n1992-02-01    6375.055005\n1992-03-01    6355.849712\n1992-04-01    6336.644418\n1992-05-01    6317.439124\n1992-06-01    6298.233830\n1992-07-01    6279.028536\n1992-08-01    6259.823242\n1992-09-01    6240.617948\n1992-10-01    6221.412654\n1992-11-01    6202.207360\n1992-12-01    6183.002066\n1993-01-01    6163.796772\n1993-02-01    6144.591478\n1993-03-01    6125.386184\n1993-04-01    6106.180890\n1993-05-01    6086.975596\n1993-06-01    6067.770302\n1993-07-01    6048.565008\n1993-08-01    6029.359714\n1993-09-01    6010.154420\n1993-10-01    5990.949127\n1993-11-01    5971.743833\n1993-12-01    5952.538539\nFreq: MS, dtype: float64\n\n\nWe can run a damped trend using the damped_trend parameter:\n\n\nCode\nfit = Holt(steel[\"steelshp\"], damped_trend=True).fit()\nfit.summary()\n\n\n\nHolt Model Results\n\n\nDep. Variable:\nsteelshp\nNo. Observations:\n96\n\n\nModel:\nHolt\nSSE\n22092850.854\n\n\nOptimized:\nTrue\nAIC\n1195.256\n\n\nTrend:\nAdditive\nBIC\n1208.078\n\n\nSeasonal:\nNone\nAICC\n1196.529\n\n\nSeasonal Periods:\nNone\nDate:\nThu, 07 Sep 2023\n\n\nBox-Cox:\nFalse\nTime:\n11:01:41\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.5235714\nalpha\nTrue\n\n\nsmoothing_trend\n0.0506682\nbeta\nTrue\n\n\ninitial_level\n5980.0000\nl.0\nFalse\n\n\ninitial_trend\n168.30000\nb.0\nFalse\n\n\ndamping_trend\n0.9900000\nphi\nTrue"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#damped-trend-exponential-smoothing",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#damped-trend-exponential-smoothing",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "",
    "text": "We have a new dampening parameter incorporated into our model, \\(\\phi\\).\n\\[\n\\hat{Y}_{t+h} = L_t + \\sum_{i=1}^{h} \\phi^i T_t\n\\]\n\n\\(0 &lt; \\phi &lt; 1\\)\n\n\nPython\n\n\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom statsmodels.tsa.api import Holt\n\nsteel = pd.read_csv(\n    \"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/steel.csv\"\n)\ndf = pd.date_range(\"1/1/1984\", \"12/1/1991\", freq=\"MS\")\nsteel.set_index(pd.to_datetime(df), inplace=True)\n\nfit = Holt(steel[\"steelshp\"]).fit()\nfit.summary()\n\n\n\nHolt Model Results\n\n\nDep. Variable:\nsteelshp\nNo. Observations:\n96\n\n\nModel:\nHolt\nSSE\n22317588.149\n\n\nOptimized:\nTrue\nAIC\n1194.228\n\n\nTrend:\nAdditive\nBIC\n1204.485\n\n\nSeasonal:\nNone\nAICC\n1195.171\n\n\nSeasonal Periods:\nNone\nDate:\nThu, 07 Sep 2023\n\n\nBox-Cox:\nFalse\nTime:\n11:01:41\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.5454469\nalpha\nTrue\n\n\nsmoothing_trend\n0.0545996\nbeta\nTrue\n\n\ninitial_level\n5980.0000\nl.0\nFalse\n\n\ninitial_trend\n170.00000\nb.0\nFalse\n\n\n\n\n\n\n\nCode\nfit.forecast(24)\n\n\n1992-01-01    6394.260299\n1992-02-01    6375.055005\n1992-03-01    6355.849712\n1992-04-01    6336.644418\n1992-05-01    6317.439124\n1992-06-01    6298.233830\n1992-07-01    6279.028536\n1992-08-01    6259.823242\n1992-09-01    6240.617948\n1992-10-01    6221.412654\n1992-11-01    6202.207360\n1992-12-01    6183.002066\n1993-01-01    6163.796772\n1993-02-01    6144.591478\n1993-03-01    6125.386184\n1993-04-01    6106.180890\n1993-05-01    6086.975596\n1993-06-01    6067.770302\n1993-07-01    6048.565008\n1993-08-01    6029.359714\n1993-09-01    6010.154420\n1993-10-01    5990.949127\n1993-11-01    5971.743833\n1993-12-01    5952.538539\nFreq: MS, dtype: float64\n\n\nWe can run a damped trend using the damped_trend parameter:\n\n\nCode\nfit = Holt(steel[\"steelshp\"], damped_trend=True).fit()\nfit.summary()\n\n\n\nHolt Model Results\n\n\nDep. Variable:\nsteelshp\nNo. Observations:\n96\n\n\nModel:\nHolt\nSSE\n22092850.854\n\n\nOptimized:\nTrue\nAIC\n1195.256\n\n\nTrend:\nAdditive\nBIC\n1208.078\n\n\nSeasonal:\nNone\nAICC\n1196.529\n\n\nSeasonal Periods:\nNone\nDate:\nThu, 07 Sep 2023\n\n\nBox-Cox:\nFalse\nTime:\n11:01:41\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.5235714\nalpha\nTrue\n\n\nsmoothing_trend\n0.0506682\nbeta\nTrue\n\n\ninitial_level\n5980.0000\nl.0\nFalse\n\n\ninitial_trend\n168.30000\nb.0\nFalse\n\n\ndamping_trend\n0.9900000\nphi\nTrue"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#additive-model",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#additive-model",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "2.1 Additive Model",
    "text": "2.1 Additive Model\n\\[\n\\begin{align*}\n\\hat{Y}_{t+h} &= L_t + hT_t + S_{t-p+h} \\\\\nL_t &= \\theta(Y_t - S_{t-p}) + (1 - \\theta)(L_{t-1} + T_{t-1}) \\\\\nT_t &= \\gamma(L_t - L_{t-1}) + (1 - \\gamma)T_{t-1} \\\\\nS_t = \\delta(Y_t - L_{t-1} - T_{t-1}) + (1 - \\delta)(S_{t-p})\n\\end{align*}\n\\]\n\n\\(p\\) is the length of the season"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#multiplicative-model",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#multiplicative-model",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "2.2 Multiplicative Model",
    "text": "2.2 Multiplicative Model\n\\[\n\\hat{Y}_{t+h} = (L_t + hT_t)S_{t-p+h}\n\\]\n\nPython\n\n\n\n\nCode\nfrom statsmodels.tsa.api import ExponentialSmoothing\n\nusair_p = pd.read_csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv\")\ndf = pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')\nusair_p.set_index(pd.to_datetime(df), inplace=True)\n\nusair_p['HWES_ADD'] = ExponentialSmoothing(usair_p['Passengers'], trend='add', seasonal='add', seasonal_periods=12).fit().fittedvalues\n\nhw_add = ExponentialSmoothing(usair_p['Passengers'], trend='add', seasonal='add', seasonal_periods=12).fit()\nhw_add.summary()\n\nhw_mult = ExponentialSmoothing(usair_p['Passengers'], trend='add', seasonal='mul', seasonal_periods=12).fit()\nhw_mult.summary()\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:917: ConvergenceWarning:\n\nOptimization failed to converge. Check mle_retvals.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:917: ConvergenceWarning:\n\nOptimization failed to converge. Check mle_retvals.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/statsmodels/tsa/holtwinters/model.py:917: ConvergenceWarning:\n\nOptimization failed to converge. Check mle_retvals.\n\n\n\n\nExponentialSmoothing Model Results\n\n\nDep. Variable:\nPassengers\nNo. Observations:\n219\n\n\nModel:\nExponentialSmoothing\nSSE\n810065387.907\n\n\nOptimized:\nTrue\nAIC\n3344.058\n\n\nTrend:\nAdditive\nBIC\n3398.283\n\n\nSeasonal:\nMultiplicative\nAICC\n3347.478\n\n\nSeasonal Periods:\n12\nDate:\nThu, 07 Sep 2023\n\n\nBox-Cox:\nFalse\nTime:\n11:01:41\n\n\nBox-Cox Coeff.:\nNone\n\n\n\n\n\n\n\n\n\ncoeff\ncode\noptimized\n\n\nsmoothing_level\n0.3939286\nalpha\nTrue\n\n\nsmoothing_trend\n0.0218849\nbeta\nTrue\n\n\nsmoothing_seasonal\n0.3030357\ngamma\nTrue\n\n\ninitial_level\n38232.794\nl.0\nTrue\n\n\ninitial_trend\n-95.864899\nb.0\nTrue\n\n\ninitial_seasons.0\n0.8828055\ns.0\nTrue\n\n\ninitial_seasons.1\n0.8473624\ns.1\nTrue\n\n\ninitial_seasons.2\n1.0108916\ns.2\nTrue\n\n\ninitial_seasons.3\n0.9777503\ns.3\nTrue\n\n\ninitial_seasons.4\n1.0088677\ns.4\nTrue\n\n\ninitial_seasons.5\n1.0761260\ns.5\nTrue\n\n\ninitial_seasons.6\n1.1385855\ns.6\nTrue\n\n\ninitial_seasons.7\n1.1824059\ns.7\nTrue\n\n\ninitial_seasons.8\n0.9607449\ns.8\nTrue\n\n\ninitial_seasons.9\n1.0081718\ns.9\nTrue\n\n\ninitial_seasons.10\n0.9385590\ns.10\nTrue\n\n\ninitial_seasons.11\n0.9677292\ns.11\nTrue"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#judment-forecasting",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#judment-forecasting",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "3.1 Judment Forecasting",
    "text": "3.1 Judment Forecasting\nForecasts are found using quantitative or modeling approaches. However, there are instances where models are not availabe and qualitative or judgment forecast is used. Occasionally, we merge the two together."
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#accuracy-vs.-goodness-of-fit",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#accuracy-vs.-goodness-of-fit",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "3.2 Accuracy vs. Goodness-of-Fit",
    "text": "3.2 Accuracy vs. Goodness-of-Fit\nGoodness-of-fit is calculated on the same sample used to build the model.\nA diagnostic statistic calculated using a hold out sample taht was not used in model building is an accuracy statistic."
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#mean-absolute-percent-error",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#mean-absolute-percent-error",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "5.1 Mean Absolute Percent Error",
    "text": "5.1 Mean Absolute Percent Error\n\\[\n\\text{MAPE} = \\frac{1}{n}\\sum_{t=1}^n \\left| \\frac{Y_t - \\hat{Y}_t}{Y_t} \\right|\n\\]\n\nCan overweight over-predictions\nCan’t divide by 0"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#mean-absolute-error",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#mean-absolute-error",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "5.2 Mean Absolute Error",
    "text": "5.2 Mean Absolute Error\n\\[\n\\text{MAE} = \\frac{1}{n}\\sum_{t=1}^n \\left| Y_t - \\hat{Y}_t \\right|\n\\]\n\nNot scale invariant"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#square-root-of-mean-square-error",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#square-root-of-mean-square-error",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "5.3 Square Root of Mean Square Error",
    "text": "5.3 Square Root of Mean Square Error\n\\[\n\\text{RMSE} = \\sqrt{\\frac{1}{n}\\sum_{t=1}^n (Y_t - \\hat{Y}_t)^2}\n\\]\n\nOverweight of larger errors\nNot scale invariant"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#symmetric-mean-absolute-percent-error",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#symmetric-mean-absolute-percent-error",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "5.4 Symmetric Mean Absolute Percent Error",
    "text": "5.4 Symmetric Mean Absolute Percent Error\n\\[\n\\text{sMAPE} = \\frac{1}{n}\\sum_{t=1}^n \\frac{\\left| Y_t - \\hat{Y}_t \\right|}{(\\left| Y_t \\right| + \\left| \\hat{Y}_t \\right|)}\n\\]\n\nDivide by 0\nStill asymmetric"
  },
  {
    "objectID": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#comparison-across-diagnostics",
    "href": "notes/analytics/timeseries/exponential-smoothing/linear-esm/index.html#comparison-across-diagnostics",
    "title": "Linear Trend for Exponential Smoothing",
    "section": "5.5 Comparison Across Diagnostics",
    "text": "5.5 Comparison Across Diagnostics\n\n\n\nComparison Across Diagnostics"
  },
  {
    "objectID": "notes/analytics/timeseries/correlation-functions/index.html",
    "href": "notes/analytics/timeseries/correlation-functions/index.html",
    "title": "Correlation Functions",
    "section": "",
    "text": "When we have a stationary distribution we can now look at correlation. If we have a random walk, then we cannot look at the correlation.\nTime series is typically analyzed with an assumption that observations have a potential relationship across time. One example is weight over time."
  },
  {
    "objectID": "notes/analytics/timeseries/correlation-functions/index.html#partial-autocorrelation-function",
    "href": "notes/analytics/timeseries/correlation-functions/index.html#partial-autocorrelation-function",
    "title": "Correlation Functions",
    "section": "1.1 Partial Autocorrelation Function",
    "text": "1.1 Partial Autocorrelation Function\nPartial correlation is the correlation between two sets of observations, separated by \\(k\\) points in time, after adjusting for all previous (\\(1, 2, \\cdots, k-1\\)) autocorrelations.\n\\[\n\\phi_k = \\text{Corr}(Y_t, Y_{t-k} | Y_{t-1}, Y_{t-2}, \\cdots, Y_{t-k-1})\n\\]\n\n\n\nPartial Autocorrelation Function\n\n\nThe partial autocorrelation for the \\(k\\)th lag is calculated from the regression:\n\\[\nY_t = \\beta_0 + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\cdots + \\phi_kY_{t-k} + \\epsilon_t\n\\]\nThe 2nd partial autocorrelation \\(\\phi_2\\) is estimated from:\n\\[\n\\hat{Y}_t = \\hat{\\beta}_0 + \\hat{\\phi}_1Y_{t-1} + \\hat{\\phi}_2Y_{t-2}\n\\]"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/lab_9.html",
    "href": "notes/analytics/linear-regression/influence/lab_9.html",
    "title": "Lab 9",
    "section": "",
    "text": "Code\nlibrary(AppliedPredictiveModeling)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(lmtest)\n\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nCode\ndata(FuelEconomy)\n\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(idx = row_number())\nglimpse(cars2010)\n\n\nRows: 1,107\nColumns: 15\n$ EngDispl            &lt;dbl&gt; 4.7, 4.7, 4.2, 4.2, 5.2, 5.2, 2.0, 6.0, 3.0, 3.0, …\n$ NumCyl              &lt;int&gt; 8, 8, 8, 8, 10, 10, 4, 12, 6, 6, 6, 6, 16, 8, 8, 8…\n$ Transmission        &lt;fct&gt; AM6, M6, M6, AM6, AM6, M6, S6, S6, S6, M6, S7, M6,…\n$ FE                  &lt;dbl&gt; 28.0198, 25.6094, 26.8000, 25.0451, 24.8000, 23.90…\n$ AirAspirationMethod &lt;fct&gt; NaturallyAspirated, NaturallyAspirated, NaturallyA…\n$ NumGears            &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 7, 6, 6, 6, 6,…\n$ TransLockup         &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ TransCreeperGear    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ DriveDesc           &lt;fct&gt; TwoWheelDriveRear, TwoWheelDriveRear, AllWheelDriv…\n$ IntakeValvePerCyl   &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,…\n$ ExhaustValvesPerCyl &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,…\n$ CarlineClassDesc    &lt;fct&gt; 2Seaters, 2Seaters, 2Seaters, 2Seaters, 2Seaters, …\n$ VarValveTiming      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,…\n$ VarValveLift        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ idx                 &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,…\n\n\nCode\ncars2010.model &lt;- lm(FE ~ EngDispl + Transmission + AirAspirationMethod + TransLockup + TransCreeperGear + DriveDesc + IntakeValvePerCyl + CarlineClassDesc + VarValveLift, data = cars2010)\n\n\n\n\n\n\nCode\ncars2010.time &lt;- lm(FE ~ idx, data = cars2010)\n\ndwtest(cars2010.time, alternative = \"greater\")\n\n\n\n    Durbin-Watson test\n\ndata:  cars2010.time\nDW = 0.71797, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0\n\n\n\n\n\nStudentized residuals:\n\n\nCode\nggplot(cars2010.model, aes(x = cars2010$idx, y = rstudent(cars2010.model))) +\n    geom_point() +\n    geom_line(y = -3) +\n    geom_line(y = 3) +\n    labs(title = \"External Studentized Residuals\", x = \"Observation\", y = \"Residuals\")\n\n\n\n\n\nCode\ndf.cut &lt;- 2 * (sqrt((length(cars2010.model$coefficients)) / nrow(cars2010)))\n\nggplot(cars2010.model, aes(x = cars2010$idx, y = dffits(cars2010.model))) +\n    geom_point(color = \"orange\") +\n    geom_line(y = df.cut) +\n    geom_line(y = -df.cut) +\n    labs(title = \"DFFITS\", x = \"Observation\", y = \"DFFITS\")\n\n\n\n\n\nCode\nrstud &lt;- abs(rstudent(cars2010.model))\ndf.test &lt;- abs(dffits(cars2010.model))\n\nlength(union(\n    names(rstud[rstud &gt; 3]),\n    names(df.test[df.test &gt; 1])\n))\n\n\n[1] 21\n\n\nCode\nlength(rstud[rstud &gt; 3])\n\n\n[1] 18\n\n\n\n\nCode\nmax(cooks.distance(cars2010.model))\n\n\n[1] 0.06202869"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/lab_9.html#a",
    "href": "notes/analytics/linear-regression/influence/lab_9.html#a",
    "title": "Lab 9",
    "section": "",
    "text": "Code\ncars2010.time &lt;- lm(FE ~ idx, data = cars2010)\n\ndwtest(cars2010.time, alternative = \"greater\")\n\n\n\n    Durbin-Watson test\n\ndata:  cars2010.time\nDW = 0.71797, p-value &lt; 2.2e-16\nalternative hypothesis: true autocorrelation is greater than 0"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/lab_9.html#b",
    "href": "notes/analytics/linear-regression/influence/lab_9.html#b",
    "title": "Lab 9",
    "section": "",
    "text": "Studentized residuals:\n\n\nCode\nggplot(cars2010.model, aes(x = cars2010$idx, y = rstudent(cars2010.model))) +\n    geom_point() +\n    geom_line(y = -3) +\n    geom_line(y = 3) +\n    labs(title = \"External Studentized Residuals\", x = \"Observation\", y = \"Residuals\")\n\n\n\n\n\nCode\ndf.cut &lt;- 2 * (sqrt((length(cars2010.model$coefficients)) / nrow(cars2010)))\n\nggplot(cars2010.model, aes(x = cars2010$idx, y = dffits(cars2010.model))) +\n    geom_point(color = \"orange\") +\n    geom_line(y = df.cut) +\n    geom_line(y = -df.cut) +\n    labs(title = \"DFFITS\", x = \"Observation\", y = \"DFFITS\")\n\n\n\n\n\nCode\nrstud &lt;- abs(rstudent(cars2010.model))\ndf.test &lt;- abs(dffits(cars2010.model))\n\nlength(union(\n    names(rstud[rstud &gt; 3]),\n    names(df.test[df.test &gt; 1])\n))\n\n\n[1] 21\n\n\nCode\nlength(rstud[rstud &gt; 3])\n\n\n[1] 18\n\n\n\n\nCode\nmax(cooks.distance(cars2010.model))\n\n\n[1] 0.06202869"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html",
    "href": "notes/analytics/linear-regression/influence/index.html",
    "title": "Correlated Error Terms",
    "section": "",
    "text": "When we’re trying to understand correlated error terms, we need to know the source of our data:"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#durbin-watson-statistic-first-order",
    "href": "notes/analytics/linear-regression/influence/index.html#durbin-watson-statistic-first-order",
    "title": "Correlated Error Terms",
    "section": "1.1 Durbin-Watson Statistic (First-Order)",
    "text": "1.1 Durbin-Watson Statistic (First-Order)\nWe can also assess autocorrelation using the Durbin-Watson statistic which compares a residual against the previous time residual over the sum of residuals squared.\n\\[\nd = \\frac{\\sum_{t=2}^{T} (e_t - e_{t-1})^2}{\\sum_{t=1}^{T} e_t^2}\n\\]\n\nBounded in \\([0, 4]\\)\nWhen \\(d=2\\), fail to reject \\(H_0\\) and assume there is not enough evidence supporting autocorrelation\n\\(d &lt; 2\\), possible positive autocorrelation\n\\(d &gt; 2\\), possible negative autocorrelation"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#handling-correlated-errors",
    "href": "notes/analytics/linear-regression/influence/index.html#handling-correlated-errors",
    "title": "Correlated Error Terms",
    "section": "1.2 Handling Correlated Errors",
    "text": "1.2 Handling Correlated Errors\n\nIf correlated due to time, perform time series\nIf correlated due to clustered data, perform a hierarchical model\nLongitudinal analysis/panel data"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#diagnostic-statistics",
    "href": "notes/analytics/linear-regression/influence/index.html#diagnostic-statistics",
    "title": "Correlated Error Terms",
    "section": "2.1 Diagnostic Statistics",
    "text": "2.1 Diagnostic Statistics\n\n\n\n\n\nflowchart LR\n    A[Detecting Outliers] --&gt; B[Standardized Residuals]\n    A --&gt; C[Studentized Residuals]\n    X[Detecting Influential Obs.] --&gt; D[Cook's D]\n    X --&gt; E[DFFITS]\n    X --&gt; F[DFBETAS]\n    X --&gt; G[Hat Values]\n\n\n\n\n\n\n\n2.1.1 Studentized Residuals\nDivide the residuals by their standard errors after deleting that one observation\n\n\\(\\left|SR\\right| &gt; 2\\) for datasets with a relatively small number of observations\n\\(\\left|SR\\right| &gt; 3\\) for datasets with relatively large number of observations\n\n\n\n2.1.2 Cook’s D\nMeasures the difference in the regression estimates when the \\(i^{th}\\) observation is left out\nCutoff formula:\n\\[\nD_i &gt; \\frac{4}{n - p - 1}\n\\]\n\n\\(p\\) is the number of parameters including the intercept\n\n\n\n2.1.3 DFFITS\nMeasures impact that the \\(i^{th}\\) observation has on predicted value\n\\[\n\\left| DFFITS_i \\right| &gt; 2\\sqrt{\\frac{p}{n}}\n\\]\n\n\n2.1.4 Hat Values\nFrom the normal equation, the estimate of the parameters is:\n\\[\nb = (X'X)^{-1}X'y\n\\]\nEstimated line is:\n\\[\n\\hat{y} = X(X'X)^{-1}X'y\n\\]\nWith the hat values being:\n\\[\nX(X'X)^{-1}X'\n\\]\nSuggested cutoff is:\n\\[\nh_{ii} &gt; \\frac{2p}{n}\n\\]\n\n\n2.1.5 DFBETA\nMeasure of change in the \\(j^{th}\\) parameter estimate with deletion of \\(i^{th}\\) observation.\nOne DFBETA per parameter per observation. Helps to explain which parameter coefficient the influence most lies.\n\\[\n\\left| DFBETA_{ij} \\right| &gt; 2 \\sqrt{\\frac{1}{n}}\n\\]"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#scottish-hill-races",
    "href": "notes/analytics/linear-regression/influence/index.html#scottish-hill-races",
    "title": "Correlated Error Terms",
    "section": "2.2 Scottish Hill Races",
    "text": "2.2 Scottish Hill Races\n\n\nCode\nlibrary(tidyverse)\n\nurl &lt;- \"http://www.statsci.org/data/general/hills.txt\"\nraces.table &lt;- read.table(url, header = TRUE, sep = \"\\t\")\n\nraces.table &lt;- races.table %&gt;%\n    mutate(idx = row_number())\n\nlm.model &lt;- lm(Time ~ Distance + Climb, data = races.table)\n\nggplot(lm.model, aes(x = races.table$idx, y = rstudent(lm.model))) +\n    geom_point(color = \"orange\") +\n    geom_line(y = -3) +\n    geom_line(y = 3) +\n    labs(title = \"External Studentized Residuals\", x = \"Observation\", y = \"Residuals\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#how-to-handle-influential-observations",
    "href": "notes/analytics/linear-regression/influence/index.html#how-to-handle-influential-observations",
    "title": "Correlated Error Terms",
    "section": "2.3 How to Handle Influential Observations",
    "text": "2.3 How to Handle Influential Observations\n\nRecheck data to ensure no transcription or data entry errors occurred.\nIf data is valid, maybe model is inadequate\n\nHigher-order terms may be necessary\nNonlinear model\n\nDetermine robustness of the inference by running analysis with and without influential observations\nRobust Regression\nWeighted Least Squares"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#collinearity-diagnostics",
    "href": "notes/analytics/linear-regression/influence/index.html#collinearity-diagnostics",
    "title": "Correlated Error Terms",
    "section": "3.1 Collinearity Diagnostics",
    "text": "3.1 Collinearity Diagnostics\nWe can look at correlation matrix of predictors, but there is also the variance inflation factor that we can consider:\n\\[\nVIF_i = \\frac{1}{1 - R_i^2}\n\\]\n\n\\(R_i^2\\) is the \\(R^2\\) value with all the other variables predicting \\(x_i\\)\n\\(VIF &gt; 10\\) indicate collinearity\n\n\n\nCode\nlibrary(car)\n\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nCode\ncor(mtcars)\n\n\n            mpg        cyl       disp         hp        drat         wt\nmpg   1.0000000 -0.8521620 -0.8475514 -0.7761684  0.68117191 -0.8676594\ncyl  -0.8521620  1.0000000  0.9020329  0.8324475 -0.69993811  0.7824958\ndisp -0.8475514  0.9020329  1.0000000  0.7909486 -0.71021393  0.8879799\nhp   -0.7761684  0.8324475  0.7909486  1.0000000 -0.44875912  0.6587479\ndrat  0.6811719 -0.6999381 -0.7102139 -0.4487591  1.00000000 -0.7124406\nwt   -0.8676594  0.7824958  0.8879799  0.6587479 -0.71244065  1.0000000\nqsec  0.4186840 -0.5912421 -0.4336979 -0.7082234  0.09120476 -0.1747159\nvs    0.6640389 -0.8108118 -0.7104159 -0.7230967  0.44027846 -0.5549157\nam    0.5998324 -0.5226070 -0.5912270 -0.2432043  0.71271113 -0.6924953\ngear  0.4802848 -0.4926866 -0.5555692 -0.1257043  0.69961013 -0.5832870\ncarb -0.5509251  0.5269883  0.3949769  0.7498125 -0.09078980  0.4276059\n            qsec         vs          am       gear        carb\nmpg   0.41868403  0.6640389  0.59983243  0.4802848 -0.55092507\ncyl  -0.59124207 -0.8108118 -0.52260705 -0.4926866  0.52698829\ndisp -0.43369788 -0.7104159 -0.59122704 -0.5555692  0.39497686\nhp   -0.70822339 -0.7230967 -0.24320426 -0.1257043  0.74981247\ndrat  0.09120476  0.4402785  0.71271113  0.6996101 -0.09078980\nwt   -0.17471588 -0.5549157 -0.69249526 -0.5832870  0.42760594\nqsec  1.00000000  0.7445354 -0.22986086 -0.2126822 -0.65624923\nvs    0.74453544  1.0000000  0.16834512  0.2060233 -0.56960714\nam   -0.22986086  0.1683451  1.00000000  0.7940588  0.05753435\ngear -0.21268223  0.2060233  0.79405876  1.0000000  0.27407284\ncarb -0.65624923 -0.5696071  0.05753435  0.2740728  1.00000000\n\n\nCode\nlm.model &lt;- lm(mpg ~ ., data = mtcars)\nv &lt;- vif(lm.model)\nv[v &gt; 10]\n\n\n     cyl     disp       wt \n15.37383 21.62024 15.16489"
  },
  {
    "objectID": "notes/analytics/linear-regression/influence/index.html#dealing-with-multicollinearity",
    "href": "notes/analytics/linear-regression/influence/index.html#dealing-with-multicollinearity",
    "title": "Correlated Error Terms",
    "section": "3.2 Dealing with Multicollinearity",
    "text": "3.2 Dealing with Multicollinearity\n\nExclude redundant independent variables\nRedefine variables\nUse biased regression techniques (e.g. LASSO)\nCenter the independent variables in polynomial regression models or models with interaction terms\n\nSubtract each value of the predictor by the mean of that column\n\n\nYou should be dealing with multicollinearity before you do any model selection.\nAny time you take or add variables in, you should be modifying one at a time and recalculating VIF at each step."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html",
    "href": "notes/analytics/linear-regression/MLR/index.html",
    "title": "Multiple Linear Regression",
    "section": "",
    "text": "Models with more than one predictor variable are called multiple regression models.\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\varepsilon\n\\]\nWe are still trying to minimize the sum of squared errors:\n\\[\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n\\]\nLinear in MLR refers to the linear combination of variables in the model–not how they’re visualized in multiple dimensions.\nA model like \\(y = \\beta_0 + \\beta_1x_1 + \\beta_1x_1^2\\) is still a linear regression!"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#r-code",
    "href": "notes/analytics/linear-regression/MLR/index.html#r-code",
    "title": "Multiple Linear Regression",
    "section": "1.1 R Code",
    "text": "1.1 R Code\n\n\nCode\nlibrary(tidyverse)\nlibrary(AmesHousing)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\names &lt;- make_ordinal_ames()\nset.seed(123)\names &lt;- ames |&gt; mutate(id = row_number())\ntrain &lt;- ames |&gt; sample_frac(0.7)\ntest &lt;- anti_join(ames, train, by = \"id\")\n\names_lm2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, train)\nsummary(ames_lm2)\n\n\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-528656  -30077   -1230   21427  361465 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    42562.657   5365.721   7.932 3.51e-15 ***\nGr_Liv_Area      136.982      4.207  32.558  &lt; 2e-16 ***\nTotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56630 on 2048 degrees of freedom\nMultiple R-squared:  0.5024,    Adjusted R-squared:  0.5019 \nF-statistic:  1034 on 2 and 2048 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#python-code",
    "href": "notes/analytics/linear-regression/MLR/index.html#python-code",
    "title": "Multiple Linear Regression",
    "section": "1.2 Python Code",
    "text": "1.2 Python Code\n\n\nCode\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = r.train\names_lm2 = smf.ols(\"Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd\", train).fit()\n\names_lm2.f_pvalue\n\n\n4.3720518917664e-311"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#assumptions",
    "href": "notes/analytics/linear-regression/MLR/index.html#assumptions",
    "title": "Multiple Linear Regression",
    "section": "2.1 Assumptions",
    "text": "2.1 Assumptions\n\nThe mean of \\(y\\) is accurately modeled by a linear function of the independent variables\n\\(\\varepsilon\\) is Normal with a mean of 0\n\\(\\varepsilon\\) has a constant variance\nThe errors are independent\nNo perfect collinearity"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#multicollinearity",
    "href": "notes/analytics/linear-regression/MLR/index.html#multicollinearity",
    "title": "Multiple Linear Regression",
    "section": "2.2 Multicollinearity",
    "text": "2.2 Multicollinearity\nMulticollinearity is when predictor variables are correlated with one another.\nNo perfect collinearity means no predictor variables as a perfect linear combination of each other. In practice, we only care when collinearity has a significant impact.\n\n2.2.1 R Code\n\n\nCode\npar(mfrow = c(2, 2))\nplot(ames_lm2)\n\n\n\n\n\n\n\n2.2.2 Python Code\n\n\nCode\ntrain[\"resid\"] = ames_lm2.resid\ntrain[\"predict\"] = ames_lm2.predict()\nax = sns.relplot(train, x=\"predict\", y=\"resid\")\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n\n\nCode\nplt.show()\n\n\n\n\n\n\n\nCode\nsm.qqplot(train[\"resid\"])"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#predict",
    "href": "notes/analytics/linear-regression/MLR/index.html#predict",
    "title": "Multiple Linear Regression",
    "section": "4.1 Predict",
    "text": "4.1 Predict\nDevelop a model to predict future values of a response variable based on its relationship with other predictor variables.\nThe parameters in the model and their statistical significance are secondary importance. Focus is on producing a model that can predict future values well.\nWe should take care to be aware of and address overfitting a model in this case."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#explain",
    "href": "notes/analytics/linear-regression/MLR/index.html#explain",
    "title": "Multiple Linear Regression",
    "section": "4.2 Explain",
    "text": "4.2 Explain\nTo develop an understanding of relationships between the response and predictor.\nThe statistical significance of coefficients as well as the magnitudes and signs of coefficients are important."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#adjusted-coefficient-of-determination",
    "href": "notes/analytics/linear-regression/MLR/index.html#adjusted-coefficient-of-determination",
    "title": "Multiple Linear Regression",
    "section": "5.1 Adjusted Coefficient of Determination",
    "text": "5.1 Adjusted Coefficient of Determination\n\\(R_a^2\\) penalizes a model for adding variables that do not provide useful information.\n\\[\n\\begin{align*}\nR_a^2 &= 1 - [(\\frac{n - 1}{n - k - 1})(\\frac{SSE}{TSS})] \\\\\n&= 1 - [(1 - R^2)(\\frac{n - 1}{n - k - 1})]\n\\end{align*}\n\\]\n\n\\(R_a^2 \\leq R^2\\)\n\nAlthough better at determining utility of model, we lose the interpretation as the coefficient can be negative."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#dummy-encoding",
    "href": "notes/analytics/linear-regression/MLR/index.html#dummy-encoding",
    "title": "Multiple Linear Regression",
    "section": "6.1 Dummy Encoding",
    "text": "6.1 Dummy Encoding\nWe can encode categorical variables with dummy encoding.\n\\[\nx_1 =\n\\begin{cases}\n1 \\hspace{0.2cm} \\text{if Y}\\\\\n0 \\hspace{0.2cm} \\text{if N}\n\\end{cases}\n\\]\n\\(\\beta_1\\) will represent the average difference between category Y and N\n\n\nCode\nimport numpy as np\n\nsimple_array = np.array([0, 2, 1])\nencoded_array = np.zeros((simple_array.size, simple_array.max() + 1), dtype=int)\n\nencoded_array[np.arange(simple_array.size), simple_array] = 1\n\nencoded_array\n\n\narray([[1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0]])"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/index.html#effects-encoding",
    "href": "notes/analytics/linear-regression/MLR/index.html#effects-encoding",
    "title": "Multiple Linear Regression",
    "section": "6.2 Effects Encoding",
    "text": "6.2 Effects Encoding\n\\[\nx_1 =\n\\begin{cases}\n1 \\hspace{0.2cm} \\text{if Y} \\\\\n-1 \\hspace{0.2cm} \\text{if N}\n\\end{cases}\n\\]\n\\(\\beta_1\\) is the average difference between category Y and the overall average of categories Y and N. Essentially, the overall average of all groups in a category variable."
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/lab_6.html",
    "href": "notes/analytics/linear-regression/MLR/lab_6.html",
    "title": "Lab 6",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(AppliedPredictiveModeling)\ndata(FuelEconomy)\nglimpse(cars2010)\n\n\nRows: 1,107\nColumns: 14\n$ EngDispl            &lt;dbl&gt; 4.7, 4.7, 4.2, 4.2, 5.2, 5.2, 2.0, 6.0, 3.0, 3.0, …\n$ NumCyl              &lt;int&gt; 8, 8, 8, 8, 10, 10, 4, 12, 6, 6, 6, 6, 16, 8, 8, 8…\n$ Transmission        &lt;fct&gt; AM6, M6, M6, AM6, AM6, M6, S6, S6, S6, M6, S7, M6,…\n$ FE                  &lt;dbl&gt; 28.0198, 25.6094, 26.8000, 25.0451, 24.8000, 23.90…\n$ AirAspirationMethod &lt;fct&gt; NaturallyAspirated, NaturallyAspirated, NaturallyA…\n$ NumGears            &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 6, 7, 6, 6, 6, 6,…\n$ TransLockup         &lt;dbl&gt; 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,…\n$ TransCreeperGear    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ DriveDesc           &lt;fct&gt; TwoWheelDriveRear, TwoWheelDriveRear, AllWheelDriv…\n$ IntakeValvePerCyl   &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,…\n$ ExhaustValvesPerCyl &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1,…\n$ CarlineClassDesc    &lt;fct&gt; 2Seaters, 2Seaters, 2Seaters, 2Seaters, 2Seaters, …\n$ VarValveTiming      &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,…\n$ VarValveLift        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,…\n\n\n\n\n\n\nCode\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(across(!c(EngDispl, FE), as.factor))\ncars_lm &lt;- lm(FE ~ ., data = cars2010)\nsummary(cars_lm)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars2010)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6399  -1.6583   0.0582   1.5708  21.6002 \n\nCoefficients: (5 not defined because of singularities)\n                                                 Estimate Std. Error t value\n(Intercept)                                      35.95655    3.34991  10.734\nEngDispl                                         -2.24571    0.26269  -8.549\nNumCyl3                                          15.88136    5.11001   3.108\nNumCyl4                                           7.76711    3.94430   1.969\nNumCyl5                                           4.89858    3.97620   1.232\nNumCyl6                                           4.19528    3.94208   1.064\nNumCyl8                                           2.51528    3.98438   0.631\nNumCyl10                                         -0.01541    4.13445  -0.004\nNumCyl12                                         -1.02329    4.11855  -0.248\nNumCyl16                                         -0.31250    5.42018  -0.058\nTransmissionA4                                   -6.93754    2.27354  -3.051\nTransmissionA5                                   -6.53146    2.27928  -2.866\nTransmissionA6                                   -4.88712    2.27829  -2.145\nTransmissionA7                                    5.70476    2.44239   2.336\nTransmissionAM6                                  -9.48575    2.46325  -3.851\nTransmissionAM7                                   0.59731    2.65233   0.225\nTransmissionAV                                   -4.40251    2.28451  -1.927\nTransmissionAVS6                                 -6.72835    2.41754  -2.783\nTransmissionM5                                   -7.00105    2.27746  -3.074\nTransmissionM6                                   -7.03693    2.26627  -3.105\nTransmissionS4                                  -10.42310    2.42863  -4.292\nTransmissionS5                                   -7.17879    2.30519  -3.114\nTransmissionS6                                   -5.09671    2.26315  -2.252\nTransmissionS7                                    4.08689    2.51141   1.627\nTransmissionS8                                   -4.61764    4.02547  -1.147\nAirAspirationMethodSupercharged                  -1.66003    0.78945  -2.103\nAirAspirationMethodTurbocharged                  -1.12911    0.32214  -3.505\nNumGears4                                              NA         NA      NA\nNumGears5                                              NA         NA      NA\nNumGears6                                              NA         NA      NA\nNumGears7                                       -10.74200    3.27897  -3.276\nNumGears8                                         1.78308    3.19710   0.558\nTransLockup1                                     -0.89442    0.35715  -2.504\nTransCreeperGear1                                -1.04006    0.49553  -2.099\nDriveDescFourWheelDrive                          -0.45145    0.43461  -1.039\nDriveDescParttimeFourWheelDrive                  -0.29399    1.06503  -0.276\nDriveDescTwoWheelDriveFront                       4.31845    0.37701  11.455\nDriveDescTwoWheelDriveRear                        1.19634    0.37255   3.211\nIntakeValvePerCyl1                                6.33644    3.32150   1.908\nIntakeValvePerCyl2                                4.88952    3.21060   1.523\nIntakeValvePerCyl3                                     NA         NA      NA\nExhaustValvesPerCyl1                              1.54229    0.75433   2.045\nExhaustValvesPerCyl2                                   NA         NA      NA\nCarlineClassDesc2Seaters                          2.85693    1.17833   2.425\nCarlineClassDescCompactCars                       3.78908    1.09963   3.446\nCarlineClassDescLargeCars                         2.56219    1.13079   2.266\nCarlineClassDescMidsizeCars                       3.39390    1.09686   3.094\nCarlineClassDescMinicompactCars                   3.63416    1.19375   3.044\nCarlineClassDescSmallPickupTrucks2WD             -1.85140    1.25181  -1.479\nCarlineClassDescSmallPickupTrucks4WD             -0.95072    1.35268  -0.703\nCarlineClassDescSmallStationWagons                2.20724    1.13608   1.943\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.07995    1.36307  -1.526\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.51997    1.10807  -1.372\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.56991    1.12243  -0.508\nCarlineClassDescStandardPickupTrucks2WD          -1.74467    1.27006  -1.374\nCarlineClassDescStandardPickupTrucks4WD          -1.94205    1.30286  -1.491\nCarlineClassDescSubcompactCars                    3.43057    1.11242   3.084\nCarlineClassDescVansCargoTypes                   -4.07446    1.51702  -2.686\nCarlineClassDescVansPassengerType                -4.27396    1.95092  -2.191\nVarValveTiming1                                   0.15943    0.29071   0.548\nVarValveLift1                                     0.82579    0.30704   2.690\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         0.001935 ** \nNumCyl4                                         0.049193 *  \nNumCyl5                                         0.218234    \nNumCyl6                                         0.287469    \nNumCyl8                                         0.527992    \nNumCyl10                                        0.997028    \nNumCyl12                                        0.803827    \nNumCyl16                                        0.954034    \nTransmissionA4                                  0.002335 ** \nTransmissionA5                                  0.004245 ** \nTransmissionA6                                  0.032175 *  \nTransmissionA7                                  0.019693 *  \nTransmissionAM6                                 0.000125 ***\nTransmissionAM7                                 0.821867    \nTransmissionAV                                  0.054234 .  \nTransmissionAVS6                                0.005480 ** \nTransmissionM5                                  0.002166 ** \nTransmissionM6                                  0.001953 ** \nTransmissionS4                                  1.94e-05 ***\nTransmissionS5                                  0.001894 ** \nTransmissionS6                                  0.024526 *  \nTransmissionS7                                  0.103967    \nTransmissionS8                                  0.251599    \nAirAspirationMethodSupercharged                 0.035723 *  \nAirAspirationMethodTurbocharged                 0.000476 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001087 ** \nNumGears8                                       0.577155    \nTransLockup1                                    0.012420 *  \nTransCreeperGear1                               0.036067 *  \nDriveDescFourWheelDrive                         0.299167    \nDriveDescParttimeFourWheelDrive                 0.782571    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.001362 ** \nIntakeValvePerCyl1                              0.056702 .  \nIntakeValvePerCyl2                              0.128077    \nIntakeValvePerCyl3                                    NA    \nExhaustValvesPerCyl1                            0.041146 *  \nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015495 *  \nCarlineClassDescCompactCars                     0.000592 ***\nCarlineClassDescLargeCars                       0.023664 *  \nCarlineClassDescMidsizeCars                     0.002026 ** \nCarlineClassDescMinicompactCars                 0.002390 ** \nCarlineClassDescSmallPickupTrucks2WD            0.139447    \nCarlineClassDescSmallPickupTrucks4WD            0.482307    \nCarlineClassDescSmallStationWagons              0.052300 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.127330    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.170443    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.611738    \nCarlineClassDescStandardPickupTrucks2WD         0.169828    \nCarlineClassDescStandardPickupTrucks4WD         0.136365    \nCarlineClassDescSubcompactCars                  0.002097 ** \nCarlineClassDescVansCargoTypes                  0.007349 ** \nCarlineClassDescVansPassengerType               0.028690 *  \nVarValveTiming1                                 0.583517    \nVarValveLift1                                   0.007269 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.14 on 1051 degrees of freedom\nMultiple R-squared:  0.8333,    Adjusted R-squared:  0.8246 \nF-statistic: 95.55 on 55 and 1051 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe F p-value is significant meaning that our overall model is significant in predicting FE\nThe 13 variables explain 83.33 percent of variation in fuel economy\n\n\n\n\n\n\nCode\ncar::Anova(cars_lm)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              720.6    1 73.0842 &lt; 2.2e-16 ***\nNumCyl                889.6    6 15.0374 &lt; 2.2e-16 ***\nTransmission          707.7   12  5.9813 3.553e-10 ***\nAirAspirationMethod   151.2    2  7.6686 0.0004939 ***\nNumGears              109.2    2  5.5361 0.0040576 ** \nTransLockup            61.8    1  6.2715 0.0124202 *  \nTransCreeperGear       43.4    1  4.4052 0.0360667 *  \nDriveDesc            1535.0    4 38.9205 &lt; 2.2e-16 ***\nIntakeValvePerCyl      56.6    2  2.8720 0.0570315 .  \nExhaustValvesPerCyl    41.2    1  4.1803 0.0411460 *  \nCarlineClassDesc     3495.4   16 22.1561 &lt; 2.2e-16 ***\nVarValveTiming          3.0    1  0.3008 0.5835171    \nVarValveLift           71.3    1  7.2336 0.0072685 ** \nResiduals           10363.0 1051                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nHighest p-value is VarValveTiming with 0.584\n\n\n\n\n\n\nCode\ncars_2010_sub &lt;- cars2010 %&gt;%\n    select(-VarValveTiming)\n\ncars_lm2 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm2)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6242  -1.6504   0.0541   1.5540  21.5852 \n\nCoefficients: (5 not defined because of singularities)\n                                                 Estimate Std. Error t value\n(Intercept)                                      35.99395    3.34810  10.751\nEngDispl                                         -2.26003    0.26130  -8.649\nNumCyl3                                          16.00965    5.10296   3.137\nNumCyl4                                           7.91363    3.93394   2.012\nNumCyl5                                           5.03429    3.96717   1.269\nNumCyl6                                           4.35537    3.92996   1.108\nNumCyl8                                           2.71420    3.96652   0.684\nNumCyl10                                          0.18914    4.11622   0.046\nNumCyl12                                         -0.81976    4.10043  -0.200\nNumCyl16                                         -0.08503    5.40249  -0.016\nTransmissionA4                                   -6.98768    2.27095  -3.077\nTransmissionA5                                   -6.53850    2.27848  -2.870\nTransmissionA6                                   -4.90059    2.27740  -2.152\nTransmissionA7                                    5.71566    2.44150   2.341\nTransmissionAM6                                  -9.49295    2.46240  -3.855\nTransmissionAM7                                   0.59242    2.65143   0.223\nTransmissionAV                                   -4.40233    2.28375  -1.928\nTransmissionAVS6                                 -6.72853    2.41673  -2.784\nTransmissionM5                                   -7.01977    2.27644  -3.084\nTransmissionM6                                   -7.04911    2.26540  -3.112\nTransmissionS4                                  -10.51052    2.42259  -4.339\nTransmissionS5                                   -7.20354    2.30398  -3.127\nTransmissionS6                                   -5.10605    2.26234  -2.257\nTransmissionS7                                    4.09144    2.51056   1.630\nTransmissionS8                                   -4.62893    4.02408  -1.150\nAirAspirationMethodSupercharged                  -1.67713    0.78857  -2.127\nAirAspirationMethodTurbocharged                  -1.13613    0.32177  -3.531\nNumGears4                                              NA         NA      NA\nNumGears5                                              NA         NA      NA\nNumGears6                                              NA         NA      NA\nNumGears7                                       -10.74376    3.27788  -3.278\nNumGears8                                         1.79152    3.19600   0.561\nTransLockup1                                     -0.89373    0.35703  -2.503\nTransCreeperGear1                                -1.09666    0.48450  -2.263\nDriveDescFourWheelDrive                          -0.44026    0.43399  -1.014\nDriveDescParttimeFourWheelDrive                  -0.25475    1.06227  -0.240\nDriveDescTwoWheelDriveFront                       4.32752    0.37652  11.494\nDriveDescTwoWheelDriveRear                        1.19041    0.37227   3.198\nIntakeValvePerCyl1                                6.36976    3.31984   1.919\nIntakeValvePerCyl2                                4.90088    3.20946   1.527\nIntakeValvePerCyl3                                     NA         NA      NA\nExhaustValvesPerCyl1                              1.49669    0.74949   1.997\nExhaustValvesPerCyl2                                   NA         NA      NA\nCarlineClassDesc2Seaters                          2.85888    1.17793   2.427\nCarlineClassDescCompactCars                       3.77775    1.09907   3.437\nCarlineClassDescLargeCars                         2.55092    1.13023   2.257\nCarlineClassDescMidsizeCars                       3.38958    1.09647   3.091\nCarlineClassDescMinicompactCars                   3.62150    1.19313   3.035\nCarlineClassDescSmallPickupTrucks2WD             -1.88501    1.24990  -1.508\nCarlineClassDescSmallPickupTrucks4WD             -0.99946    1.34930  -0.741\nCarlineClassDescSmallStationWagons                2.19864    1.13559   1.936\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.14681    1.35716  -1.582\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.52376    1.10769  -1.376\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.58904    1.12151  -0.525\nCarlineClassDescStandardPickupTrucks2WD          -1.76375    1.26916  -1.390\nCarlineClassDescStandardPickupTrucks4WD          -1.97791    1.30079  -1.521\nCarlineClassDescSubcompactCars                    3.42813    1.11204   3.083\nCarlineClassDescVansCargoTypes                   -4.03720    1.51499  -2.665\nCarlineClassDescVansPassengerType                -4.21030    1.94682  -2.163\nVarValveLift1                                     0.82139    0.30683   2.677\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         0.001752 ** \nNumCyl4                                         0.044513 *  \nNumCyl5                                         0.204726    \nNumCyl6                                         0.268007    \nNumCyl8                                         0.493951    \nNumCyl10                                        0.963360    \nNumCyl12                                        0.841581    \nNumCyl16                                        0.987446    \nTransmissionA4                                  0.002145 ** \nTransmissionA5                                  0.004191 ** \nTransmissionA6                                  0.031638 *  \nTransmissionA7                                  0.019416 *  \nTransmissionAM6                                 0.000123 ***\nTransmissionAM7                                 0.823240    \nTransmissionAV                                  0.054164 .  \nTransmissionAVS6                                0.005463 ** \nTransmissionM5                                  0.002098 ** \nTransmissionM6                                  0.001911 ** \nTransmissionS4                                  1.57e-05 ***\nTransmissionS5                                  0.001817 ** \nTransmissionS6                                  0.024214 *  \nTransmissionS7                                  0.103466    \nTransmissionS8                                  0.250279    \nAirAspirationMethodSupercharged                 0.033668 *  \nAirAspirationMethodTurbocharged                 0.000432 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001081 ** \nNumGears8                                       0.575223    \nTransLockup1                                    0.012458 *  \nTransCreeperGear1                               0.023808 *  \nDriveDescFourWheelDrive                         0.310604    \nDriveDescParttimeFourWheelDrive                 0.810521    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.001427 ** \nIntakeValvePerCyl1                              0.055293 .  \nIntakeValvePerCyl2                              0.127060    \nIntakeValvePerCyl3                                    NA    \nExhaustValvesPerCyl1                            0.046088 *  \nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015390 *  \nCarlineClassDescCompactCars                     0.000611 ***\nCarlineClassDescLargeCars                       0.024213 *  \nCarlineClassDescMidsizeCars                     0.002045 ** \nCarlineClassDescMinicompactCars                 0.002462 ** \nCarlineClassDescSmallPickupTrucks2WD            0.131822    \nCarlineClassDescSmallPickupTrucks4WD            0.459026    \nCarlineClassDescSmallStationWagons              0.053120 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.113986    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.169229    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.599543    \nCarlineClassDescStandardPickupTrucks2WD         0.164913    \nCarlineClassDescStandardPickupTrucks4WD         0.128675    \nCarlineClassDescSubcompactCars                  0.002105 ** \nCarlineClassDescVansCargoTypes                  0.007821 ** \nCarlineClassDescVansPassengerType               0.030792 *  \nVarValveLift1                                   0.007544 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.139 on 1052 degrees of freedom\nMultiple R-squared:  0.8333,    Adjusted R-squared:  0.8247 \nF-statistic: 97.38 on 54 and 1052 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm2)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              737.1    1 74.8066 &lt; 2.2e-16 ***\nNumCyl                887.9    6 15.0181 &lt; 2.2e-16 ***\nTransmission          712.6   12  6.0266 2.849e-10 ***\nAirAspirationMethod   153.8    2  7.8053 0.0004316 ***\nNumGears              109.2    2  5.5431 0.0040294 ** \nTransLockup            61.7    1  6.2661 0.0124578 *  \nTransCreeperGear       50.5    1  5.1234 0.0238082 *  \nDriveDesc            1545.6    4 39.2146 &lt; 2.2e-16 ***\nIntakeValvePerCyl      57.9    2  2.9386 0.0533736 .  \nExhaustValvesPerCyl    39.3    1  3.9878 0.0460877 *  \nCarlineClassDesc     3504.2   16 22.2267 &lt; 2.2e-16 ***\nVarValveLift           70.6    1  7.1663 0.0075441 ** \nResiduals           10366.0 1052                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value for the model did not change significantly\n\\(R^2\\) did not change significantly and \\(R_a^2\\) increased a little\n\n\n\n\n\nDropping IntakeValvePerCyl:\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-IntakeValvePerCyl)\n\ncars_lm3 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm3)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5964  -1.7032   0.0413   1.5730  21.6235 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      36.0050     3.3542  10.734\nEngDispl                                         -2.1120     0.2530  -8.346\nNumCyl3                                          20.9056     3.9117   5.344\nNumCyl4                                          12.6843     2.2897   5.540\nNumCyl5                                           9.6779     2.3617   4.098\nNumCyl6                                           9.0036     2.3148   3.889\nNumCyl8                                           7.0608     2.4285   2.908\nNumCyl10                                          4.4870     2.6277   1.708\nNumCyl12                                          3.0682     2.6138   1.174\nNumCyl16                                          3.8516     4.3735   0.881\nTransmissionA4                                   -7.1007     2.2745  -3.122\nTransmissionA5                                   -6.7679     2.2799  -2.968\nTransmissionA6                                   -5.2017     2.2777  -2.284\nTransmissionA7                                    5.3619     2.4409   2.197\nTransmissionAM6                                  -9.5275     2.4663  -3.863\nTransmissionAM7                                   0.5339     2.6561   0.201\nTransmissionAV                                   -4.5677     2.2867  -1.998\nTransmissionAVS6                                 -6.9582     2.4186  -2.877\nTransmissionM5                                   -7.2063     2.2788  -3.162\nTransmissionM6                                   -7.1857     2.2683  -3.168\nTransmissionS4                                  -10.7361     2.4247  -4.428\nTransmissionS5                                   -7.3945     2.3063  -3.206\nTransmissionS6                                   -5.2347     2.2655  -2.311\nTransmissionS7                                    3.8448     2.5130   1.530\nTransmissionS8                                   -4.6021     4.0315  -1.142\nAirAspirationMethodSupercharged                  -1.8711     0.7825  -2.391\nAirAspirationMethodTurbocharged                  -1.1602     0.3221  -3.602\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.6135     3.2834  -3.233\nNumGears8                                         1.7231     3.2016   0.538\nTransLockup1                                     -0.9069     0.3576  -2.536\nTransCreeperGear1                                -1.2475     0.4777  -2.612\nDriveDescFourWheelDrive                          -0.4799     0.4345  -1.105\nDriveDescParttimeFourWheelDrive                  -0.3666     1.0628  -0.345\nDriveDescTwoWheelDriveFront                       4.3236     0.3772  11.462\nDriveDescTwoWheelDriveRear                        1.1403     0.3722   3.063\nExhaustValvesPerCyl1                              2.7454     0.3965   6.925\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          2.7680     1.1781   2.350\nCarlineClassDescCompactCars                       3.7770     1.1011   3.430\nCarlineClassDescLargeCars                         2.5646     1.1323   2.265\nCarlineClassDescMidsizeCars                       3.3732     1.0985   3.071\nCarlineClassDescMinicompactCars                   3.6283     1.1952   3.036\nCarlineClassDescSmallPickupTrucks2WD             -1.9032     1.2522  -1.520\nCarlineClassDescSmallPickupTrucks4WD             -1.0239     1.3517  -0.757\nCarlineClassDescSmallStationWagons                2.1934     1.1377   1.928\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.1150     1.3596  -1.556\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5296     1.1097  -1.378\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.5922     1.1235  -0.527\nCarlineClassDescStandardPickupTrucks2WD          -1.7232     1.2714  -1.355\nCarlineClassDescStandardPickupTrucks4WD          -1.9711     1.3031  -1.513\nCarlineClassDescSubcompactCars                    3.4139     1.1141   3.064\nCarlineClassDescVansCargoTypes                   -3.9331     1.5169  -2.593\nCarlineClassDescVansPassengerType                -4.0805     1.9494  -2.093\nVarValveLift1                                     0.8053     0.3070   2.623\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         1.11e-07 ***\nNumCyl4                                         3.83e-08 ***\nNumCyl5                                         4.49e-05 ***\nNumCyl6                                         0.000107 ***\nNumCyl8                                         0.003719 ** \nNumCyl10                                        0.088005 .  \nNumCyl12                                        0.240713    \nNumCyl16                                        0.378695    \nTransmissionA4                                  0.001846 ** \nTransmissionA5                                  0.003061 ** \nTransmissionA6                                  0.022584 *  \nTransmissionA7                                  0.028260 *  \nTransmissionAM6                                 0.000119 ***\nTransmissionAM7                                 0.840740    \nTransmissionAV                                  0.046025 *  \nTransmissionAVS6                                0.004096 ** \nTransmissionM5                                  0.001610 ** \nTransmissionM6                                  0.001580 ** \nTransmissionS4                                  1.05e-05 ***\nTransmissionS5                                  0.001385 ** \nTransmissionS6                                  0.021044 *  \nTransmissionS7                                  0.126328    \nTransmissionS8                                  0.253901    \nAirAspirationMethodSupercharged                 0.016962 *  \nAirAspirationMethodTurbocharged                 0.000331 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001265 ** \nNumGears8                                       0.590565    \nTransLockup1                                    0.011365 *  \nTransCreeperGear1                               0.009142 ** \nDriveDescFourWheelDrive                         0.269550    \nDriveDescParttimeFourWheelDrive                 0.730220    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.002245 ** \nExhaustValvesPerCyl1                            7.58e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.018981 *  \nCarlineClassDescCompactCars                     0.000626 ***\nCarlineClassDescLargeCars                       0.023715 *  \nCarlineClassDescMidsizeCars                     0.002189 ** \nCarlineClassDescMinicompactCars                 0.002458 ** \nCarlineClassDescSmallPickupTrucks2WD            0.128836    \nCarlineClassDescSmallPickupTrucks4WD            0.448949    \nCarlineClassDescSmallStationWagons              0.054126 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.120096    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.168369    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.598240    \nCarlineClassDescStandardPickupTrucks2WD         0.175593    \nCarlineClassDescStandardPickupTrucks4WD         0.130664    \nCarlineClassDescSubcompactCars                  0.002237 ** \nCarlineClassDescVansCargoTypes                  0.009648 ** \nCarlineClassDescVansPassengerType               0.036568 *  \nVarValveLift1                                   0.008839 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.145 on 1054 degrees of freedom\nMultiple R-squared:  0.8324,    Adjusted R-squared:  0.8241 \nF-statistic: 100.6 on 52 and 1054 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm3)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              688.9    1 69.6604 &lt; 2.2e-16 ***\nNumCyl               1000.0    6 16.8523 &lt; 2.2e-16 ***\nTransmission          699.4   12  5.8935 5.430e-10 ***\nAirAspirationMethod   169.7    2  8.5817 0.0002009 ***\nNumGears              106.5    2  5.3833 0.0047196 ** \nTransLockup            63.6    1  6.4299 0.0113653 *  \nTransCreeperGear       67.4    1  6.8201 0.0091419 ** \nDriveDesc            1559.4    4 39.4183 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   474.2    1 47.9516 7.583e-12 ***\nCarlineClassDesc     3489.4   16 22.0517 &lt; 2.2e-16 ***\nVarValveLift           68.0    1  6.8807 0.0088387 ** \nResiduals           10423.9 1054                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-TransLockup)\n\ncars_lm4 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm4)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6831  -1.7755   0.0193   1.5767  22.1002 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      35.5540     3.3581  10.588\nEngDispl                                         -2.0676     0.2531  -8.170\nNumCyl3                                          20.3570     3.9157   5.199\nNumCyl4                                          12.6350     2.2955   5.504\nNumCyl5                                           9.6799     2.3677   4.088\nNumCyl6                                           8.8784     2.3203   3.826\nNumCyl8                                           6.8252     2.4329   2.805\nNumCyl10                                          4.2129     2.6322   1.601\nNumCyl12                                          2.7488     2.6175   1.050\nNumCyl16                                          4.1444     4.3832   0.946\nTransmissionA4                                   -7.5779     2.2725  -3.335\nTransmissionA5                                   -7.2188     2.2788  -3.168\nTransmissionA6                                   -5.5830     2.2785  -2.450\nTransmissionA7                                    4.7030     2.4333   1.933\nTransmissionAM6                                  -9.6845     2.4718  -3.918\nTransmissionAM7                                   0.5441     2.6629   0.204\nTransmissionAV                                   -4.5705     2.2925  -1.994\nTransmissionAVS6                                 -6.8711     2.4245  -2.834\nTransmissionM5                                   -6.7947     2.2788  -2.982\nTransmissionM6                                   -6.8804     2.2710  -3.030\nTransmissionS4                                  -11.2457     2.4226  -4.642\nTransmissionS5                                   -7.8915     2.3038  -3.425\nTransmissionS6                                   -5.5467     2.2680  -2.446\nTransmissionS7                                    3.1101     2.5027   1.243\nTransmissionS8                                   -5.4739     4.0271  -1.359\nAirAspirationMethodSupercharged                  -1.7876     0.7838  -2.281\nAirAspirationMethodTurbocharged                  -1.1043     0.3222  -3.427\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.2165     3.2880  -3.107\nNumGears8                                         2.2205     3.2038   0.693\nTransCreeperGear1                                -1.2791     0.4788  -2.672\nDriveDescFourWheelDrive                          -0.4976     0.4355  -1.143\nDriveDescParttimeFourWheelDrive                  -0.4061     1.0655  -0.381\nDriveDescTwoWheelDriveFront                       4.3413     0.3781  11.482\nDriveDescTwoWheelDriveRear                        1.1035     0.3729   2.959\nExhaustValvesPerCyl1                              2.7948     0.3970   7.040\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          2.8530     1.1807   2.416\nCarlineClassDescCompactCars                       3.7853     1.1039   3.429\nCarlineClassDescLargeCars                         2.5289     1.1351   2.228\nCarlineClassDescMidsizeCars                       3.3020     1.1009   2.999\nCarlineClassDescMinicompactCars                   3.7542     1.1972   3.136\nCarlineClassDescSmallPickupTrucks2WD             -1.9188     1.2554  -1.528\nCarlineClassDescSmallPickupTrucks4WD             -1.0621     1.3551  -0.784\nCarlineClassDescSmallStationWagons                2.1620     1.1405   1.896\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.2133     1.3625  -1.624\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5927     1.1122  -1.432\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.6482     1.1262  -0.576\nCarlineClassDescStandardPickupTrucks2WD          -1.7299     1.2746  -1.357\nCarlineClassDescStandardPickupTrucks4WD          -2.0053     1.3064  -1.535\nCarlineClassDescSubcompactCars                    3.3939     1.1169   3.039\nCarlineClassDescVansCargoTypes                   -3.9518     1.5207  -2.599\nCarlineClassDescVansPassengerType                -4.0897     1.9544  -2.093\nVarValveLift1                                     0.8016     0.3078   2.604\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                        8.79e-16 ***\nNumCyl3                                         2.41e-07 ***\nNumCyl4                                         4.66e-08 ***\nNumCyl5                                         4.68e-05 ***\nNumCyl6                                         0.000138 ***\nNumCyl8                                         0.005119 ** \nNumCyl10                                        0.109781    \nNumCyl12                                        0.293871    \nNumCyl16                                        0.344606    \nTransmissionA4                                  0.000884 ***\nTransmissionA5                                  0.001580 ** \nTransmissionA6                                  0.014438 *  \nTransmissionA7                                  0.053532 .  \nTransmissionAM6                                 9.50e-05 ***\nTransmissionAM7                                 0.838126    \nTransmissionAV                                  0.046450 *  \nTransmissionAVS6                                0.004685 ** \nTransmissionM5                                  0.002933 ** \nTransmissionM6                                  0.002507 ** \nTransmissionS4                                  3.88e-06 ***\nTransmissionS5                                  0.000638 ***\nTransmissionS6                                  0.014620 *  \nTransmissionS7                                  0.214253    \nTransmissionS8                                  0.174354    \nAirAspirationMethodSupercharged                 0.022764 *  \nAirAspirationMethodTurbocharged                 0.000633 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001939 ** \nNumGears8                                       0.488417    \nTransCreeperGear1                               0.007665 ** \nDriveDescFourWheelDrive                         0.253447    \nDriveDescParttimeFourWheelDrive                 0.703142    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.003153 ** \nExhaustValvesPerCyl1                            3.46e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015841 *  \nCarlineClassDescCompactCars                     0.000629 ***\nCarlineClassDescLargeCars                       0.026094 *  \nCarlineClassDescMidsizeCars                     0.002770 ** \nCarlineClassDescMinicompactCars                 0.001761 ** \nCarlineClassDescSmallPickupTrucks2WD            0.126696    \nCarlineClassDescSmallPickupTrucks4WD            0.433337    \nCarlineClassDescSmallStationWagons              0.058280 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.104587    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.152446    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.565022    \nCarlineClassDescStandardPickupTrucks2WD         0.175017    \nCarlineClassDescStandardPickupTrucks4WD         0.125076    \nCarlineClassDescSubcompactCars                  0.002435 ** \nCarlineClassDescVansCargoTypes                  0.009490 ** \nCarlineClassDescVansPassengerType               0.036625 *  \nVarValveLift1                                   0.009338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.153 on 1055 degrees of freedom\nMultiple R-squared:  0.8313,    Adjusted R-squared:  0.8232 \nF-statistic:   102 on 51 and 1055 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm4)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              663.5    1 66.7421 8.786e-16 ***\nNumCyl               1037.6    6 17.3966 &lt; 2.2e-16 ***\nTransmission          638.4   12  5.3520 7.426e-09 ***\nAirAspirationMethod   155.0    2  7.7971  0.000435 ***\nNumGears              101.0    2  5.0780  0.006385 ** \nTransCreeperGear       71.0    1  7.1376  0.007665 ** \nDriveDesc            1586.1    4 39.8882 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   492.7    1 49.5606 3.460e-12 ***\nCarlineClassDesc     3548.8   16 22.3123 &lt; 2.2e-16 ***\nVarValveLift           67.4    1  6.7819  0.009338 ** \nResiduals           10487.5 1055                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-VarValveLift)\n\ncars_lm5 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm5)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7951  -1.7259   0.0248   1.6395  22.0010 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      35.6506     3.3670  10.588\nEngDispl                                         -2.0915     0.2536  -8.247\nNumCyl3                                          20.1462     3.9256   5.132\nNumCyl4                                          12.9901     2.2977   5.653\nNumCyl5                                           9.9691     2.3716   4.204\nNumCyl6                                           9.2880     2.3213   4.001\nNumCyl8                                           7.1931     2.4355   2.953\nNumCyl10                                          4.2902     2.6392   1.626\nNumCyl12                                          2.9960     2.6229   1.142\nNumCyl16                                          4.2509     4.3950   0.967\nTransmissionA4                                   -7.9263     2.2748  -3.484\nTransmissionA5                                   -7.4632     2.2831  -3.269\nTransmissionA6                                   -5.9012     2.2815  -2.587\nTransmissionA7                                    4.9316     2.4383   2.023\nTransmissionAM6                                 -10.0571     2.4744  -4.064\nTransmissionAM7                                   0.4535     2.6700   0.170\nTransmissionAV                                   -4.8272     2.2967  -2.102\nTransmissionAVS6                                 -7.2674     2.4264  -2.995\nTransmissionM5                                   -7.0899     2.2822  -3.107\nTransmissionM6                                   -7.0372     2.2764  -3.091\nTransmissionS4                                  -11.2013     2.4291  -4.611\nTransmissionS5                                   -8.0730     2.3091  -3.496\nTransmissionS6                                   -5.7943     2.2722  -2.550\nTransmissionS7                                    3.4280     2.5065   1.368\nTransmissionS8                                   -5.5848     4.0379  -1.383\nAirAspirationMethodSupercharged                  -1.9893     0.7821  -2.544\nAirAspirationMethodTurbocharged                  -1.2457     0.3184  -3.912\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.6500     3.2928  -3.234\nNumGears8                                         2.0751     3.2121   0.646\nTransCreeperGear1                                -1.3661     0.4789  -2.852\nDriveDescFourWheelDrive                          -0.6841     0.4308  -1.588\nDriveDescParttimeFourWheelDrive                  -0.7039     1.0622  -0.663\nDriveDescTwoWheelDriveFront                       4.1905     0.3747  11.185\nDriveDescTwoWheelDriveRear                        0.9965     0.3717   2.681\nExhaustValvesPerCyl1                              2.7810     0.3980   6.986\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          3.0982     1.1801   2.625\nCarlineClassDescCompactCars                       3.8878     1.1062   3.515\nCarlineClassDescLargeCars                         2.6874     1.1366   2.364\nCarlineClassDescMidsizeCars                       3.4001     1.1033   3.082\nCarlineClassDescMinicompactCars                   4.3247     1.1802   3.664\nCarlineClassDescSmallPickupTrucks2WD             -1.8684     1.2587  -1.484\nCarlineClassDescSmallPickupTrucks4WD             -0.9067     1.3575  -0.668\nCarlineClassDescSmallStationWagons                2.2224     1.1434   1.944\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.1858     1.3662  -1.600\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5070     1.1148  -1.352\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.4822     1.1274  -0.428\nCarlineClassDescStandardPickupTrucks2WD          -1.6614     1.2778  -1.300\nCarlineClassDescStandardPickupTrucks4WD          -1.8270     1.3081  -1.397\nCarlineClassDescSubcompactCars                    3.6375     1.1160   3.259\nCarlineClassDescVansCargoTypes                   -3.8763     1.5246  -2.542\nCarlineClassDescVansPassengerType                -4.0117     1.9595  -2.047\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                        4.79e-16 ***\nNumCyl3                                         3.41e-07 ***\nNumCyl4                                         2.02e-08 ***\nNumCyl5                                         2.85e-05 ***\nNumCyl6                                         6.74e-05 ***\nNumCyl8                                         0.003212 ** \nNumCyl10                                        0.104342    \nNumCyl12                                        0.253614    \nNumCyl16                                        0.333656    \nTransmissionA4                                  0.000513 ***\nTransmissionA5                                  0.001115 ** \nTransmissionA6                                  0.009827 ** \nTransmissionA7                                  0.043374 *  \nTransmissionAM6                                 5.17e-05 ***\nTransmissionAM7                                 0.865169    \nTransmissionAV                                  0.035807 *  \nTransmissionAVS6                                0.002807 ** \nTransmissionM5                                  0.001944 ** \nTransmissionM6                                  0.002044 ** \nTransmissionS4                                  4.49e-06 ***\nTransmissionS5                                  0.000492 ***\nTransmissionS6                                  0.010908 *  \nTransmissionS7                                  0.171715    \nTransmissionS8                                  0.166927    \nAirAspirationMethodSupercharged                 0.011113 *  \nAirAspirationMethodTurbocharged                 9.74e-05 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001257 ** \nNumGears8                                       0.518403    \nTransCreeperGear1                               0.004423 ** \nDriveDescFourWheelDrive                         0.112532    \nDriveDescParttimeFourWheelDrive                 0.507654    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.007448 ** \nExhaustValvesPerCyl1                            4.98e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.008782 ** \nCarlineClassDescCompactCars                     0.000459 ***\nCarlineClassDescLargeCars                       0.018235 *  \nCarlineClassDescMidsizeCars                     0.002111 ** \nCarlineClassDescMinicompactCars                 0.000260 ***\nCarlineClassDescSmallPickupTrucks2WD            0.137996    \nCarlineClassDescSmallPickupTrucks4WD            0.504354    \nCarlineClassDescSmallStationWagons              0.052206 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.109914    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.176714    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.668970    \nCarlineClassDescStandardPickupTrucks2WD         0.193828    \nCarlineClassDescStandardPickupTrucks4WD         0.162813    \nCarlineClassDescSubcompactCars                  0.001152 ** \nCarlineClassDescVansCargoTypes                  0.011149 *  \nCarlineClassDescVansPassengerType               0.040873 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.162 on 1056 degrees of freedom\nMultiple R-squared:  0.8303,    Adjusted R-squared:  0.8222 \nF-statistic: 103.3 on 50 and 1056 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm5)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              679.8    1 68.0122 4.791e-16 ***\nNumCyl               1043.7    6 17.4026 &lt; 2.2e-16 ***\nTransmission          613.6   12  5.1154 2.307e-08 ***\nAirAspirationMethod   202.9    2 10.1494 4.305e-05 ***\nNumGears              108.9    2  5.4475  0.004429 ** \nTransCreeperGear       81.3    1  8.1365  0.004423 ** \nDriveDesc            1537.5    4 38.4558 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   487.9    1 48.8110 4.982e-12 ***\nCarlineClassDesc     3715.0   16 23.2300 &lt; 2.2e-16 ***\nResiduals           10554.9 1056                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nBoth \\(R^2\\) and \\(R_a^2\\) decreased\n9 variables significant at the 0.008 level"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/lab_6.html#a",
    "href": "notes/analytics/linear-regression/MLR/lab_6.html#a",
    "title": "Lab 6",
    "section": "",
    "text": "Code\ncars2010 &lt;- cars2010 %&gt;%\n    mutate(across(!c(EngDispl, FE), as.factor))\ncars_lm &lt;- lm(FE ~ ., data = cars2010)\nsummary(cars_lm)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars2010)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6399  -1.6583   0.0582   1.5708  21.6002 \n\nCoefficients: (5 not defined because of singularities)\n                                                 Estimate Std. Error t value\n(Intercept)                                      35.95655    3.34991  10.734\nEngDispl                                         -2.24571    0.26269  -8.549\nNumCyl3                                          15.88136    5.11001   3.108\nNumCyl4                                           7.76711    3.94430   1.969\nNumCyl5                                           4.89858    3.97620   1.232\nNumCyl6                                           4.19528    3.94208   1.064\nNumCyl8                                           2.51528    3.98438   0.631\nNumCyl10                                         -0.01541    4.13445  -0.004\nNumCyl12                                         -1.02329    4.11855  -0.248\nNumCyl16                                         -0.31250    5.42018  -0.058\nTransmissionA4                                   -6.93754    2.27354  -3.051\nTransmissionA5                                   -6.53146    2.27928  -2.866\nTransmissionA6                                   -4.88712    2.27829  -2.145\nTransmissionA7                                    5.70476    2.44239   2.336\nTransmissionAM6                                  -9.48575    2.46325  -3.851\nTransmissionAM7                                   0.59731    2.65233   0.225\nTransmissionAV                                   -4.40251    2.28451  -1.927\nTransmissionAVS6                                 -6.72835    2.41754  -2.783\nTransmissionM5                                   -7.00105    2.27746  -3.074\nTransmissionM6                                   -7.03693    2.26627  -3.105\nTransmissionS4                                  -10.42310    2.42863  -4.292\nTransmissionS5                                   -7.17879    2.30519  -3.114\nTransmissionS6                                   -5.09671    2.26315  -2.252\nTransmissionS7                                    4.08689    2.51141   1.627\nTransmissionS8                                   -4.61764    4.02547  -1.147\nAirAspirationMethodSupercharged                  -1.66003    0.78945  -2.103\nAirAspirationMethodTurbocharged                  -1.12911    0.32214  -3.505\nNumGears4                                              NA         NA      NA\nNumGears5                                              NA         NA      NA\nNumGears6                                              NA         NA      NA\nNumGears7                                       -10.74200    3.27897  -3.276\nNumGears8                                         1.78308    3.19710   0.558\nTransLockup1                                     -0.89442    0.35715  -2.504\nTransCreeperGear1                                -1.04006    0.49553  -2.099\nDriveDescFourWheelDrive                          -0.45145    0.43461  -1.039\nDriveDescParttimeFourWheelDrive                  -0.29399    1.06503  -0.276\nDriveDescTwoWheelDriveFront                       4.31845    0.37701  11.455\nDriveDescTwoWheelDriveRear                        1.19634    0.37255   3.211\nIntakeValvePerCyl1                                6.33644    3.32150   1.908\nIntakeValvePerCyl2                                4.88952    3.21060   1.523\nIntakeValvePerCyl3                                     NA         NA      NA\nExhaustValvesPerCyl1                              1.54229    0.75433   2.045\nExhaustValvesPerCyl2                                   NA         NA      NA\nCarlineClassDesc2Seaters                          2.85693    1.17833   2.425\nCarlineClassDescCompactCars                       3.78908    1.09963   3.446\nCarlineClassDescLargeCars                         2.56219    1.13079   2.266\nCarlineClassDescMidsizeCars                       3.39390    1.09686   3.094\nCarlineClassDescMinicompactCars                   3.63416    1.19375   3.044\nCarlineClassDescSmallPickupTrucks2WD             -1.85140    1.25181  -1.479\nCarlineClassDescSmallPickupTrucks4WD             -0.95072    1.35268  -0.703\nCarlineClassDescSmallStationWagons                2.20724    1.13608   1.943\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.07995    1.36307  -1.526\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.51997    1.10807  -1.372\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.56991    1.12243  -0.508\nCarlineClassDescStandardPickupTrucks2WD          -1.74467    1.27006  -1.374\nCarlineClassDescStandardPickupTrucks4WD          -1.94205    1.30286  -1.491\nCarlineClassDescSubcompactCars                    3.43057    1.11242   3.084\nCarlineClassDescVansCargoTypes                   -4.07446    1.51702  -2.686\nCarlineClassDescVansPassengerType                -4.27396    1.95092  -2.191\nVarValveTiming1                                   0.15943    0.29071   0.548\nVarValveLift1                                     0.82579    0.30704   2.690\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         0.001935 ** \nNumCyl4                                         0.049193 *  \nNumCyl5                                         0.218234    \nNumCyl6                                         0.287469    \nNumCyl8                                         0.527992    \nNumCyl10                                        0.997028    \nNumCyl12                                        0.803827    \nNumCyl16                                        0.954034    \nTransmissionA4                                  0.002335 ** \nTransmissionA5                                  0.004245 ** \nTransmissionA6                                  0.032175 *  \nTransmissionA7                                  0.019693 *  \nTransmissionAM6                                 0.000125 ***\nTransmissionAM7                                 0.821867    \nTransmissionAV                                  0.054234 .  \nTransmissionAVS6                                0.005480 ** \nTransmissionM5                                  0.002166 ** \nTransmissionM6                                  0.001953 ** \nTransmissionS4                                  1.94e-05 ***\nTransmissionS5                                  0.001894 ** \nTransmissionS6                                  0.024526 *  \nTransmissionS7                                  0.103967    \nTransmissionS8                                  0.251599    \nAirAspirationMethodSupercharged                 0.035723 *  \nAirAspirationMethodTurbocharged                 0.000476 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001087 ** \nNumGears8                                       0.577155    \nTransLockup1                                    0.012420 *  \nTransCreeperGear1                               0.036067 *  \nDriveDescFourWheelDrive                         0.299167    \nDriveDescParttimeFourWheelDrive                 0.782571    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.001362 ** \nIntakeValvePerCyl1                              0.056702 .  \nIntakeValvePerCyl2                              0.128077    \nIntakeValvePerCyl3                                    NA    \nExhaustValvesPerCyl1                            0.041146 *  \nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015495 *  \nCarlineClassDescCompactCars                     0.000592 ***\nCarlineClassDescLargeCars                       0.023664 *  \nCarlineClassDescMidsizeCars                     0.002026 ** \nCarlineClassDescMinicompactCars                 0.002390 ** \nCarlineClassDescSmallPickupTrucks2WD            0.139447    \nCarlineClassDescSmallPickupTrucks4WD            0.482307    \nCarlineClassDescSmallStationWagons              0.052300 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.127330    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.170443    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.611738    \nCarlineClassDescStandardPickupTrucks2WD         0.169828    \nCarlineClassDescStandardPickupTrucks4WD         0.136365    \nCarlineClassDescSubcompactCars                  0.002097 ** \nCarlineClassDescVansCargoTypes                  0.007349 ** \nCarlineClassDescVansPassengerType               0.028690 *  \nVarValveTiming1                                 0.583517    \nVarValveLift1                                   0.007269 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.14 on 1051 degrees of freedom\nMultiple R-squared:  0.8333,    Adjusted R-squared:  0.8246 \nF-statistic: 95.55 on 55 and 1051 DF,  p-value: &lt; 2.2e-16\n\n\n\nThe F p-value is significant meaning that our overall model is significant in predicting FE\nThe 13 variables explain 83.33 percent of variation in fuel economy"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/lab_6.html#b",
    "href": "notes/analytics/linear-regression/MLR/lab_6.html#b",
    "title": "Lab 6",
    "section": "",
    "text": "Code\ncar::Anova(cars_lm)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              720.6    1 73.0842 &lt; 2.2e-16 ***\nNumCyl                889.6    6 15.0374 &lt; 2.2e-16 ***\nTransmission          707.7   12  5.9813 3.553e-10 ***\nAirAspirationMethod   151.2    2  7.6686 0.0004939 ***\nNumGears              109.2    2  5.5361 0.0040576 ** \nTransLockup            61.8    1  6.2715 0.0124202 *  \nTransCreeperGear       43.4    1  4.4052 0.0360667 *  \nDriveDesc            1535.0    4 38.9205 &lt; 2.2e-16 ***\nIntakeValvePerCyl      56.6    2  2.8720 0.0570315 .  \nExhaustValvesPerCyl    41.2    1  4.1803 0.0411460 *  \nCarlineClassDesc     3495.4   16 22.1561 &lt; 2.2e-16 ***\nVarValveTiming          3.0    1  0.3008 0.5835171    \nVarValveLift           71.3    1  7.2336 0.0072685 ** \nResiduals           10363.0 1051                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nHighest p-value is VarValveTiming with 0.584"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/lab_6.html#c",
    "href": "notes/analytics/linear-regression/MLR/lab_6.html#c",
    "title": "Lab 6",
    "section": "",
    "text": "Code\ncars_2010_sub &lt;- cars2010 %&gt;%\n    select(-VarValveTiming)\n\ncars_lm2 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm2)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6242  -1.6504   0.0541   1.5540  21.5852 \n\nCoefficients: (5 not defined because of singularities)\n                                                 Estimate Std. Error t value\n(Intercept)                                      35.99395    3.34810  10.751\nEngDispl                                         -2.26003    0.26130  -8.649\nNumCyl3                                          16.00965    5.10296   3.137\nNumCyl4                                           7.91363    3.93394   2.012\nNumCyl5                                           5.03429    3.96717   1.269\nNumCyl6                                           4.35537    3.92996   1.108\nNumCyl8                                           2.71420    3.96652   0.684\nNumCyl10                                          0.18914    4.11622   0.046\nNumCyl12                                         -0.81976    4.10043  -0.200\nNumCyl16                                         -0.08503    5.40249  -0.016\nTransmissionA4                                   -6.98768    2.27095  -3.077\nTransmissionA5                                   -6.53850    2.27848  -2.870\nTransmissionA6                                   -4.90059    2.27740  -2.152\nTransmissionA7                                    5.71566    2.44150   2.341\nTransmissionAM6                                  -9.49295    2.46240  -3.855\nTransmissionAM7                                   0.59242    2.65143   0.223\nTransmissionAV                                   -4.40233    2.28375  -1.928\nTransmissionAVS6                                 -6.72853    2.41673  -2.784\nTransmissionM5                                   -7.01977    2.27644  -3.084\nTransmissionM6                                   -7.04911    2.26540  -3.112\nTransmissionS4                                  -10.51052    2.42259  -4.339\nTransmissionS5                                   -7.20354    2.30398  -3.127\nTransmissionS6                                   -5.10605    2.26234  -2.257\nTransmissionS7                                    4.09144    2.51056   1.630\nTransmissionS8                                   -4.62893    4.02408  -1.150\nAirAspirationMethodSupercharged                  -1.67713    0.78857  -2.127\nAirAspirationMethodTurbocharged                  -1.13613    0.32177  -3.531\nNumGears4                                              NA         NA      NA\nNumGears5                                              NA         NA      NA\nNumGears6                                              NA         NA      NA\nNumGears7                                       -10.74376    3.27788  -3.278\nNumGears8                                         1.79152    3.19600   0.561\nTransLockup1                                     -0.89373    0.35703  -2.503\nTransCreeperGear1                                -1.09666    0.48450  -2.263\nDriveDescFourWheelDrive                          -0.44026    0.43399  -1.014\nDriveDescParttimeFourWheelDrive                  -0.25475    1.06227  -0.240\nDriveDescTwoWheelDriveFront                       4.32752    0.37652  11.494\nDriveDescTwoWheelDriveRear                        1.19041    0.37227   3.198\nIntakeValvePerCyl1                                6.36976    3.31984   1.919\nIntakeValvePerCyl2                                4.90088    3.20946   1.527\nIntakeValvePerCyl3                                     NA         NA      NA\nExhaustValvesPerCyl1                              1.49669    0.74949   1.997\nExhaustValvesPerCyl2                                   NA         NA      NA\nCarlineClassDesc2Seaters                          2.85888    1.17793   2.427\nCarlineClassDescCompactCars                       3.77775    1.09907   3.437\nCarlineClassDescLargeCars                         2.55092    1.13023   2.257\nCarlineClassDescMidsizeCars                       3.38958    1.09647   3.091\nCarlineClassDescMinicompactCars                   3.62150    1.19313   3.035\nCarlineClassDescSmallPickupTrucks2WD             -1.88501    1.24990  -1.508\nCarlineClassDescSmallPickupTrucks4WD             -0.99946    1.34930  -0.741\nCarlineClassDescSmallStationWagons                2.19864    1.13559   1.936\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.14681    1.35716  -1.582\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.52376    1.10769  -1.376\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.58904    1.12151  -0.525\nCarlineClassDescStandardPickupTrucks2WD          -1.76375    1.26916  -1.390\nCarlineClassDescStandardPickupTrucks4WD          -1.97791    1.30079  -1.521\nCarlineClassDescSubcompactCars                    3.42813    1.11204   3.083\nCarlineClassDescVansCargoTypes                   -4.03720    1.51499  -2.665\nCarlineClassDescVansPassengerType                -4.21030    1.94682  -2.163\nVarValveLift1                                     0.82139    0.30683   2.677\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         0.001752 ** \nNumCyl4                                         0.044513 *  \nNumCyl5                                         0.204726    \nNumCyl6                                         0.268007    \nNumCyl8                                         0.493951    \nNumCyl10                                        0.963360    \nNumCyl12                                        0.841581    \nNumCyl16                                        0.987446    \nTransmissionA4                                  0.002145 ** \nTransmissionA5                                  0.004191 ** \nTransmissionA6                                  0.031638 *  \nTransmissionA7                                  0.019416 *  \nTransmissionAM6                                 0.000123 ***\nTransmissionAM7                                 0.823240    \nTransmissionAV                                  0.054164 .  \nTransmissionAVS6                                0.005463 ** \nTransmissionM5                                  0.002098 ** \nTransmissionM6                                  0.001911 ** \nTransmissionS4                                  1.57e-05 ***\nTransmissionS5                                  0.001817 ** \nTransmissionS6                                  0.024214 *  \nTransmissionS7                                  0.103466    \nTransmissionS8                                  0.250279    \nAirAspirationMethodSupercharged                 0.033668 *  \nAirAspirationMethodTurbocharged                 0.000432 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001081 ** \nNumGears8                                       0.575223    \nTransLockup1                                    0.012458 *  \nTransCreeperGear1                               0.023808 *  \nDriveDescFourWheelDrive                         0.310604    \nDriveDescParttimeFourWheelDrive                 0.810521    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.001427 ** \nIntakeValvePerCyl1                              0.055293 .  \nIntakeValvePerCyl2                              0.127060    \nIntakeValvePerCyl3                                    NA    \nExhaustValvesPerCyl1                            0.046088 *  \nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015390 *  \nCarlineClassDescCompactCars                     0.000611 ***\nCarlineClassDescLargeCars                       0.024213 *  \nCarlineClassDescMidsizeCars                     0.002045 ** \nCarlineClassDescMinicompactCars                 0.002462 ** \nCarlineClassDescSmallPickupTrucks2WD            0.131822    \nCarlineClassDescSmallPickupTrucks4WD            0.459026    \nCarlineClassDescSmallStationWagons              0.053120 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.113986    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.169229    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.599543    \nCarlineClassDescStandardPickupTrucks2WD         0.164913    \nCarlineClassDescStandardPickupTrucks4WD         0.128675    \nCarlineClassDescSubcompactCars                  0.002105 ** \nCarlineClassDescVansCargoTypes                  0.007821 ** \nCarlineClassDescVansPassengerType               0.030792 *  \nVarValveLift1                                   0.007544 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.139 on 1052 degrees of freedom\nMultiple R-squared:  0.8333,    Adjusted R-squared:  0.8247 \nF-statistic: 97.38 on 54 and 1052 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm2)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              737.1    1 74.8066 &lt; 2.2e-16 ***\nNumCyl                887.9    6 15.0181 &lt; 2.2e-16 ***\nTransmission          712.6   12  6.0266 2.849e-10 ***\nAirAspirationMethod   153.8    2  7.8053 0.0004316 ***\nNumGears              109.2    2  5.5431 0.0040294 ** \nTransLockup            61.7    1  6.2661 0.0124578 *  \nTransCreeperGear       50.5    1  5.1234 0.0238082 *  \nDriveDesc            1545.6    4 39.2146 &lt; 2.2e-16 ***\nIntakeValvePerCyl      57.9    2  2.9386 0.0533736 .  \nExhaustValvesPerCyl    39.3    1  3.9878 0.0460877 *  \nCarlineClassDesc     3504.2   16 22.2267 &lt; 2.2e-16 ***\nVarValveLift           70.6    1  7.1663 0.0075441 ** \nResiduals           10366.0 1052                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe p-value for the model did not change significantly\n\\(R^2\\) did not change significantly and \\(R_a^2\\) increased a little"
  },
  {
    "objectID": "notes/analytics/linear-regression/MLR/lab_6.html#d",
    "href": "notes/analytics/linear-regression/MLR/lab_6.html#d",
    "title": "Lab 6",
    "section": "",
    "text": "Dropping IntakeValvePerCyl:\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-IntakeValvePerCyl)\n\ncars_lm3 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm3)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.5964  -1.7032   0.0413   1.5730  21.6235 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      36.0050     3.3542  10.734\nEngDispl                                         -2.1120     0.2530  -8.346\nNumCyl3                                          20.9056     3.9117   5.344\nNumCyl4                                          12.6843     2.2897   5.540\nNumCyl5                                           9.6779     2.3617   4.098\nNumCyl6                                           9.0036     2.3148   3.889\nNumCyl8                                           7.0608     2.4285   2.908\nNumCyl10                                          4.4870     2.6277   1.708\nNumCyl12                                          3.0682     2.6138   1.174\nNumCyl16                                          3.8516     4.3735   0.881\nTransmissionA4                                   -7.1007     2.2745  -3.122\nTransmissionA5                                   -6.7679     2.2799  -2.968\nTransmissionA6                                   -5.2017     2.2777  -2.284\nTransmissionA7                                    5.3619     2.4409   2.197\nTransmissionAM6                                  -9.5275     2.4663  -3.863\nTransmissionAM7                                   0.5339     2.6561   0.201\nTransmissionAV                                   -4.5677     2.2867  -1.998\nTransmissionAVS6                                 -6.9582     2.4186  -2.877\nTransmissionM5                                   -7.2063     2.2788  -3.162\nTransmissionM6                                   -7.1857     2.2683  -3.168\nTransmissionS4                                  -10.7361     2.4247  -4.428\nTransmissionS5                                   -7.3945     2.3063  -3.206\nTransmissionS6                                   -5.2347     2.2655  -2.311\nTransmissionS7                                    3.8448     2.5130   1.530\nTransmissionS8                                   -4.6021     4.0315  -1.142\nAirAspirationMethodSupercharged                  -1.8711     0.7825  -2.391\nAirAspirationMethodTurbocharged                  -1.1602     0.3221  -3.602\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.6135     3.2834  -3.233\nNumGears8                                         1.7231     3.2016   0.538\nTransLockup1                                     -0.9069     0.3576  -2.536\nTransCreeperGear1                                -1.2475     0.4777  -2.612\nDriveDescFourWheelDrive                          -0.4799     0.4345  -1.105\nDriveDescParttimeFourWheelDrive                  -0.3666     1.0628  -0.345\nDriveDescTwoWheelDriveFront                       4.3236     0.3772  11.462\nDriveDescTwoWheelDriveRear                        1.1403     0.3722   3.063\nExhaustValvesPerCyl1                              2.7454     0.3965   6.925\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          2.7680     1.1781   2.350\nCarlineClassDescCompactCars                       3.7770     1.1011   3.430\nCarlineClassDescLargeCars                         2.5646     1.1323   2.265\nCarlineClassDescMidsizeCars                       3.3732     1.0985   3.071\nCarlineClassDescMinicompactCars                   3.6283     1.1952   3.036\nCarlineClassDescSmallPickupTrucks2WD             -1.9032     1.2522  -1.520\nCarlineClassDescSmallPickupTrucks4WD             -1.0239     1.3517  -0.757\nCarlineClassDescSmallStationWagons                2.1934     1.1377   1.928\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.1150     1.3596  -1.556\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5296     1.1097  -1.378\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.5922     1.1235  -0.527\nCarlineClassDescStandardPickupTrucks2WD          -1.7232     1.2714  -1.355\nCarlineClassDescStandardPickupTrucks4WD          -1.9711     1.3031  -1.513\nCarlineClassDescSubcompactCars                    3.4139     1.1141   3.064\nCarlineClassDescVansCargoTypes                   -3.9331     1.5169  -2.593\nCarlineClassDescVansPassengerType                -4.0805     1.9494  -2.093\nVarValveLift1                                     0.8053     0.3070   2.623\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                         &lt; 2e-16 ***\nNumCyl3                                         1.11e-07 ***\nNumCyl4                                         3.83e-08 ***\nNumCyl5                                         4.49e-05 ***\nNumCyl6                                         0.000107 ***\nNumCyl8                                         0.003719 ** \nNumCyl10                                        0.088005 .  \nNumCyl12                                        0.240713    \nNumCyl16                                        0.378695    \nTransmissionA4                                  0.001846 ** \nTransmissionA5                                  0.003061 ** \nTransmissionA6                                  0.022584 *  \nTransmissionA7                                  0.028260 *  \nTransmissionAM6                                 0.000119 ***\nTransmissionAM7                                 0.840740    \nTransmissionAV                                  0.046025 *  \nTransmissionAVS6                                0.004096 ** \nTransmissionM5                                  0.001610 ** \nTransmissionM6                                  0.001580 ** \nTransmissionS4                                  1.05e-05 ***\nTransmissionS5                                  0.001385 ** \nTransmissionS6                                  0.021044 *  \nTransmissionS7                                  0.126328    \nTransmissionS8                                  0.253901    \nAirAspirationMethodSupercharged                 0.016962 *  \nAirAspirationMethodTurbocharged                 0.000331 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001265 ** \nNumGears8                                       0.590565    \nTransLockup1                                    0.011365 *  \nTransCreeperGear1                               0.009142 ** \nDriveDescFourWheelDrive                         0.269550    \nDriveDescParttimeFourWheelDrive                 0.730220    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.002245 ** \nExhaustValvesPerCyl1                            7.58e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.018981 *  \nCarlineClassDescCompactCars                     0.000626 ***\nCarlineClassDescLargeCars                       0.023715 *  \nCarlineClassDescMidsizeCars                     0.002189 ** \nCarlineClassDescMinicompactCars                 0.002458 ** \nCarlineClassDescSmallPickupTrucks2WD            0.128836    \nCarlineClassDescSmallPickupTrucks4WD            0.448949    \nCarlineClassDescSmallStationWagons              0.054126 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.120096    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.168369    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.598240    \nCarlineClassDescStandardPickupTrucks2WD         0.175593    \nCarlineClassDescStandardPickupTrucks4WD         0.130664    \nCarlineClassDescSubcompactCars                  0.002237 ** \nCarlineClassDescVansCargoTypes                  0.009648 ** \nCarlineClassDescVansPassengerType               0.036568 *  \nVarValveLift1                                   0.008839 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.145 on 1054 degrees of freedom\nMultiple R-squared:  0.8324,    Adjusted R-squared:  0.8241 \nF-statistic: 100.6 on 52 and 1054 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm3)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              688.9    1 69.6604 &lt; 2.2e-16 ***\nNumCyl               1000.0    6 16.8523 &lt; 2.2e-16 ***\nTransmission          699.4   12  5.8935 5.430e-10 ***\nAirAspirationMethod   169.7    2  8.5817 0.0002009 ***\nNumGears              106.5    2  5.3833 0.0047196 ** \nTransLockup            63.6    1  6.4299 0.0113653 *  \nTransCreeperGear       67.4    1  6.8201 0.0091419 ** \nDriveDesc            1559.4    4 39.4183 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   474.2    1 47.9516 7.583e-12 ***\nCarlineClassDesc     3489.4   16 22.0517 &lt; 2.2e-16 ***\nVarValveLift           68.0    1  6.8807 0.0088387 ** \nResiduals           10423.9 1054                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-TransLockup)\n\ncars_lm4 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm4)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.6831  -1.7755   0.0193   1.5767  22.1002 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      35.5540     3.3581  10.588\nEngDispl                                         -2.0676     0.2531  -8.170\nNumCyl3                                          20.3570     3.9157   5.199\nNumCyl4                                          12.6350     2.2955   5.504\nNumCyl5                                           9.6799     2.3677   4.088\nNumCyl6                                           8.8784     2.3203   3.826\nNumCyl8                                           6.8252     2.4329   2.805\nNumCyl10                                          4.2129     2.6322   1.601\nNumCyl12                                          2.7488     2.6175   1.050\nNumCyl16                                          4.1444     4.3832   0.946\nTransmissionA4                                   -7.5779     2.2725  -3.335\nTransmissionA5                                   -7.2188     2.2788  -3.168\nTransmissionA6                                   -5.5830     2.2785  -2.450\nTransmissionA7                                    4.7030     2.4333   1.933\nTransmissionAM6                                  -9.6845     2.4718  -3.918\nTransmissionAM7                                   0.5441     2.6629   0.204\nTransmissionAV                                   -4.5705     2.2925  -1.994\nTransmissionAVS6                                 -6.8711     2.4245  -2.834\nTransmissionM5                                   -6.7947     2.2788  -2.982\nTransmissionM6                                   -6.8804     2.2710  -3.030\nTransmissionS4                                  -11.2457     2.4226  -4.642\nTransmissionS5                                   -7.8915     2.3038  -3.425\nTransmissionS6                                   -5.5467     2.2680  -2.446\nTransmissionS7                                    3.1101     2.5027   1.243\nTransmissionS8                                   -5.4739     4.0271  -1.359\nAirAspirationMethodSupercharged                  -1.7876     0.7838  -2.281\nAirAspirationMethodTurbocharged                  -1.1043     0.3222  -3.427\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.2165     3.2880  -3.107\nNumGears8                                         2.2205     3.2038   0.693\nTransCreeperGear1                                -1.2791     0.4788  -2.672\nDriveDescFourWheelDrive                          -0.4976     0.4355  -1.143\nDriveDescParttimeFourWheelDrive                  -0.4061     1.0655  -0.381\nDriveDescTwoWheelDriveFront                       4.3413     0.3781  11.482\nDriveDescTwoWheelDriveRear                        1.1035     0.3729   2.959\nExhaustValvesPerCyl1                              2.7948     0.3970   7.040\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          2.8530     1.1807   2.416\nCarlineClassDescCompactCars                       3.7853     1.1039   3.429\nCarlineClassDescLargeCars                         2.5289     1.1351   2.228\nCarlineClassDescMidsizeCars                       3.3020     1.1009   2.999\nCarlineClassDescMinicompactCars                   3.7542     1.1972   3.136\nCarlineClassDescSmallPickupTrucks2WD             -1.9188     1.2554  -1.528\nCarlineClassDescSmallPickupTrucks4WD             -1.0621     1.3551  -0.784\nCarlineClassDescSmallStationWagons                2.1620     1.1405   1.896\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.2133     1.3625  -1.624\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5927     1.1122  -1.432\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.6482     1.1262  -0.576\nCarlineClassDescStandardPickupTrucks2WD          -1.7299     1.2746  -1.357\nCarlineClassDescStandardPickupTrucks4WD          -2.0053     1.3064  -1.535\nCarlineClassDescSubcompactCars                    3.3939     1.1169   3.039\nCarlineClassDescVansCargoTypes                   -3.9518     1.5207  -2.599\nCarlineClassDescVansPassengerType                -4.0897     1.9544  -2.093\nVarValveLift1                                     0.8016     0.3078   2.604\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                        8.79e-16 ***\nNumCyl3                                         2.41e-07 ***\nNumCyl4                                         4.66e-08 ***\nNumCyl5                                         4.68e-05 ***\nNumCyl6                                         0.000138 ***\nNumCyl8                                         0.005119 ** \nNumCyl10                                        0.109781    \nNumCyl12                                        0.293871    \nNumCyl16                                        0.344606    \nTransmissionA4                                  0.000884 ***\nTransmissionA5                                  0.001580 ** \nTransmissionA6                                  0.014438 *  \nTransmissionA7                                  0.053532 .  \nTransmissionAM6                                 9.50e-05 ***\nTransmissionAM7                                 0.838126    \nTransmissionAV                                  0.046450 *  \nTransmissionAVS6                                0.004685 ** \nTransmissionM5                                  0.002933 ** \nTransmissionM6                                  0.002507 ** \nTransmissionS4                                  3.88e-06 ***\nTransmissionS5                                  0.000638 ***\nTransmissionS6                                  0.014620 *  \nTransmissionS7                                  0.214253    \nTransmissionS8                                  0.174354    \nAirAspirationMethodSupercharged                 0.022764 *  \nAirAspirationMethodTurbocharged                 0.000633 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001939 ** \nNumGears8                                       0.488417    \nTransCreeperGear1                               0.007665 ** \nDriveDescFourWheelDrive                         0.253447    \nDriveDescParttimeFourWheelDrive                 0.703142    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.003153 ** \nExhaustValvesPerCyl1                            3.46e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.015841 *  \nCarlineClassDescCompactCars                     0.000629 ***\nCarlineClassDescLargeCars                       0.026094 *  \nCarlineClassDescMidsizeCars                     0.002770 ** \nCarlineClassDescMinicompactCars                 0.001761 ** \nCarlineClassDescSmallPickupTrucks2WD            0.126696    \nCarlineClassDescSmallPickupTrucks4WD            0.433337    \nCarlineClassDescSmallStationWagons              0.058280 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.104587    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.152446    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.565022    \nCarlineClassDescStandardPickupTrucks2WD         0.175017    \nCarlineClassDescStandardPickupTrucks4WD         0.125076    \nCarlineClassDescSubcompactCars                  0.002435 ** \nCarlineClassDescVansCargoTypes                  0.009490 ** \nCarlineClassDescVansPassengerType               0.036625 *  \nVarValveLift1                                   0.009338 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.153 on 1055 degrees of freedom\nMultiple R-squared:  0.8313,    Adjusted R-squared:  0.8232 \nF-statistic:   102 on 51 and 1055 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm4)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              663.5    1 66.7421 8.786e-16 ***\nNumCyl               1037.6    6 17.3966 &lt; 2.2e-16 ***\nTransmission          638.4   12  5.3520 7.426e-09 ***\nAirAspirationMethod   155.0    2  7.7971  0.000435 ***\nNumGears              101.0    2  5.0780  0.006385 ** \nTransCreeperGear       71.0    1  7.1376  0.007665 ** \nDriveDesc            1586.1    4 39.8882 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   492.7    1 49.5606 3.460e-12 ***\nCarlineClassDesc     3548.8   16 22.3123 &lt; 2.2e-16 ***\nVarValveLift           67.4    1  6.7819  0.009338 ** \nResiduals           10487.5 1055                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nCode\ncars_2010_sub &lt;- cars_2010_sub %&gt;%\n    select(-VarValveLift)\n\ncars_lm5 &lt;- lm(FE ~ ., cars_2010_sub)\nsummary(cars_lm5)\n\n\n\nCall:\nlm(formula = FE ~ ., data = cars_2010_sub)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.7951  -1.7259   0.0248   1.6395  22.0010 \n\nCoefficients: (4 not defined because of singularities)\n                                                Estimate Std. Error t value\n(Intercept)                                      35.6506     3.3670  10.588\nEngDispl                                         -2.0915     0.2536  -8.247\nNumCyl3                                          20.1462     3.9256   5.132\nNumCyl4                                          12.9901     2.2977   5.653\nNumCyl5                                           9.9691     2.3716   4.204\nNumCyl6                                           9.2880     2.3213   4.001\nNumCyl8                                           7.1931     2.4355   2.953\nNumCyl10                                          4.2902     2.6392   1.626\nNumCyl12                                          2.9960     2.6229   1.142\nNumCyl16                                          4.2509     4.3950   0.967\nTransmissionA4                                   -7.9263     2.2748  -3.484\nTransmissionA5                                   -7.4632     2.2831  -3.269\nTransmissionA6                                   -5.9012     2.2815  -2.587\nTransmissionA7                                    4.9316     2.4383   2.023\nTransmissionAM6                                 -10.0571     2.4744  -4.064\nTransmissionAM7                                   0.4535     2.6700   0.170\nTransmissionAV                                   -4.8272     2.2967  -2.102\nTransmissionAVS6                                 -7.2674     2.4264  -2.995\nTransmissionM5                                   -7.0899     2.2822  -3.107\nTransmissionM6                                   -7.0372     2.2764  -3.091\nTransmissionS4                                  -11.2013     2.4291  -4.611\nTransmissionS5                                   -8.0730     2.3091  -3.496\nTransmissionS6                                   -5.7943     2.2722  -2.550\nTransmissionS7                                    3.4280     2.5065   1.368\nTransmissionS8                                   -5.5848     4.0379  -1.383\nAirAspirationMethodSupercharged                  -1.9893     0.7821  -2.544\nAirAspirationMethodTurbocharged                  -1.2457     0.3184  -3.912\nNumGears4                                             NA         NA      NA\nNumGears5                                             NA         NA      NA\nNumGears6                                             NA         NA      NA\nNumGears7                                       -10.6500     3.2928  -3.234\nNumGears8                                         2.0751     3.2121   0.646\nTransCreeperGear1                                -1.3661     0.4789  -2.852\nDriveDescFourWheelDrive                          -0.6841     0.4308  -1.588\nDriveDescParttimeFourWheelDrive                  -0.7039     1.0622  -0.663\nDriveDescTwoWheelDriveFront                       4.1905     0.3747  11.185\nDriveDescTwoWheelDriveRear                        0.9965     0.3717   2.681\nExhaustValvesPerCyl1                              2.7810     0.3980   6.986\nExhaustValvesPerCyl2                                  NA         NA      NA\nCarlineClassDesc2Seaters                          3.0982     1.1801   2.625\nCarlineClassDescCompactCars                       3.8878     1.1062   3.515\nCarlineClassDescLargeCars                         2.6874     1.1366   2.364\nCarlineClassDescMidsizeCars                       3.4001     1.1033   3.082\nCarlineClassDescMinicompactCars                   4.3247     1.1802   3.664\nCarlineClassDescSmallPickupTrucks2WD             -1.8684     1.2587  -1.484\nCarlineClassDescSmallPickupTrucks4WD             -0.9067     1.3575  -0.668\nCarlineClassDescSmallStationWagons                2.2224     1.1434   1.944\nCarlineClassDescSpecialPurposeVehicleminivan2WD  -2.1858     1.3662  -1.600\nCarlineClassDescSpecialPurposeVehicleSUV2WD      -1.5070     1.1148  -1.352\nCarlineClassDescSpecialPurposeVehicleSUV4WD      -0.4822     1.1274  -0.428\nCarlineClassDescStandardPickupTrucks2WD          -1.6614     1.2778  -1.300\nCarlineClassDescStandardPickupTrucks4WD          -1.8270     1.3081  -1.397\nCarlineClassDescSubcompactCars                    3.6375     1.1160   3.259\nCarlineClassDescVansCargoTypes                   -3.8763     1.5246  -2.542\nCarlineClassDescVansPassengerType                -4.0117     1.9595  -2.047\n                                                Pr(&gt;|t|)    \n(Intercept)                                      &lt; 2e-16 ***\nEngDispl                                        4.79e-16 ***\nNumCyl3                                         3.41e-07 ***\nNumCyl4                                         2.02e-08 ***\nNumCyl5                                         2.85e-05 ***\nNumCyl6                                         6.74e-05 ***\nNumCyl8                                         0.003212 ** \nNumCyl10                                        0.104342    \nNumCyl12                                        0.253614    \nNumCyl16                                        0.333656    \nTransmissionA4                                  0.000513 ***\nTransmissionA5                                  0.001115 ** \nTransmissionA6                                  0.009827 ** \nTransmissionA7                                  0.043374 *  \nTransmissionAM6                                 5.17e-05 ***\nTransmissionAM7                                 0.865169    \nTransmissionAV                                  0.035807 *  \nTransmissionAVS6                                0.002807 ** \nTransmissionM5                                  0.001944 ** \nTransmissionM6                                  0.002044 ** \nTransmissionS4                                  4.49e-06 ***\nTransmissionS5                                  0.000492 ***\nTransmissionS6                                  0.010908 *  \nTransmissionS7                                  0.171715    \nTransmissionS8                                  0.166927    \nAirAspirationMethodSupercharged                 0.011113 *  \nAirAspirationMethodTurbocharged                 9.74e-05 ***\nNumGears4                                             NA    \nNumGears5                                             NA    \nNumGears6                                             NA    \nNumGears7                                       0.001257 ** \nNumGears8                                       0.518403    \nTransCreeperGear1                               0.004423 ** \nDriveDescFourWheelDrive                         0.112532    \nDriveDescParttimeFourWheelDrive                 0.507654    \nDriveDescTwoWheelDriveFront                      &lt; 2e-16 ***\nDriveDescTwoWheelDriveRear                      0.007448 ** \nExhaustValvesPerCyl1                            4.98e-12 ***\nExhaustValvesPerCyl2                                  NA    \nCarlineClassDesc2Seaters                        0.008782 ** \nCarlineClassDescCompactCars                     0.000459 ***\nCarlineClassDescLargeCars                       0.018235 *  \nCarlineClassDescMidsizeCars                     0.002111 ** \nCarlineClassDescMinicompactCars                 0.000260 ***\nCarlineClassDescSmallPickupTrucks2WD            0.137996    \nCarlineClassDescSmallPickupTrucks4WD            0.504354    \nCarlineClassDescSmallStationWagons              0.052206 .  \nCarlineClassDescSpecialPurposeVehicleminivan2WD 0.109914    \nCarlineClassDescSpecialPurposeVehicleSUV2WD     0.176714    \nCarlineClassDescSpecialPurposeVehicleSUV4WD     0.668970    \nCarlineClassDescStandardPickupTrucks2WD         0.193828    \nCarlineClassDescStandardPickupTrucks4WD         0.162813    \nCarlineClassDescSubcompactCars                  0.001152 ** \nCarlineClassDescVansCargoTypes                  0.011149 *  \nCarlineClassDescVansPassengerType               0.040873 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.162 on 1056 degrees of freedom\nMultiple R-squared:  0.8303,    Adjusted R-squared:  0.8222 \nF-statistic: 103.3 on 50 and 1056 DF,  p-value: &lt; 2.2e-16\n\n\nCode\ncar::Anova(cars_lm5)\n\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\nAnova Table (Type II tests)\n\nResponse: FE\n                     Sum Sq   Df F value    Pr(&gt;F)    \nEngDispl              679.8    1 68.0122 4.791e-16 ***\nNumCyl               1043.7    6 17.4026 &lt; 2.2e-16 ***\nTransmission          613.6   12  5.1154 2.307e-08 ***\nAirAspirationMethod   202.9    2 10.1494 4.305e-05 ***\nNumGears              108.9    2  5.4475  0.004429 ** \nTransCreeperGear       81.3    1  8.1365  0.004423 ** \nDriveDesc            1537.5    4 38.4558 &lt; 2.2e-16 ***\nExhaustValvesPerCyl   487.9    1 48.8110 4.982e-12 ***\nCarlineClassDesc     3715.0   16 23.2300 &lt; 2.2e-16 ***\nResiduals           10554.9 1056                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nBoth \\(R^2\\) and \\(R_a^2\\) decreased\n9 variables significant at the 0.008 level"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html",
    "href": "notes/analytics/linear-regression/diagnostics/index.html",
    "title": "Diagnostics",
    "section": "",
    "text": "Recall the assumptions of linear regression:\n\nMean of the Ys is accurately modeled by linear function of the Xs\n\\(\\varepsilon\\) is assumed to be Normal with a mean of 0\n\\(\\varepsilon\\) is assumed to have constant variance \\(\\sigma^2\\)\nErrors are independent\nNo perfect collinearity\n\nWe can investigate many of our assumptions through residuals in residuals vs. fitted values plots.\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(httpgd)\nlibrary(reticulate)\n\nuse_condaenv(condaenv = \"msa\", required = TRUE)\ndata_path &lt;- \"data/Salary.csv\"\nsalary &lt;- read.csv(data_path)\npar(mfrow = c(2, 2))\nsalary_lm &lt;- lm(Salary ~ YearsExperience, data = salary)\n\nggplot(salary_lm, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\nsalary = r.salary\nsns.residplot(salary, x=\"YearsExperience\", y=\"Salary\", order=2)"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#r-code",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#r-code",
    "title": "Diagnostics",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(httpgd)\nlibrary(reticulate)\n\nuse_condaenv(condaenv = \"msa\", required = TRUE)\ndata_path &lt;- \"data/Salary.csv\"\nsalary &lt;- read.csv(data_path)\npar(mfrow = c(2, 2))\nsalary_lm &lt;- lm(Salary ~ YearsExperience, data = salary)\n\nggplot(salary_lm, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#python-code",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#python-code",
    "title": "Diagnostics",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\nsalary = r.salary\nsns.residplot(salary, x=\"YearsExperience\", y=\"Salary\", order=2)"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#r-code-1",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#r-code-1",
    "title": "Diagnostics",
    "section": "4.1 R Code",
    "text": "4.1 R Code\n\n\nCode\nsalary_quad &lt;- lm(Salary ~ YearsExperience + I(YearsExperience^2), data = salary)\nsummary(salary_quad)\n\n\n\nCall:\nlm(formula = Salary ~ YearsExperience + I(YearsExperience^2), \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9210.7 -4037.8  -467.7  3485.5 11052.0 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)          21855.58    3630.80   6.019 1.03e-06 ***\nYearsExperience      11456.37    1217.28   9.411 9.79e-11 ***\nI(YearsExperience^2)  -193.90      84.45  -2.296   0.0284 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5733 on 32 degrees of freedom\nMultiple R-squared:  0.9701,    Adjusted R-squared:  0.9682 \nF-statistic:   519 on 2 and 32 DF,  p-value: &lt; 2.2e-16\n\n\nCode\nggplot(salary_quad, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n\n\n\n\n\n\nIn R we use the I() function to create a higher order term\n\nWhen a straight line is inappropriate, we can consider:\n\nFit a polynomial/more complex regression\nTransform the dependent and/or independent variables to obtain linearity\nFit a nonlinear regression model if appropriate\nFit a nonparametric regression model (e.g. LOESS)"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#spearman-rank-correlation",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#spearman-rank-correlation",
    "title": "Diagnostics",
    "section": "5.1 Spearman Rank Correlation",
    "text": "5.1 Spearman Rank Correlation\nIf Spearman rank coefficient between the absolute value of the residuals and predicted values is:\n\nClose to zero: variance is potentially homoscedastic\nPositive: variance increases as mean increases\nNegative: variance descreases as the mean increases\n\n\n\\(H_0:\\) Variance is homoscedastic\n\\(H_a:\\) Variance is heteroscedastic\n\nIf there is a relationship between the absolute value of residuals and predicted value but it is not linear, this test will not discover it.\n\n5.1.1 R Code\n\n\nCode\nlm_var &lt;- lm(Salary ~ YearsExperience, data = salary)\ncor.test(abs(resid(lm_var)), fitted.values(lm_var), method = \"spearman\", exact = T)\n\n\nWarning in cor.test.default(abs(resid(lm_var)), fitted.values(lm_var), method =\n\"spearman\", : Cannot compute exact p-value with ties\n\n\n\n    Spearman's rank correlation rho\n\ndata:  abs(resid(lm_var)) and fitted.values(lm_var)\nS = 7493, p-value = 0.7779\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.0494467"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#formal-tests",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#formal-tests",
    "title": "Diagnostics",
    "section": "6.1 Formal Tests",
    "text": "6.1 Formal Tests\n\n\\(H_0:\\) Normality\n\\(H_a:\\) Not Normal\n\nAnderson-Darling is based on empirical cumulative distribution function of data and gives more weight to the tails.\nShapiro-Wilk uses correlation between sample data and normal scores. The Shapiro-Wilk is better for smaller data sets.\n\n6.1.1 R Code\n\n\nCode\nlibrary(nortest)\nad.test(resid(lm_var))\n\n\n\n    Anderson-Darling normality test\n\ndata:  resid(lm_var)\nA = 0.4791, p-value = 0.2205\n\n\nCode\nshapiro.test(resid(lm_var))\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(lm_var)\nW = 0.94679, p-value = 0.09028"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#accounting-for-lack-of-normality",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#accounting-for-lack-of-normality",
    "title": "Diagnostics",
    "section": "6.2 Accounting for Lack of Normality",
    "text": "6.2 Accounting for Lack of Normality\nDepends on why the assumption was broken:\n\nOutliers \\(\\rightarrow\\) Robust Regression\nNonnormal \\(\\rightarrow\\) Transformation Needed\n\nCan try Box-Cox transformation"
  },
  {
    "objectID": "notes/analytics/linear-regression/diagnostics/index.html#box-cox-transformation",
    "href": "notes/analytics/linear-regression/diagnostics/index.html#box-cox-transformation",
    "title": "Diagnostics",
    "section": "6.3 Box-Cox Transformation",
    "text": "6.3 Box-Cox Transformation\nBox-Cox developed method to determine best power transformation to induce Normality.\n\\[\n\\begin{align*}\n(y^{\\lambda} - 1) / \\lambda, \\hspace{0.2cm} \\lambda &\\neq 0 \\\\\n\\log(y), \\hspace{0.2cm} \\lambda &= 0\n\\end{align*}\n\\]\n\n6.3.1 R Code\n\n\nCode\nlibrary(MASS)\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nCode\nboxcox(lm_var)"
  },
  {
    "objectID": "notes/analytics/linear-regression/SLR/breakout_4.html",
    "href": "notes/analytics/linear-regression/SLR/breakout_4.html",
    "title": "1",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\n\ndf.head()\n\n\n\n\n\n\n\n\n\ndteday\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n14975\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n1\n14975\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2\n14975\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n3\n14975\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n4\n14975\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n\n\n\nCode\nnp.corrcoef(df[['temp', 'atemp', 'hum', 'windspeed', 'cnt']])\n\n\narray([[1.        , 0.9996723 , 0.99977045, ..., 0.99927588, 0.99933747,\n        0.99944454],\n       [0.9996723 , 1.        , 0.99999127, ..., 0.99992236, 0.99994156,\n        0.99997001],\n       [0.99977045, 0.99999127, 1.        , ..., 0.99986162, 0.99988766,\n        0.99992905],\n       ...,\n       [0.99927588, 0.99992236, 0.99986162, ..., 1.        , 0.99999853,\n        0.99998878],\n       [0.99933747, 0.99994156, 0.99988766, ..., 0.99999853, 1.        ,\n        0.99999501],\n       [0.99944454, 0.99997001, 0.99992905, ..., 0.99998878, 0.99999501,\n        1.        ]])\n\n\n\n\nCode\nax = sns.pairplot(df[['temp', 'atemp', 'hum', 'windspeed', 'cnt']])\n\nplt.show()\n\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning:\n\nis_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning:\n\nThe figure layout has changed to tight\n\n\n\n\n\n\n\nStrong linear relationship between temp and atemp\n\n\n1 2\nPicking atemp\n\n\nCode\nslr = smf.ols('cnt ~ atemp', df).fit()\nslr.summary()\nslr.rsquared\n\n\n0.16074430690746544\n\n\n\natemp explains about 16% of the variability in cnt\natemp has a coefficient of 423.1802. For every unit increase in atemp the cnt increases by 423.1802 all other variables held constant"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html",
    "href": "notes/analytics/linear-regression/regularized/index.html",
    "title": "Model Building and Scoring for Prediction",
    "section": "",
    "text": "Linear regression is a good initial approach to model building, but not the only form of regression.\nLinear regression is the best linear unbiased estimator.\n\n\n\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, s_{\\hat{\\beta}_j})\n\\]\nOn average, coefficients from all samples are centered from the true coefficient. What does it mean to be best? If assumptions hold, \\(s_{\\hat{\\beta}_j}\\) is the minimum variance of all the unbiased estimators.\nPut another way: The spread of our guesses is as narrow as it can be.\nWhat if biased estimators had smaller variance?"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#best-linear-unbiased-estimator",
    "href": "notes/analytics/linear-regression/regularized/index.html#best-linear-unbiased-estimator",
    "title": "Model Building and Scoring for Prediction",
    "section": "",
    "text": "\\[\n\\hat{\\beta}_j \\sim N(\\beta_j, s_{\\hat{\\beta}_j})\n\\]\nOn average, coefficients from all samples are centered from the true coefficient. What does it mean to be best? If assumptions hold, \\(s_{\\hat{\\beta}_j}\\) is the minimum variance of all the unbiased estimators.\nPut another way: The spread of our guesses is as narrow as it can be.\nWhat if biased estimators had smaller variance?"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#penalties-in-models",
    "href": "notes/analytics/linear-regression/regularized/index.html#penalties-in-models",
    "title": "Model Building and Scoring for Prediction",
    "section": "2.1 Penalties in Models",
    "text": "2.1 Penalties in Models\nRecall OLS regression minimizes the sum of squared errors:\n\\[\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2) = \\min(SSE)\n\\]\nRegularized regression introduces a penalty term to the minimization:\n\\[\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\text{Penalty}) = \\min(SSE + \\text{Penalty})\n\\]"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#r-code",
    "href": "notes/analytics/linear-regression/regularized/index.html#r-code",
    "title": "Model Building and Scoring for Prediction",
    "section": "3.1 R Code",
    "text": "3.1 R Code\nIn R, all regularized regression functions have to take in a model.matrix instead of the usual lm\n\n\nCode\nlibrary(tidyverse)\nlibrary(AmesHousing)\nlibrary(car)\n\nset.seed(123)\n\names &lt;- make_ordinal_ames()\n\names &lt;- ames %&gt;%\n    mutate(id = row_number())\n\ntrain &lt;- ames %&gt;%\n    sample_frac(0.7)\n\ntest &lt;- anti_join(ames, train, by = \"id\")\ndim(train)\n\n\n[1] 2051   82\n\n\n\n\nCode\ntrain_reg &lt;- train %&gt;%\n    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %&gt;%\n    replace(is.na(.), 0)\n\n# Leave all continuous variables alone\n# Dummy encode all categorical variables\n# Still need to factor any numeric categoricals beforehand\n# We delete the first column from the model matrix since we don't need the intercept column that model.matrix provides\ntrain_x &lt;- model.matrix(Sale_Price ~ ., data = train_reg)[, -1]\ntrain_y &lt;- train_reg$Sale_Price\n\ntest_reg &lt;- test %&gt;%\n    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %&gt;%\n    replace(is.na(.), 0)\n\ntest_x &lt;- model.matrix(Sale_Price ~ ., data = test_reg)[, -1]\ntest_y &lt;- train_reg$Sale_Price\n\n\n\n\nCode\nlibrary(glmnet)\n\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'Matrix'\n\n\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n\n\nLoaded glmnet 4.1-7\n\n\nCode\n# alpha controls the option as to which penalty to use\n# alpha = 0 uses Ridge\names_ridge &lt;- glmnet(x = train_x, y = train_y, alpha = 0)\nplot(ames_ridge, xvar = \"lambda\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#python-code",
    "href": "notes/analytics/linear-regression/regularized/index.html#python-code",
    "title": "Model Building and Scoring for Prediction",
    "section": "3.2 Python Code",
    "text": "3.2 Python Code\n\n\n\n\n\n\n\nCaution\n\n\n\nTODO: Adding Python regularized regression implementations\n\n\n\n\nCode\ntrain = r.train\ntest = r.test"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#differences-in-effects",
    "href": "notes/analytics/linear-regression/regularized/index.html#differences-in-effects",
    "title": "Model Building and Scoring for Prediction",
    "section": "4.1 Differences in Effects",
    "text": "4.1 Differences in Effects\nLASSO can delete variables whereas Ridge can only get close to 0. Differences in effects are due to differences in penalty. The deletion of variables from LASSO can actually act as variable selection.\n\\[\n\\begin{align*}\n\\hat{\\beta}_{OLS} &= (X^TX)^{-1}X^TY \\\\\n\\hat{\\beta}_R &= (X^TX + \\lambda I)^{-1}X^TY \\\\\n\\hat{\\beta}_L &= (X^TX)^{-1}(X^TY - \\lambda I)\n\\end{align*}\n\\]\nIf \\(\\lambda = X^TY\\), \\(\\hat{\\beta}_L\\) can be 0"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#r-code-1",
    "href": "notes/analytics/linear-regression/regularized/index.html#r-code-1",
    "title": "Model Building and Scoring for Prediction",
    "section": "4.2 R Code",
    "text": "4.2 R Code\n\n\nCode\nlibrary(glmnet)\n\n# alpha = 1 sets glmnet to use LASSO\names_lasso &lt;- glmnet(x = train_x, y = train_y, alpha = 1)\nplot(ames_lasso, xvar = \"lambda\")"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#cross-validation",
    "href": "notes/analytics/linear-regression/regularized/index.html#cross-validation",
    "title": "Model Building and Scoring for Prediction",
    "section": "6.1 Cross-Validation",
    "text": "6.1 Cross-Validation\nCross-validation is one approach to prevent overfitting when tuning a parameter.\n\nSplit training data into multiple pieces\nBuild model on majority of pieces\nEvaluate on remaining piece\nRepeat process with switching out pieces for building and evaluation\n\n\n\nCode\n# Gives us lambda.min and lambda.1se on the graph\n# By default, k-fold = 10\names_lasso_cv &lt;- cv.glmnet(x = train_x, y = train_y, alpha = 1)\nplot(ames_lasso_cv)\n\n\n\n\n\nIn regularized regression our evaluation statistic is mean-squared error. With each fold, we train a model on the remaining trainning data and calculate the mean-squared error on the fold. We do this for \\(k\\) folds and then calculate the average of all the mean-squared errors to get the final statistic for the model.\nWhen tuning \\(\\lambda\\), we will do this across a range of different \\(\\lambda\\) values with average mean-squared error being compared throughout the range."
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#important-variables",
    "href": "notes/analytics/linear-regression/regularized/index.html#important-variables",
    "title": "Model Building and Scoring for Prediction",
    "section": "6.2 Important Variables",
    "text": "6.2 Important Variables\n\n\nCode\ncoef(ames_lasso, s = c(ames_lasso_cv$lambda.min, ames_lasso_cv$lambda.1se))\n\n\n37 x 2 sparse Matrix of class \"dgCMatrix\"\n                                       s1            s2\n(Intercept)                  4.809883e+04  8.326132e+04\nLot_Area                     5.455632e-01  3.727557e-01\nStreetPave                   7.742774e+03  .           \nBldg_TypeTwoFmCon           -9.791571e+03  .           \nBldg_TypeDuplex             -2.380411e+04  .           \nBldg_TypeTwnhs              -1.755640e+04 -1.552627e+03\nBldg_TypeTwnhsE             -8.901776e+03  .           \nHouse_StyleOne_and_Half_Unf  1.006755e+04  .           \nHouse_StyleOne_Story         2.100854e+04  .           \nHouse_StyleSFoyer            3.314566e+04  .           \nHouse_StyleSLvl              9.806126e+03  .           \nHouse_StyleTwo_and_Half_Fin -2.786798e+04  .           \nHouse_StyleTwo_and_Half_Unf -8.735039e+03  .           \nHouse_StyleTwo_Story         .             .           \nOverall_Qual.L               2.270985e+05  1.963587e+05\nOverall_Qual.Q               1.004889e+05  1.061646e+05\nOverall_Qual.C               1.201336e+04  .           \nOverall_Qual^4              -9.446123e+02  .           \nOverall_Qual^5              -2.520393e+04 -5.544525e+03\nOverall_Qual^6              -8.477324e+03  .           \nOverall_Qual^7              -2.899510e+03  .           \nOverall_Qual^8               .             .           \nOverall_Qual^9              -7.317252e+01  .           \nRoof_StyleGable             -1.083345e+03  .           \nRoof_StyleGambrel            .             .           \nRoof_StyleHip                4.183190e+03  3.243469e+03\nRoof_StyleMansard           -4.033550e+04  .           \nRoof_StyleShed              -2.192610e+04  .           \nCentral_AirY                 1.360755e+04  1.155248e+04\nFirst_Flr_SF                 4.577067e-02  1.421832e+01\nSecond_Flr_SF                4.756848e+00  .           \nFull_Bath                    1.503856e+04  4.316036e+03\nHalf_Bath                    1.073726e+04  1.820427e+02\nFireplaces                   8.158764e+03  6.631559e+03\nGarage_Area                  3.623071e+01  4.285732e+01\nGr_Liv_Area                  4.233104e+01  3.285089e+01\nTotRms_AbvGrd               -7.343589e+02  ."
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/index.html#model-metrics",
    "href": "notes/analytics/linear-regression/regularized/index.html#model-metrics",
    "title": "Model Building and Scoring for Prediction",
    "section": "7.1 Model Metrics",
    "text": "7.1 Model Metrics\nRoot MSE:\n\\[\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n\\]\nMean Absolute Error:\n\\[\nMAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n\\]\nMean Absolute Percentage Error:\n\\[\nMAPE = 100 \\cdot \\frac{1}{n}\\sum_{i=1}^{n} |\\frac{y_i - \\hat{y}_i}{y_i}\n\\]"
  },
  {
    "objectID": "notes/analytics/linear-regression/regularized/breakout_11.html",
    "href": "notes/analytics/linear-regression/regularized/breakout_11.html",
    "title": "Breakout 11",
    "section": "",
    "text": "1 1\n\n\nCode\nlibrary(glmnet)\n\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-7\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ tidyr::expand() masks Matrix::expand()\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n✖ tidyr::pack()   masks Matrix::pack()\n✖ tidyr::unpack() masks Matrix::unpack()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\nset.seed(123)\n\nbike &lt;- bike %&gt;%\n    mutate(id = row_number())\ntrain &lt;- bike %&gt;%\n    sample_frac(0.7)\ntest &lt;- anti_join(bike, train, by = \"id\")\n\n\n\n\nCode\nnrow(train)\n\n\n[1] 12165\n\n\nCode\nnrow(test)\n\n\n[1] 5214\n\n\n12165 rows in train, 5214 rows in test\n\n\nCode\ntrain_x &lt;- model.matrix(cnt ~ temp + atemp + hum + windspeed, data = train)[, -1]\ntrain_y &lt;- train$cnt\n\ntest_x &lt;- model.matrix(cnt ~ temp + atemp + hum + windspeed, data = test)[, -1]\ntest_y &lt;- test$cnt\n\n\n\n\nCode\ntrain_ridge &lt;- glmnet(x = train_x, y = train_y, alpha = 0)\ntrain_ridge_cv &lt;- cv.glmnet(x = train_x, y = train_y, alpha = 0)\ntrain_ridge_cv$lambda.min\n\n\n[1] 7.388165\n\n\nMin. \\(\\lambda\\) is 7.388165\n\n\nCode\ntest$pred_ridge &lt;- predict(train_ridge, s = train_ridge_cv$lambda.min, newx = test_x)\nhead(test$pred_ridge)\n\n\n         s1\n1  25.04585\n2  92.24244\n3  96.43314\n4 117.79311\n5  98.29221\n6  98.29221\n\n\nCode\nhead(test$cnt)\n\n\n[1]  3 56 34 39  6  3"
  },
  {
    "objectID": "notes/analytics/linear-regression/model-selection/breakout_7.html",
    "href": "notes/analytics/linear-regression/model-selection/breakout_7.html",
    "title": "Breakout 7",
    "section": "",
    "text": "1 1\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\nglimpse(bike)\n\n\nRows: 17,379\nColumns: 16\n$ dteday     &lt;int&gt; 14975, 14975, 14975, 14975, 14975, 14975, 14975, 14975, 149…\n$ season     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yr         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hr         &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ holiday    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weekday    &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ workingday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weathersit &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3,…\n$ temp       &lt;dbl&gt; 0.24, 0.22, 0.22, 0.24, 0.24, 0.24, 0.22, 0.20, 0.24, 0.32,…\n$ atemp      &lt;dbl&gt; 0.2879, 0.2727, 0.2727, 0.2879, 0.2879, 0.2576, 0.2727, 0.2…\n$ hum        &lt;dbl&gt; 0.81, 0.80, 0.80, 0.75, 0.75, 0.75, 0.80, 0.86, 0.75, 0.76,…\n$ windspeed  &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0896, 0.0000, 0.0…\n$ casual     &lt;int&gt; 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 40, 41, 1…\n$ registered &lt;int&gt; 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 71, 70, 5…\n$ cnt        &lt;int&gt; 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, 106, 110…\n\n\n\n\nCode\nset.seed(18954)\nbike &lt;- bike %&gt;%\n    mutate(id = row_number()) %&gt;%\n    select(-dteday)\n\ntrain &lt;- sample_frac(bike, 0.7)\ntest &lt;- anti_join(bike, train, by = \"id\")\n\n\n\n\nCode\ntrain &lt;- train %&gt;%\n    select(-id)\n\nfull.model &lt;- lm(cnt ~ ., data = train)\nempty.model &lt;- lm(cnt ~ 1, data = train)\n\ncor(train)\n\n\n                  season           yr          mnth            hr      holiday\nseason      1.0000000000 -0.017728740  0.8341892714  0.0007029716 -0.008004203\nyr         -0.0177287399  1.000000000 -0.0127230269  0.0015380450  0.009701818\nmnth        0.8341892714 -0.012723027  1.0000000000  0.0005688982  0.016990981\nhr          0.0007029716  0.001538045  0.0005688982  1.0000000000  0.005471284\nholiday    -0.0080042032  0.009701818  0.0169909807  0.0054712840  1.000000000\nweekday    -0.0005697724  0.001429914  0.0107269128 -0.0014009498 -0.103965916\nworkingday  0.0165320138 -0.008277955  0.0017000694 -0.0005299508 -0.251746297\nweathersit -0.0151361998 -0.017434710  0.0054373800 -0.0187000757 -0.022398585\ntemp        0.3127250206  0.044994421  0.2027039404  0.1373762513 -0.022839374\natemp       0.3206906241  0.043824690  0.2093992596  0.1329508772 -0.027588827\nhum         0.1566301682 -0.078213509  0.1672219767 -0.2791132639 -0.012661765\nwindspeed  -0.1583175184 -0.011899846 -0.1395026596  0.1423059728 -0.001187728\ncasual      0.1200666387  0.139655612  0.0678288263  0.3015976700  0.029554121\nregistered  0.1732086022  0.251636065  0.1214334433  0.3750279189 -0.044113763\ncnt         0.1771539640  0.247838770  0.1197199984  0.3949779016 -0.028641358\n                 weekday    workingday   weathersit          temp        atemp\nseason     -0.0005697724  0.0165320138 -0.015136200  0.3127250206  0.320690624\nyr          0.0014299143 -0.0082779549 -0.017434710  0.0449944213  0.043824690\nmnth        0.0107269128  0.0017000694  0.005437380  0.2027039404  0.209399260\nhr         -0.0014009498 -0.0005299508 -0.018700076  0.1373762513  0.132950877\nholiday    -0.1039659165 -0.2517462966 -0.022398585 -0.0228393738 -0.027588827\nweekday     1.0000000000  0.0440390626  0.002079925 -0.0002332587 -0.006308329\nworkingday  0.0440390626  1.0000000000  0.046345654  0.0503730352  0.050603957\nweathersit  0.0020799250  0.0463456539  1.000000000 -0.1107357489 -0.113473368\ntemp       -0.0002332587  0.0503730352 -0.110735749  1.0000000000  0.988793611\natemp      -0.0063083285  0.0506039568 -0.113473368  0.9887936108  1.000000000\nhum        -0.0423660688  0.0171985675  0.418553933 -0.0711446535 -0.053368085\nwindspeed   0.0085516606 -0.0086861227  0.026242630 -0.0238094722 -0.062370548\ncasual      0.0247987465 -0.3092849530 -0.157273847  0.4601798782  0.456775438\nregistered  0.0261143092  0.1312660111 -0.127020760  0.3366572816  0.335080666\ncnt         0.0285440726  0.0245754369 -0.148882272  0.4064747925  0.404228875\n                   hum    windspeed      casual  registered         cnt\nseason      0.15663017 -0.158317518  0.12006664  0.17320860  0.17715396\nyr         -0.07821351 -0.011899846  0.13965561  0.25163607  0.24783877\nmnth        0.16722198 -0.139502660  0.06782883  0.12143344  0.11972000\nhr         -0.27911326  0.142305973  0.30159767  0.37502792  0.39497790\nholiday    -0.01266176 -0.001187728  0.02955412 -0.04411376 -0.02864136\nweekday    -0.04236607  0.008551661  0.02479875  0.02611431  0.02854407\nworkingday  0.01719857 -0.008686123 -0.30928495  0.13126601  0.02457544\nweathersit  0.41855393  0.026242630 -0.15727385 -0.12702076 -0.14888227\ntemp       -0.07114465 -0.023809472  0.46017988  0.33665728  0.40647479\natemp      -0.05336809 -0.062370548  0.45677544  0.33508067  0.40422887\nhum         1.00000000 -0.295339230 -0.34747646 -0.27486663 -0.32413130\nwindspeed  -0.29533923  1.000000000  0.08710282  0.07999503  0.09048990\ncasual     -0.34747646  0.087102823  1.00000000  0.50678257  0.69607810\nregistered -0.27486663  0.079995030  0.50678257  1.00000000  0.97170007\ncnt        -0.32413130  0.090489897  0.69607810  0.97170007  1.00000000\n\n\nCode\nfor.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ), direction = \"forward\", k = 2, trace = FALSE\n)\n\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\nWarning: attempting model selection on an essentially perfect fit is nonsense\n\n\nCode\nfor.model\n\n\n\nCall:\nlm(formula = cnt ~ registered + casual + workingday + hum + season + \n    weathersit + weekday, data = train)\n\nCoefficients:\n(Intercept)   registered       casual   workingday          hum       season  \n  1.417e-12    1.000e+00    1.000e+00   -5.752e-14   -1.960e-13    2.421e-14  \n weathersit      weekday  \n  3.765e-14    9.453e-15  \n\n\n\n\n2 2\n\n\nCode\ntrain &lt;- train %&gt;%\n    select(-c(casual, registered))\n\nfull.model &lt;- lm(cnt ~ ., data = train)\nempty.model &lt;- lm(cnt ~ 1, data = train)\n\nfor.model &lt;- step(empty.model,\n    scope = list(\n        lower = empty.model,\n        upper = full.model\n    ), direction = \"forward\", k = 2, trace = FALSE\n)\nfor.model\n\n\n\nCall:\nlm(formula = cnt ~ temp + hr + yr + hum + season + atemp + holiday + \n    windspeed + weekday + weathersit, data = train)\n\nCoefficients:\n(Intercept)         temp           hr           yr          hum       season  \n    -22.084       13.217        7.669       80.195     -201.396       19.819  \n      atemp      holiday    windspeed      weekday   weathersit  \n    312.098      -26.051       39.301        1.701       -4.288"
  },
  {
    "objectID": "notes/analytics/bayesian/introduction/index.html",
    "href": "notes/analytics/bayesian/introduction/index.html",
    "title": "Bayesian Workshop",
    "section": "",
    "text": "The main difference between Frequentist and Bayesian statistics is how they view population parameters. Bayesian says there is a whole distribution behind these parameters and we are trying to uncover the underlying distribution. We are no longer focused on fixed values."
  },
  {
    "objectID": "notes/analytics/bayesian/introduction/index.html#bayes-theorem",
    "href": "notes/analytics/bayesian/introduction/index.html#bayes-theorem",
    "title": "Bayesian Workshop",
    "section": "2.1 Bayes Theorem",
    "text": "2.1 Bayes Theorem\n\\[\nP(\\mu|Y) = \\frac{P(Y|\\mu)P(\\mu)}{P(Y)}\n\\]"
  },
  {
    "objectID": "notes/analytics/bayesian/introduction/index.html#process",
    "href": "notes/analytics/bayesian/introduction/index.html#process",
    "title": "Bayesian Workshop",
    "section": "2.2 Process",
    "text": "2.2 Process\n\nDecide type of data being collected and decide what distribution the data will follow.\nAsk what parameters are needed for this distribution? Pick appropriate priors.\nGet data.\nUse sampling distribution (with data) and prior to get posterior distribution."
  },
  {
    "objectID": "notes/analytics/bayesian/introduction/index.html#common-sampling-distributions",
    "href": "notes/analytics/bayesian/introduction/index.html#common-sampling-distributions",
    "title": "Bayesian Workshop",
    "section": "2.3 Common Sampling Distributions",
    "text": "2.3 Common Sampling Distributions\n\nNormal: Continuous data that can take on negative and positive values\nGamma: Continuous data that can only be positive (\\(\\chi^2\\) is a special case)\nBinomial: Counting number of successes\nPoisson: Count data (number of accidents at an intersection)"
  },
  {
    "objectID": "notes/analytics/bayesian/introduction/index.html#common-priors",
    "href": "notes/analytics/bayesian/introduction/index.html#common-priors",
    "title": "Bayesian Workshop",
    "section": "2.4 Common Priors",
    "text": "2.4 Common Priors\n\nNormal:\n\n\\(\\mu\\): Normal\n\\(\\sigma^2\\): Gamma or Inverse-Gamma or Uniform\n\nGamma:\n\n\\(\\alpha, \\beta\\): Gamma or Inverse Gamma or Uniform\n\nBinomial:\n\n\\(p\\): Uniform(0, 1) or Beta\n\nPoisson:\n\n\\(\\lambda\\): Gamma or Uniform\n\n\nYou need hyperparameters for each of these distributions. You can choose them so that these distributions are very spread out."
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html",
    "href": "notes/analytics/ANOVA/introduction/index.html",
    "title": "Introduction to ANOVA and Regression",
    "section": "",
    "text": "The population model for our linear model is written as:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_kx_k + \\varepsilon\n\\]\n\n\\(\\varepsilon\\) is the random error\nAll the modeled signal is the rest of the equation which is called the deterministic component\n\\(x_1, \\cdots, x_k\\) are the explanatory variables\n\\(y\\) is the response variable\n\nTypically linear models are used in an explanatory model fashion–we are trying to answer how our explanatory variables are related to our response. We are not predicting the response. Models tend to be simpler and we focus on p-values and confidence intervals for interpretation.\n\n\nBefore you look for any relationships, you should split into training, validation and test samples.\nDifferent rules of thumb for splits:\n\nLots of data? 50-40-10 split\nNot so much data? 70-20-10 split\nNot enough data? Use Cross-Validation\n\n\n\nModels will capture nuances of the data on which they’re built (training data)\nWhen these “patterns” do not hold up in validation or test, the model performance suffers. We call this overfitting.\n\n\n\nOverfitting Example\n\n\n\n\n\n\n\n\nCode\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(AmesHousing)\n\names &lt;- make_ordinal_ames()\nset.seed(123)\names &lt;- ames |&gt; mutate(id = row_number())\ntrain &lt;- ames |&gt; sample_frac(0.7)\ntest &lt;- anti_join(ames, train, by = \"id\")\n\ndim(train)\n\n\n[1] 2051   82\n\n\nCode\ndim(test)\n\n\n[1] 879  82"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#honest-model-assessment",
    "href": "notes/analytics/ANOVA/introduction/index.html#honest-model-assessment",
    "title": "Introduction to ANOVA and Regression",
    "section": "",
    "text": "Before you look for any relationships, you should split into training, validation and test samples.\nDifferent rules of thumb for splits:\n\nLots of data? 50-40-10 split\nNot so much data? 70-20-10 split\nNot enough data? Use Cross-Validation\n\n\n\nModels will capture nuances of the data on which they’re built (training data)\nWhen these “patterns” do not hold up in validation or test, the model performance suffers. We call this overfitting.\n\n\n\nOverfitting Example"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#train-test-split-in-r",
    "href": "notes/analytics/ANOVA/introduction/index.html#train-test-split-in-r",
    "title": "Introduction to ANOVA and Regression",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nlibrary(AmesHousing)\n\names &lt;- make_ordinal_ames()\nset.seed(123)\names &lt;- ames |&gt; mutate(id = row_number())\ntrain &lt;- ames |&gt; sample_frac(0.7)\ntest &lt;- anti_join(ames, train, by = \"id\")\n\ndim(train)\n\n\n[1] 2051   82\n\n\nCode\ndim(test)\n\n\n[1] 879  82"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#assumptions-for-anova",
    "href": "notes/analytics/ANOVA/introduction/index.html#assumptions-for-anova",
    "title": "Introduction to ANOVA and Regression",
    "section": "3.1 Assumptions for ANOVA",
    "text": "3.1 Assumptions for ANOVA\n\nObservations are independent\nEach group is normally distributed\n\nOr the residuals of the ANOVA model are normally distributed\n\nAll groups have equal variances (homoskedasticity)\n\nIf true, use “pooled” variance\nIf false, use Welch’s ANOVA\n\n\n\n3.1.1 Assessing ANOVA Assumptions\n\nGood data collection designs help the independence assumption\nInformal plots (QQ-Plots) or formal tests can verify the normally distributed assumption\nFormal test of equal variances or viewing residual plot to assess homoskedasticity"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#anova-hypothesis-test-in-r",
    "href": "notes/analytics/ANOVA/introduction/index.html#anova-hypothesis-test-in-r",
    "title": "Introduction to ANOVA and Regression",
    "section": "3.2 ANOVA Hypothesis Test in R",
    "text": "3.2 ANOVA Hypothesis Test in R\n\\(H_0\\) is the means of each level of Exter-Qual are equal.\n\\(H_a\\) is at least one mean is different.\n\n\nCode\names_lm &lt;- lm(Sale_Price ~ Exter_Qual, data = train)\nanova(ames_lm)\n\n\nAnalysis of Variance Table\n\nResponse: Sale_Price\n             Df     Sum Sq    Mean Sq F value    Pr(&gt;F)    \nExter_Qual    3 6.6913e+12 2.2304e+12  701.83 &lt; 2.2e-16 ***\nResiduals  2047 6.5054e+12 3.1780e+09                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThere appears to be a significant difference in mean sales price between the different levels of exterior quality.\n\n\n\nCode\ntrain$pred_anova &lt;- predict(ames_lm, data = train)\ntrain$resid_anova &lt;- resid(ames_lm, data = train)\n\nmodel_output &lt;- train |&gt; select(Sale_Price, pred_anova, resid_anova)\nmodel_output\n\n\n# A tibble: 2,051 × 3\n   Sale_Price pred_anova resid_anova\n        &lt;int&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1     232600    228910.       3690.\n 2     166000    228910.     -62910.\n 3     170000    142107.      27893.\n 4     252000    228910.      23090.\n 5     134000    142107.      -8107.\n 6     164700    228910.     -64210.\n 7     193500    142107.      51393.\n 8     118500    142107.     -23607.\n 9      94000    142107.     -48107.\n10     111250    142107.     -30857.\n# ℹ 2,041 more rows\n\n\nAnd then to test assumptions:\n\n\nCode\npar(mfrow = c(2, 2))\nplot(ames_lm)\n\n\n\n\n\nTo formally test our variance, we have Levene’s Test which requires normality of underlying data and Fligner’s Test which does not require normality.\n\n\\(H_0:\\) The group variances are equal\n\\(H_a:\\) The group variances are NOT equal\n\n\n\nCode\nlibrary(car)\n\nleveneTest(Sale_Price ~ Exter_Qual, data = train)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    3  76.879 &lt; 2.2e-16 ***\n      2047                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\nfligner.test(Sale_Price ~ Exter_Qual, data = train)$p.value\n\n\n[1] 1.873388e-44"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#r-code",
    "href": "notes/analytics/ANOVA/introduction/index.html#r-code",
    "title": "Introduction to ANOVA and Regression",
    "section": "5.1 R Code",
    "text": "5.1 R Code\n\n\nCode\nkruskal.test(Sale_Price ~ Exter_Qual, data = train)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Sale_Price by Exter_Qual\nKruskal-Wallis chi-squared = 975.98, df = 3, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#tukeys-honest-significant-difference",
    "href": "notes/analytics/ANOVA/introduction/index.html#tukeys-honest-significant-difference",
    "title": "Introduction to ANOVA and Regression",
    "section": "7.1 Tukey’s Honest Significant Difference",
    "text": "7.1 Tukey’s Honest Significant Difference\nAppropriate for making all pairwise comparisons between groups.\nExperimentwise error rate is equal to \\(\\alpha\\) when all pairwise comparisons are made and less than \\(\\alpha\\) otherwise.\n\n\nCode\names_aov &lt;- aov(Sale_Price ~ Exter_Qual, data = train)\ntukey.ames &lt;- TukeyHSD(ames_aov)\nprint(tukey.ames)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = Sale_Price ~ Exter_Qual, data = train)\n\n$Exter_Qual\n                       diff       lwr       upr p adj\nTypical-Fair       57887.91  30194.31  85581.52 5e-07\nGood-Fair         144690.25 116739.87 172640.63 0e+00\nExcellent-Fair    291684.79 259752.41 323617.16 0e+00\nGood-Typical       86802.34  79910.03  93694.64 0e+00\nExcellent-Typical 233796.87 216886.62 250707.12 0e+00\nExcellent-Good    146994.54 129666.98 164322.10 0e+00\n\n\n\nConclusion is that all pairs are significantly different"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/index.html#dunnetts-test-for-control-comparison",
    "href": "notes/analytics/ANOVA/introduction/index.html#dunnetts-test-for-control-comparison",
    "title": "Introduction to ANOVA and Regression",
    "section": "7.2 Dunnett’s Test for Control Comparison",
    "text": "7.2 Dunnett’s Test for Control Comparison\nIf you’re not making all pairwise comparisons, Tukey’s is overly conservative.\n\n\nCode\nlibrary(DescTools)\nDunnettTest(x = train$Sale_Price, g = train$Exter_Qual, control = \"Typical\")\n\n\n\n  Dunnett's test for comparing several treatments with a control :  \n    95% family-wise confidence level\n\n$Typical\n                       diff    lwr.ci    upr.ci    pval    \nFair-Typical      -57887.91 -83628.55 -32147.28 2.6e-07 ***\nGood-Typical       86802.34  80396.08  93208.59 &lt; 2e-16 ***\nExcellent-Typical 233796.87 218079.15 249514.60 &lt; 2e-16 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "notes/analytics/ANOVA/introduction/breakout_3.html",
    "href": "notes/analytics/ANOVA/introduction/breakout_3.html",
    "title": "1 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\nbike &lt;- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv\")\nglimpse(bike)\n\n\nRows: 17,379\nColumns: 16\n$ dteday     &lt;int&gt; 14975, 14975, 14975, 14975, 14975, 14975, 14975, 14975, 149…\n$ season     &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ yr         &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ hr         &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 1…\n$ holiday    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weekday    &lt;int&gt; 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,…\n$ workingday &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ weathersit &lt;int&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3,…\n$ temp       &lt;dbl&gt; 0.24, 0.22, 0.22, 0.24, 0.24, 0.24, 0.22, 0.20, 0.24, 0.32,…\n$ atemp      &lt;dbl&gt; 0.2879, 0.2727, 0.2727, 0.2879, 0.2879, 0.2576, 0.2727, 0.2…\n$ hum        &lt;dbl&gt; 0.81, 0.80, 0.80, 0.75, 0.75, 0.75, 0.80, 0.86, 0.75, 0.76,…\n$ windspeed  &lt;dbl&gt; 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0896, 0.0000, 0.0…\n$ casual     &lt;int&gt; 3, 8, 5, 3, 0, 0, 2, 1, 1, 8, 12, 26, 29, 47, 35, 40, 41, 1…\n$ registered &lt;int&gt; 13, 32, 27, 10, 1, 1, 0, 2, 7, 6, 24, 30, 55, 47, 71, 70, 5…\n$ cnt        &lt;int&gt; 16, 40, 32, 13, 1, 1, 2, 3, 8, 14, 36, 56, 84, 94, 106, 110…\n\n\n\n1 1\n\n\nCode\nbike_lm &lt;- lm(cnt ~ weathersit, data = bike)\n\nggplot(bike, aes(x = cnt, fill = factor(weathersit))) +\n    geom_density(alpha = 0.2, position = \"identity\") +\n    labs(x = \"Number of riders\")\n\n\n\n\n\nCode\nggplot(bike, aes(y = cnt, x = factor(weathersit), fill = factor(weathersit))) +\n    geom_boxplot() +\n    labs(y = \"Number of riders\", x = \"Weather Category\") +\n    stat_summary(fun = mean, geom = \"point\", shape = 2, size = 3, color = \"pink\", fill = \"red\") +\n    scale_fill_brewer(palette = \"Blues\") +\n    coord_flip()\n\n\n\n\n\nCode\nkruskal.test(cnt ~ weathersit, data = bike)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by weathersit\nKruskal-Wallis chi-squared = 397.03, df = 3, p-value &lt; 2.2e-16\n\n\n\n\n2 2\n\n\nCode\nggplot(bike, aes(x = cnt, fill = factor(season))) +\n    geom_density(alpha = 0.2, position = \"identity\") +\n    labs(x = \"Number of riders\")\n\n\n\n\n\nCode\nggplot(bike, aes(y = cnt, x = factor(season), fill = factor(season))) +\n    geom_boxplot() +\n    labs(y = \"Number of riders\", x = \"Weather Category\") +\n    stat_summary(fun = mean, geom = \"point\", shape = 2, size = 3, color = \"pink\", fill = \"red\") +\n    scale_fill_brewer(palette = \"Blues\") +\n    coord_flip()\n\n\n\n\n\nCode\nkruskal.test(cnt ~ season, data = bike)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by season\nKruskal-Wallis chi-squared = 1190.3, df = 3, p-value &lt; 2.2e-16\n\n\n\n\nCode\nggplot(bike, aes(x = cnt, fill = factor(holiday))) +\n    geom_density(alpha = 0.2, position = \"identity\") +\n    labs(x = \"Number of riders\")\n\n\n\n\n\nCode\nggplot(bike, aes(y = cnt, x = factor(holiday), fill = factor(holiday))) +\n    geom_boxplot() +\n    labs(y = \"Number of riders\", x = \"Weather Category\") +\n    stat_summary(fun = mean, geom = \"point\", shape = 2, size = 3, color = \"pink\", fill = \"red\") +\n    scale_fill_brewer(palette = \"Blues\") +\n    coord_flip()\n\n\n\n\n\nCode\nkruskal.test(cnt ~ holiday, data = bike)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  cnt by holiday\nKruskal-Wallis chi-squared = 15.15, df = 1, p-value = 9.93e-05"
  },
  {
    "objectID": "notes/analytics/ANOVA/n-Way/breakout_5.html",
    "href": "notes/analytics/ANOVA/n-Way/breakout_5.html",
    "title": "Breakout 5",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n\n\nCode\nbike = pd.read_csv('https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/bike.csv')\nbike.head()\n\n\n\n\n\n\n\n\n\ndteday\nseason\nyr\nmnth\nhr\nholiday\nweekday\nworkingday\nweathersit\ntemp\natemp\nhum\nwindspeed\ncasual\nregistered\ncnt\n\n\n\n\n0\n14975\n1\n0\n1\n0\n0\n6\n0\n1\n0.24\n0.2879\n0.81\n0.0\n3\n13\n16\n\n\n1\n14975\n1\n0\n1\n1\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n8\n32\n40\n\n\n2\n14975\n1\n0\n1\n2\n0\n6\n0\n1\n0.22\n0.2727\n0.80\n0.0\n5\n27\n32\n\n\n3\n14975\n1\n0\n1\n3\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n3\n10\n13\n\n\n4\n14975\n1\n0\n1\n4\n0\n6\n0\n1\n0.24\n0.2879\n0.75\n0.0\n0\n1\n1\n\n\n\n\n\n\n\n\n1 1\n\n\nCode\nbike_lm = smf.ols('cnt ~ C(workingday) * C(season)', bike).fit()\n\nsm.stats.anova_lm(bike_lm)\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nC(workingday)\n1.0\n5.243871e+05\n5.243871e+05\n17.082764\n3.595344e-05\n\n\nC(season)\n3.0\n3.748219e+07\n1.249406e+07\n407.014449\n1.544102e-255\n\n\nC(workingday):C(season)\n3.0\n5.199753e+05\n1.733251e+05\n5.646347\n7.301192e-04\n\n\nResidual\n17371.0\n5.332350e+08\n3.069685e+04\nNaN\nNaN\n\n\n\n\n\n\n\n\nThe interaction between workingday and season seems to be significant towards predicting cnt\n\n\n\n2 2\nTalk with breakout group.\n\n\n3 3\n\n\nCode\ncasual_lm = smf.ols('casual ~ C(workingday) * C(season)', bike).fit()\n\nsm.stats.anova_lm(casual_lm)\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nC(workingday)\n1.0\n3.826038e+06\n3.826038e+06\n1954.028610\n0.000000e+00\n\n\nC(season)\n3.0\n3.750852e+06\n1.250284e+06\n638.543326\n0.000000e+00\n\n\nC(workingday):C(season)\n3.0\n6.559238e+05\n2.186413e+05\n111.664154\n1.292550e-71\n\n\nResidual\n17371.0\n3.401286e+07\n1.958026e+03\nNaN\nNaN\n\n\n\n\n\n\n\n\nNot a significant interaction between workingday and season for casual bikers\n\n\n\nCode\nregistered_lm = smf.ols('registered ~ C(workingday) * C(season)', bike).fit()\n\nsm.stats.anova_lm(registered_lm)\n\n\n\n\n\n\n\n\n\ndf\nsum_sq\nmean_sq\nF\nPR(&gt;F)\n\n\n\n\nC(workingday)\n1.0\n7.183321e+06\n7.183321e+06\n335.449602\n3.121975e-74\n\n\nC(season)\n3.0\n1.885079e+07\n6.283597e+06\n293.433926\n7.848911e-186\n\n\nC(workingday):C(season)\n3.0\n9.621956e+04\n3.207319e+04\n1.497767\n2.129291e-01\n\n\nResidual\n17371.0\n3.719828e+08\n2.141401e+04\nNaN\nNaN\n\n\n\n\n\n\n\n\nSignificant interaction between workingday and season for registered bikers\n\n\n\nCode\nunique_season = bike['season'].unique()\n\nfor season in unique_season:\n    sliced_data = smf.ols(\"casual ~ C(workingday)\", bike[bike[\"season\"] == season]).fit()\n    print(sm.stats.anova_lm(sliced_data))\n\nfor season in unique_season:\n    sliced_data = smf.ols(\"registered ~ C(workingday)\", bike[bike[\"season\"] == season]).fit()\n    print(sm.stats.anova_lm(sliced_data))\n\n\n                   df        sum_sq        mean_sq          F        PR(&gt;F)\nC(workingday)     1.0  1.436979e+05  143697.866870  199.44088  2.751109e-44\nResidual       4240.0  3.054935e+06     720.503576        NaN           NaN\n                   df        sum_sq       mean_sq           F         PR(&gt;F)\nC(workingday)     1.0  2.044757e+06  2.044757e+06  720.507711  3.944648e-147\nResidual       4407.0  1.250680e+07  2.837939e+03         NaN            NaN\n                   df        sum_sq       mean_sq           F         PR(&gt;F)\nC(workingday)     1.0  1.623762e+06  1.623762e+06  700.640615  1.328288e-143\nResidual       4494.0  1.041502e+07  2.317539e+03         NaN            NaN\n                   df        sum_sq        mean_sq           F         PR(&gt;F)\nC(workingday)     1.0  9.299500e+05  929949.990208  489.501677  9.987658e-103\nResidual       4230.0  8.036108e+06    1899.789180         NaN            NaN\n                   df        sum_sq       mean_sq           F        PR(&gt;F)\nC(workingday)     1.0  1.278398e+06  1.278398e+06  120.723426  1.035371e-27\nResidual       4240.0  4.489940e+07  1.058948e+04         NaN           NaN\n                   df        sum_sq       mean_sq          F        PR(&gt;F)\nC(workingday)     1.0  1.351663e+06  1.351663e+06  58.687294  2.258053e-14\nResidual       4407.0  1.015003e+08  2.303161e+04        NaN           NaN\n                   df        sum_sq       mean_sq          F        PR(&gt;F)\nC(workingday)     1.0  2.328766e+06  2.328766e+06  85.657432  3.232019e-20\nResidual       4494.0  1.221782e+08  2.718697e+04        NaN           NaN\n                   df        sum_sq       mean_sq          F        PR(&gt;F)\nC(workingday)     1.0  1.629497e+06  1.629497e+06  66.658104  4.220264e-16\nResidual       4230.0  1.034048e+08  2.444559e+04        NaN           NaN\n\n\n\nFor each individual season, there is a significant difference between working days and non-working days in registered bikers"
  }
]