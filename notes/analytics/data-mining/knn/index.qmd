---
title: KNN and Other Ideas
date: 10/13/2023
---

# K-Nearest Neighbor

We want to identify several cases that are most similar to a given observation. We can also use the information from "neighbors" to classify/predict the new observation.

![K-Nearest Neighbors](images/knn.png){#fig-knn}

## Considerations

-   How should I measure nearness?
    -   Numeric attributes?
    -   Ordinal attributes?
    -   Categorical attributes?
    -   How to combine all attributes?
-   How should I combine the results of neighbors?
    -   Classification
        -   Majority rules?
        -   Weight votes by nearness?
    -   Prediction
        -   Mean
        -   Median      
-   How many neighbors should I use?

## Choosing $k$

Smaller values of $k$ lead to higher variance which tends toward overfitting. Larger values of $k$ leadto higher bias which tends toward underfitting.

Common practice: use $k = \sqrt{n}$ where $n$ is the number of training examples.

Best practice to tune this parameter with a validation set or with cross-validation.

## Advantages and Disadvantages

Advantages:

-   Easy to explain, intuitive, understandable
-   Applicable to any type of data
-   Makes no assumptions about the underlying distribution of the data
-   Large/representative training set is only assumption

Disadvantages:

-   Computationally expensive in classification phase
-   Requires storage for the training set
-   Results dependent on choice of distance function, combinatino function, and number of neighbors, $k$
-   Susceptible to noise
-   Require lots of data preprocessing and consideration for distance metrics
-   Does not produce a model and so it does not help us understand how features are related to classes


:::{.panel-tabset group="language"}
# R

```{r knn-r}
#| warning: false
library(tidyverse)
library(class)

set.seed(7515)
load("data/breast_cancer.Rdata")

perm <- sample(1:699)
BC_randomOrder <- BCdata[perm,]

train <- BC_randomOrder[1:floor(0.75 * 699), -c(1, 7)]
test <- BC_randomOrder[(floor(0.75 * 599) + 1):699, -c(1, 7)]

train_x <- subset(train, select=-Target)
train_y <- as.factor(train$Target)

test_x <- subset(test, select=-Target)
test_y <- as.factor(test$Target)

predict_test <- knn(train_x, test_x, train_y, k = 5)
sum(predict_test == test_y) / length(test_y)
```

# Python

```{python knn-python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

train_x = r.train_x
train_y = r.train_y

test_x = r.test_x
test_y = r.test_y

knn = KNeighborsClassifier(n_neighbors=5).fit(train_x, train_y)
y_pred = knn.predict(test_x)
print("Accuracy with k = 5: ", accuracy_score(test_y, y_pred) * 100.0)
```

:::

# Multidimensional Scaling (MDS)

MDS is a technique for visualizing high-dimensional data by projecting it into a lower-dimensional space. It is a non-linear dimensionality reduction technique similar to PCA. 

To perform MDS, we need a dissimilarity matrix (or distance matrix).

## Classical MDS

Classical MDS is a method for finding a low-dimensional representation of the data that preserves the distances between points as well as possible.

## Non-metric MDS

Non-metric MDS is a method for finding a low-dimensional representation of the data that preserves the rank order of the distances between points as well as possible. Think about "squashing pictures" to make them fit on a page.

## Difference between PCA and MDS

PCA is more focused on dimensions themselves (wants to explain maximum variance) where MDS is more focused on relations among scaled objects.

To visualize data, we may prefer MDS over PCA because MDS preserves the distances between points. However, if the data will be used for analysis then PCA should be used.

# Curse of Dimensionality

When we have a large number of predictors, finding the true signal is difficult. Can be hidden in all of the dimensions. In training, it could look like the model is getting better, but in reality we are just adding noise.

# Ensemble Methods

You have a number of models and you combine their predictions--this is **ensemble**.

Let's say we create a decision tree, logistic model, and KNN model. We can combine the predictions from these models to create a final prediction by averaging or weight-averaging the probabilities. This is called **model averaging**. We could also do majority rules or proportion voting.