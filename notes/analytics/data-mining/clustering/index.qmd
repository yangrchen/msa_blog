---
title: Clustering
date: 10/06/2023
---

**Clustering** is an unsupervised approach to modeling where the goal is to partition the data into groups.

-   Observations within a cluster are similar in some sense
-   Observations in difference clusters are different in some sense

There is no one-size-fits-all solutions, but there are good and bad cluster solutions. No method works best all the time. Keep in mind that clustering uses **ALL** the variables you provide it and clusters should add some business value.

# Examples of Clustering

-   Customer segmentation: Groups of customers with similar shopping or buying patterns
-   Dimension reduction: Cluster individuals together and use cluster variable as proxy for demographic or behavioral variables
-   Gather stores with similar characteristics for sales forecasting
-   Find topics in text data
-   Find communities in social networks

# Hard vs. Fuzzy

**Hard** clustering is characterized by objects only belonging to one cluster. **Fuzzy** clustering is characterized by objects having the capability to belong to more than one cluster (usually with some probability).

Hard:

-   k-means
-   DBSCAN
-   Hierarchical

Fuzzy:

-   Fuzzy C-means
-   Gaussian Mixture Models / Expectation-Maximization

# Hierarchical vs. Flat

**Hierarchical** clusters form a tree so you can visually see which clusters are most similar to each other. **Flat** clusters are created according to some apriori process, usually iteratively updating cluster assignments.

Hierarchical is expensive when you have a large dataset.

# K-Means

K-Means revolves around using **centroids**. Centroids are "centers of clusters" or the means of a group of observations.

![K-Means Clustering](images/k-means.png){#fig-k-means}

With K-Means we are trying to minimize the sum of squared distances from each point to its cluster centroid.

$$
\sum_{C_k}\sum_{x_i \in C_k} \lVert x_i - c_k \rVert^2
$$

## Algorithm

1.  Randomly initialize **k** points.
2.  Assign each data point to the closest seed point.
3.  The seed point then represents a cluster of data.
4.  Reset seed points to be the centroids of the cluster by taking the **average** of all data points belonging to the cluster.
5.  Repeat steps 2-4 updating the cluster centroids until they do not change.

How can we determine the number of clusters we should use?

We can use an "elbow" plot to find a place where the marginal benefit to objective function for adding a cluster becomes small.

![Elbow Plot](images/elbow-plot.png){#fig-elbow-plot}

## Advantages vs. Disadvantages

Advantages:

-   Modest time/storage requirements
-   Shown you can terminate method after small number of iterations with good results
-   Good for wide variety of data types

Disadvantages

-   Dependent on initialization (different runs can provide different results)
-   Can be sensitive to outliers as we are based centroids on the average of points
    -   Consider using k-medoids
-   Have to input the number of clusters
-   Difficulty detecting non-spheroidal (globular) clusters

## Preprocessing

You will need to do data epxloration before trying to cluster the data.

-   Missing Values
    -   You will need to impute or remove missing values for the variable
-   Categorical Variables
    -   Do you need to bin categorical variables?
    -   You need to one-hot encode before putting them into algorithm
-   Continuous Variables
    -   If outliers or heavy skewness, potentially consider transformation
    -   At a minimum, center the continuous variables after any transformations are made

You can try clustering on original data or you can try it on PCA of the variables, particularly if the data is big.

:::{.panel-tabset group="language"}
# R

```{r explore-plots}
par(mfrow = c(2, 2))
hist(USArrests$Murder)
hist(USArrests$Assault)
hist(USArrests$Rape)
hist(USArrests$UrbanPop)

arrest_scal <- scale(USArrests)
```

```{r k-means-clustering}
clus2 <- kmeans(arrest_scal, centers = 2, nstart = 25)
clus2
```

```{r pca-plot}
library(factoextra)
fviz_cluster(clus2, data = arrest_scal)
```

# Python

:::

# Silhouette Method

**Silhouette** is another method to define the number of clusters to use. This method estimates how well each observations falls within its cluster (distance point is to all other points in cluster and compare it to distance from that point to points in other clusters). 

We want to find the number of clusters that **maximizes this ratio**.

```{r}
#| warning: false
library(tidyverse)
fviz_nbclust(arrest_scal, kmeans, method = "silhouette", k.max = 9)
```

From silhouette, the optimal number of clusters selected is 2. 

We can profile our original data by attaching the clusters to it:

```{r k-means-profiling}
profile_kmeans <- cbind(USArrests, clus2$cluster)
all_k <- profile_kmeans %>%
    group_by(clus2$cluster) %>%
    summarise(mean_assault = mean(Assault), mean_murder = mean(Murder), mean_rape = mean(Rape), mean_pop = mean(UrbanPop))
all_k
```

# Hierarchical Clustering

Every point starts as its own cluster and then we build up clusters as hierarchies. There are several different distant measures we can use, but the two we focus on are **Euclidean distance** and **Manhattan distance**.

Euclidean distance:

$$
\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
$$

Manhattan distance:

$$
|x_1 - x_2| + |y_1 - y_2|
$$

## Hierarchical Algorithm

1.  Each point starts as its own cluster ($n$ clusters).
2.  Calculate the distance between each point using the distance measure.
3.  Choose two points that are the closest and form a cluster ($n - 1$ clusters).
4.  Calculate distance between all single points and all clustered points.
5.  Find smallest distance and combine to form a cluster.

:::{layout-ncol="2"}
![Hierarchical Clustering Step 1](images/hierarchical-step-1.png)

![Hierarchical Clustering Step 2](images/hierarchical-step-2.png)
:::

Once all points are part of a multi-point cluster we then need to agglomerate clusters.

![Hierarchical Clustering Agglomerative](images/hierarchical-agglom.png)

## Linkages

How do we measure the distance between a point / cluster and a cluster?

### Single Linkage

Distance between the closest points in the clusters.

![Single Linkage](images/single-linkage.png)

### Complete Linkage

Distance between the farthest points in the clusters.

![Complete Linkage](images/complete-linkage.png)

### Centroid Linkage

Distance between the centroids (means) of each cluster.

![Centroid Linkage](images/centroid-linkage.png)

### Average Linkage

Average distance between all points in clusters.

![Average Linkage](images/average-linkage.png)

### Ward's Method

Minimize SSE within the cluster compared to the centroid.

$$
\sum_{j=1}^{N_i} \lvert x_j - c_i \rvert^2
$$

![Ward's Method](images/wards-method.png)

## Advantages and Disadvantages

Advantages:

-   Creates hierarchy that can help choose the number of clusters and examine how those clusters relate to each other.
-   Do NOT need to know number of clusters apriori

Disadvantages:

-   Computationally intensive, large storage reqs., not good for large datasets.
-   Lacks global objective function: only makes decision based on local criteria.
-   Greedy algorithm. Merging decisions are final once a point is assigned to a cluster.
-   Poor performance on noisy or high-dimensional data like text.

At the end of the day, you should always evaluate clustering algorithms based on how it makes sense in the business context.

![Hierarchical Clustering Options](images/hierarchical-options.png)

```{r}
library(cluster)

dist_assault <- dist(arrest_scal, method = "euclidean")
h1_comp_eucl <- agnes(dist_assault, method = "complete")
pltree(h1_comp_eucl, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

We can also get the agglomerative coefficient which measures how strong the clustering structure is (want values close to 1):

```{r}
print(h1_comp_eucl$ac)
```

Loop through possibilities of linkage methods:

```{r}
method <- c(average = "average", single = "single", complete = "complete", ward = "ward")

ac <- function(x) {
    agnes(dist_assault, method = x)$ac
}

lapply(method, ac)
```

Although we can find the best mathematical cluster here, we want the best algorithm for business sense. Explore profiles to see which makes the most sense.

# DBSCAN

Density-based spatial clustering of applications with noise. Groups together points that are close to each other based on a distance measure, a minimum number of points ("neighbors"), and a "neighborhood about each point."

Points not near other points are deemed outliers. A cluster of points must have a minimum number of points around it to be considered a cluster.

# Variable Clustering

Cluster variables that are related to reduce redundancies or multicollinearity. This is also a form of dimension reduction. Variable clustering uses eigenvalues to identify similar values and assess the goodness of the partition.

In R, the package is `ClustOfVar`. R can handle quantitative and qualitative variables, but you need to split these into two data matrices first. 

<!-- ```{r}
library(readr)
library(ClustOfVar)
telco <- read_csv("https://raw.githubusercontent.com/sjsimmo2/DataMining-Fall/master/TelcoChurn.csv")

telco[is.na(telco)] = 0
quant_var <- telco[,c(3,6,19,20)]
qual_var <- telco[,c(2,4,5,6:18)]

var_clust_h <- hclustvar(quant_var, qual_var)
stab <- stability(var_clust_h, B=50)
plot(stab)
``` -->