---
title: Model Selection
date: 07/07/2023
---

# Model Building Concepts

**Information criteria** are metrics that are 
commonly used to "select" variables for the model.

 **Selection algorithm** is an automated technique to 
 evaluate variables based on some selection criteria.

-   Stepwise Selection (forward, backward, stepwise)
-   All-regression Selection ($R^2$, $R_a^2$, 
Mallow's $C_p$)

All-regression selection tends to be unusable for 
datasets of a large size so we focus on stepwise 
selection for now.

# Fear of Overfitting

Model selection should always be done with training 
data.

Always hold out validation / testing data
to help evaluate if we have overfit our data.

# Information Criteria

AIC and BIC approximate out-of-sample prediction error 
by applying a model complexity penalty.

Lower values are "better" than higher. 
No amount of lower is "better" enough.

The two criteria may not always agree, but neither
is necessarily better.

## Akaike Information Criterion

$$
AIC = -2\log(L) + 2k
$$

Crude, large-sample approximation of leave-one-out 
cross-validation.

## Bayesian Information Criterion

Favors smaller models and penalizes
model complexity more.

$$
BIC = -2\log(L) + k\log(n)
$$

# Forward Selection

Start with a "null" model (just the intercept) and build up 
the model one variable at a time.

1.  Start with intercept only model
2.  For each variable not in model, create a linear 
regression model with the base model plus this variable
3.  See which linear regression is best relative
to the criterion
4.  If better than base model, continue. Otherwise,
stop.
5.  Update base model to new model, repeat whole process

Once a variable enters the model in forward selection,
the variable never leaves the model. This is a greedy
algorithm so we might lose out on a better model since
our initial decision on the best feature determines
future decisions.

## R Code
```{r}
library(tidyverse)
library(AmesHousing)

ames <- make_ordinal_ames()
train <- sample_frac(ames, 0.7)
train_sel <- train %>%
    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %>%
    replace(is.na(.), 0)
```
```{r}
full.model <- lm(Sale_Price ~ ., data = train_sel)
empty.model <- lm(Sale_Price ~ 1, data = train_sel)

for.model <- step(empty.model,
    scope = list(
        lower = empty.model,
        upper = full.model
    ),
    direction = "forward",
    k = 2,
    trace = FALSE
)
for.model
```

-  Good practice to specify the full and empty models
-  `k = 2` is selecting the AIC criteria 

## Python Code
```{python}
# from sklearn.feature_selection import SequentialFeatureSelector
# import statsmodels.formula.api as smf

# model = 
```

The model obtained through forward selection is 
not necessarily a model where all variables are significant.

## Other Criteria

Using `k = qchisq(0.05, 1, lower.tail = FALSE)` would
replace the selection criteria with p-value selection.
$\alpha = 0.05$ in this case.

`k = log(nrow(train_sel))` would use the BIC criterion.

# Backward Selection

Remove variables in the model one at a time.

1.  Start with full model with all variables
2.  Create models such that each model has exactly
one predictor variable removed from it and calculate
the criterion for each model
3.  See which linear regression is best based on
criterion
4.  If better than base, continue to next step
otherwise STOP
5.  Update base model to new model, repeat

There are no guarantees that forward and back
will agree.

## R Code
```{r}
full.model <- lm(Sale_Price ~ ., data = train_sel)
empty.model <- lm(Sale_Price ~ 1, data = train_sel)

back.model <- step(
    full.model,
    scope = list(
        lower = empty.model,
        upper = full.model
    ),
    direction = "backward",
    k = 2,
    trace = FALSE
)

back.model
```

It's good idea to perform different types of selection to
get an idea of what features to include in your final
model.

# Stepwise Selection

Start with "null" model and build the model one variable
at a time while also deleting variables.

1.  Start with intercept only model
2.  For each variable not in model, create a linear
regression model with base model plus this variable
3.  For each variable, create models with the base
model taking away one variable at a time
4.  See which linear regression is best based on
criterion
5.  If better than base model, continue to next
step, otherwise STOP
6.  Update base model to new model, repeat process

With stepwise, once you decide to delete a model
you never add it back into your model again.

## R Code
```{r}
full.model <- lm(Sale_Price ~ ., data = train_sel)
empty.model <- lm(Sale_Price ~ 1, data = train_sel)
step.model <- step(empty.model,
    scope = list(
        lower = empty.model,
        upper = full.model
    ),
    direction = "both", k = 2
)
```

# Issues With Automatic Search Algorithms

-   Biases in parameter estimates, predictions,
standard errors
-   Incorrect calculation of degrees of freedom
(p-value method)
-   P-values that tend to err on the side of
overestimating significance
-   Can result in locally best model (not global)

In terms of p-value, you should always adjust your
$\alpha$ size based on your sample size.

![Conservative P-Values](images/conservative-p-values.png)