---
title: Model Assessment
date: 09/07/2023
date-modified: 09/13/2023
categories:
    -   modeling
---

# Setup {.unnumbered}

::: {.panel-tabset}
# R

```{r}
#| warning: false
library(AmesHousing)
library(tidyverse)
library(reticulate)

set.seed(123)

use_condaenv("msa")

ames <- make_ordinal_ames()
ames <- ames |>
    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))
train <- sample_frac(ames, 0.7)

logit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())
```

# Python

```{python}
from statsmodels.genmod.families import Binomial
from statsmodels.genmod.generalized_linear_model import GLM

ames = r.ames
train = r.train
log_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()
```
:::


# Comparing Models

Remember that statistical models are created for two different purposes: estimation and prediction.

Estimation and prediction may not necessarily agree and they can result in tradeoffs of interpretation for predictive power.

# Deviance / Likelihood Measures

AIC and BIC approximate out-of-sample prediction error by applying penalty for model complexity.

-   AIC is a crude, large-sample approximation of leave-one-out cross-validation
-   BIC favors smaller models and penalizes model complexity more

For logistic regression, we also have "pseudo"-$R^2$ quantities. Higher values indicate a "better" model.

## Generalized / Nagelkerkge $R^2$

$$
R_G^2 = 1 - (\frac{L_0}{L_1})^{\frac{2}{n}}
$$

::: {.panel-tabset}
# R

```{r}
library(DescTools)

AIC(logit_model)
BIC(logit_model)
PseudoR2(logit_model, which = "Nagelkerke")
```

# Python

```{python}
print(log_model.aic)
print(log_model.bic_llf)
print(log_model.pseudo_rsquared())
```
:::

# Assessing Predictive Power

Logistic regression was originally designed to rank-order probabilities. It *can* be used as a classification model as well.

You might want that insight into rank-ordering than you might think. An example is when customers are predicted to buy a product but in reality that are on the fence.

## Discrimination vs. Calibration

**Discrimination** is the ability to separate the events from the non-events. How good is a model at distinguishing the 1's from the 0's.

**Calibration** is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.

These two metrics may not agree with each other.

## Coefficient of Discrimination

**Coefficient of determination** is the difference in average predicted probability between 1's and 0's:

$$
D = \bar{\hat{p}}_1 - \bar{\hat{p}}_0
$$

This is a comparison metric to see which model can separate the 1's and 0's better.

::: {.panel-tabset}
# R

```{r}
train$p_hat <- predict(logit_model, type = "response")
p1 <- train$p_hat[train$Bonus == 1]
p0 <- train$p_hat[train$Bonus == 0]

coef_discrim <- mean(p1) - mean(p0)

ggplot(train, aes(p_hat, fill = factor(Bonus))) +
    geom_density(alpha = 0.7) +
    scale_fill_grey() +
    labs(x = "Predicted Probability", fill = "Outcome", title = paste("Coefficient of Discrimination = ", round(coef_discrim, 3), sep = ""))
```

# Python

```{python}
train["p_hat"] = log_model.predict()

p1 = train.loc[train["Bonus"] == 1, "p_hat"]
p0 = train.loc[train["Bonus"] == 0, "p_hat"]

coef_discrim = p1.mean() - p0.mean()
```
:::

# Rank-order Statistics

How well does a model order predictions? Recall concordance. For a pair of subjects with and without the event, the one **with the event** had the **higher** predicted probability.

Discordance is where for a pair of subjects with and without the event, the one **with the event** had the **lower** predicted probability.

## Concordance

Interpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event $Concordance\%$ of the time.

Common metrics based on concordance:

-   C-Statistic: $c = Concordance\% + \frac{1}{2}Tied\%$
-   Somer's D (Gini): $D_{xy} = 2c - 1$
-   Kendall's $\tau_\alpha$: $\tau_\alpha = \frac{\text{#concordant} - \text{#discordant}}{\frac{n(n - 1)}{2}}$

::: {.panel-tabset group="language"}
# R

```{r}
library(Hmisc)
somers2(train$p_hat, train$Bonus)
```

# Python
:::

# Assessing Predictive Power

We want our model to correctly classify events and non-events. **Classification** forces the model to predict either 1 or 0 based on whether the predicted probability exceeds some threshold.

Strict classification-based measures completely discard any information about the actual quality of the model's predicted probablities.

![Logistic Discrimination](images/logistic-discrimination.png)

![Classification Table](images/classification-table.png)

# Sensitivity vs. Specificity

Of all the actual 1's how many did you get right (**sensitivity**):

$$
TPR = \frac{TP}{TP + FN}
$$

Of all the actual 0's how many did you get right (**specificity**):

$$
TNR = \frac{TN}{TN + FP}
$$

When we raise the cutoff, we make our model more specific. When we lower the cutoff, we make our model more sensitive.

Always consider the cost of false positives and false negatives when doing classification. When **NOT** considering costs, there are different techniques to "optimize" cut-off.

## Youden J Statistic

$$
J = \text{Sensitivity} + \text{Specificity} - 1
$$

False positives and false negatives are weighed equally, so select cut-off that products highest Youden $J$ statistic.

::: {.panel-tabset group="language"}
# R

```{r}
train <- train %>%
    mutate(Bonus_hat = ifelse(p_hat > 0.5, 1, 0))

table(train$Bonus_hat, train$Bonus)
```

```{r}
library(ROCit)

logit_meas <- measureit(train$p_hat, train$Bonus, measure = c("ACC", "SENS", "SPEC"))
```

# Python

:::

## ROC Curve

The ROC curve plots TPR vs. FPR for a grid of thresholds. Area under the curve (AUC or AUROC) summarizes the overall quality of ROC curve. Equivalent to C-statistic.

We want high sensitivity and high specificity.
<!-- Need to show AUC calculation -->

![ROC Curve](images/roc-curve.png)

:::{.panel-tabset group="language"}
# R

```{r}
logit_roc <- rocit(train$p_hat, train$Bonus)
plot(logit_roc)
```

```{r}
plot(logit_roc)$optimal
```

# Python
:::

# KS Statistic

The **Two-Sample KS statistic** can determine if there is a difference between two cumulative distribution functions.

::: {.panel-tabset group="language"}
# R

```{r}
ksplot(logit_roc)
```

:::

The two curves represent the CDFs of the distribution of 1's and 0's. The KS statistic, $D$ is the maximum distance between the two curves.

## KS-Statistic or Youden?

$D$ test statistic is used for model comparison. However, it is mathematically equivalent to Youden's J statistic. The point at which we have the maximum $D$ statistic is the optimal cutoff.

# Precision vs. Recall

In a precision-recall perspective, our focus is on the 1's.

**Recall** is the same as sensitivity:

$$
TPR = \frac{TP}{TP + FN}
$$

**Precision** is slightly different than specificity:

$$
PPV = \frac{TP}{TP + FP}
$$

## Best Cut-off?

Always consider the cost of false positives and false negatives when doing classification.

When not considering costs, many different techniques to "optimize" cutoff.

$$
F_1 = 2\left(\frac{PPV \cdot TPR}{PPV + TPR}\right)
$$

Precision and recall are weighted equally, so select cut-off that produces highest $F_1$ score.

:::{.panel-tabset group="language"}
# R

```{r}
logit_meas <- measureit(train$p_hat, train$Bonus, measure = c("PREC", "REC", "FSCR"))

fscore_table <- data.frame(Cutoff = logit_meas$Cutoff, FScore = logit_meas$FSCR)

fscore_table %>%
    arrange(desc(FScore)) %>%
    head(1)
```

# Python
:::

## Precision and Lift

Common calculation in marketing. Great for interpretation around validity of model ranking / classifying observations correctly.

$$
\text{Lift} = \frac{PPV}{\pi_1}
$$

-   $\pi_1$ is the proportion of 1's in your original population

The top **depth%** of your customers, based on predicted probability, you get **lift** times as many responses compared to targeting a random sample of **depth%** of your customers.

:::{.panel-tabset group="language"}
# R

```{r}
logit_lift <- gainstable(logit_roc)
logit_lift
```

:::