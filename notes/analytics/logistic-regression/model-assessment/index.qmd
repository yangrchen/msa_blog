---
title: Model Assessment
date: 09/07/2023
date-modified: 09/15/2023
categories:
    -   modeling
---

# Setup {.unnumbered}

::: {.panel-tabset group="language"}
# R

```{r}
#| warning: false
library(AmesHousing)
library(tidyverse)
library(reticulate)

set.seed(123)

use_condaenv("msa")

ames <- make_ordinal_ames()
ames <- ames |>
    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))
train <- sample_frac(ames, 0.7)

logit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())
```

# Python

```{python}
from statsmodels.genmod.families import Binomial
from statsmodels.genmod.generalized_linear_model import GLM

ames = r.ames
train = r.train
log_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()
```
:::


# Comparing Models

Remember that statistical models are created for two different purposes: estimation and prediction.

Estimation and prediction may not necessarily agree and they can result in tradeoffs of interpretation for predictive power.

# Deviance / Likelihood Measures

AIC and BIC approximate out-of-sample prediction error by applying penalty for model complexity.

-   AIC is a crude, large-sample approximation of leave-one-out cross-validation
-   BIC favors smaller models and penalizes model complexity more

For logistic regression, we also have "pseudo"-$R^2$ quantities. Higher values indicate a "better" model.

## Generalized / Nagelkerkge $R^2$

$$
R_G^2 = 1 - (\frac{L_0}{L_1})^{\frac{2}{n}}
$$

::: {.panel-tabset group="language"}
# R

```{r}
library(DescTools)

AIC(logit_model)
BIC(logit_model)
PseudoR2(logit_model, which = "Nagelkerke")
```

# Python

```{python}
print(log_model.aic)
print(log_model.bic_llf)
print(log_model.pseudo_rsquared())
```
:::

# Probability Metrics

Logistic regression was originally designed to rank-order probabilities. It *can* be used as a classification model as well.

You might want that insight into rank-ordering than you might think. An example is when customers are predicted to buy a product but in reality that are on the fence.

## Discrimination vs. Calibration

**Discrimination** is the ability to separate the events from the non-events. How good is a model at distinguishing the 1's from the 0's.

**Calibration** is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.

These two metrics may not agree with each other.

## Coefficient of Discrimination

**Coefficient of determination** is the difference in average predicted probability between 1's and 0's:

$$
D = \bar{\hat{p}}_1 - \bar{\hat{p}}_0
$$

This is a comparison metric to see which model can separate the 1's and 0's better.

::: {.panel-tabset group="language"}
# R

```{r}
train$p_hat <- predict(logit_model, type = "response")
p1 <- train$p_hat[train$Bonus == 1]
p0 <- train$p_hat[train$Bonus == 0]

coef_discrim <- mean(p1) - mean(p0)

ggplot(train, aes(p_hat, fill = factor(Bonus))) +
    geom_density(alpha = 0.7) +
    scale_fill_grey() +
    labs(x = "Predicted Probability", fill = "Outcome", title = paste("Coefficient of Discrimination = ", round(coef_discrim, 3), sep = ""))
```

# Python

```{python}
train["p_hat"] = log_model.predict()

p1 = train.loc[train["Bonus"] == 1, "p_hat"]
p0 = train.loc[train["Bonus"] == 0, "p_hat"]

coef_discrim = p1.mean() - p0.mean()
```
:::

## Rank-order Statistics

How well does a model order predictions? Recall concordance. For a pair of subjects with and without the event, the one **with the event** had the **higher** predicted probability.

Discordance is where for a pair of subjects with and without the event, the one **with the event** had the **lower** predicted probability.

### Concordance

Interpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event $Concordance\%$ of the time.

Common metrics based on concordance:

-   C-Statistic: $c = Concordance\% + \frac{1}{2}Tied\%$
-   Somer's D (Gini): $D_{xy} = 2c - 1$
-   Kendall's $\tau_{\alpha}: \tau_{\alpha} = \frac{\text{\#concordant} - \text{\#discordant}}{\frac{n(n - 1)}{2}}$

::: {.panel-tabset group="language"}
# R

```{r}
#| warning: false
library(Hmisc)
somers2(train$p_hat, train$Bonus)
```

Our model assigned the higher predicted probability to the observation with the bonus eligible home 94.3% of the time (c-statistic).

# Python
:::

# Assessing Predictive Power

We want our model to correctly classify events and non-events. **Classification** forces the model to predict either 1 or 0 based on whether the predicted probability exceeds some threshold.

Strict classification-based measures completely discard any information about the actual quality of the model's predicted probablities.

![Logistic Discrimination](images/logistic-discrimination.png)

![Classification Table](images/classification-table.png)

## Sensitivity vs. Specificity

Of all the actual 1's how many did you get right (**sensitivity**):

$$
TPR = \frac{TP}{TP + FN}
$$

Of all the actual 0's how many did you get right (**specificity**):

$$
TNR = \frac{TN}{TN + FP}
$$

When we raise the cutoff, we make our model more specific. When we lower the cutoff, we make our model more sensitive.

Always consider the cost of false positives and false negatives when doing classification. When **NOT** considering costs, there are different techniques to "optimize" cut-off.

### Youden J Statistic

$$
J = \text{Sensitivity} + \text{Specificity} - 1
$$

False positives and false negatives are weighed equally, so select cut-off that products highest Youden $J$ statistic.

::: {.panel-tabset group="language"}
# R

We can use the `ROCit` library to calculate different metrics between our predictions and actual values. We will calculate accuracy (`ACC`), sensitivity (`SENS`), and specificity (`SPEC`).

```{r}
library(ROCit)

logit_meas <- measureit(train$p_hat, train$Bonus, measure = c("ACC", "SENS", "SPEC"))

youden_table <- data.frame(Cutoff = logit_meas$Cutoff, Sens = logit_meas$SENS, Spec = logit_meas$SPEC)

youden_table %>% head(10)
```

We could calculate the Youden index for every cutoff, but we can also use the `rocit` function to calculate the optimal Youden index. The plot shown is the ROC curve which is covered next.

```{r}
logit_roc <- rocit(train$p_hat, train$Bonus)
logit_plot <- plot(logit_roc)

logit_plot$optimal
```

# Python

:::

### ROC Curve

![ROC Curve](images/roc-curve.png)

The ROC curve plots sensitivity (TPR) vs. 1 - specificity (FPR) for different cutoff thresholds. In essence, we are trying to **balance sensitivity and specificity**. The "best" ROC curve is one that reaches the upper left hand side as the model would have high levels of sensitivity and specificity. The worst ROC curve is the diagonal line as the model would be just as good randomly assigning events and non-events to our observations. 

The lower left hand side is predicting every observation is a 0. Specificity is high, but sensitivity is 0. The upper right hand side is predicting every observation is a 1. Sensitivity is high, but specificity is 0.

Area under the curve (AUC or AUROC) summarizes the overall quality of ROC curve. Mathematically, AUC is equivalent to the $c$-statistic. AUC curves can be a useful metric in *comparing* models, but they do not necessarily detail how good the model itself is.

$$
AUC = \%\text{Concordant} + \frac{1}{2}(\%\text{Tied})
$$

![AUC Calculation Visualized](images/auc-concordance-visual.png)

:::{.panel-tabset group="language"}
# R

Like with Youden's index, we can plot the AUC curve through the `rocit` function:

```{r}
logit_roc <- rocit(train$p_hat, train$Bonus)
plot(logit_roc)
```

We can get the calculated AUC through the summary:

```{r}
summary(logit_roc)
```

# Python
:::

### KS Statistic

The **KS statistic** is a popular metric in the finance and banking industry. The two-sample KS statistic determines whether there is a difference between two cumulative distribution functions. 

In binary classification, the KS statistic is the maximum distance between the distribution functions for the event and non-event groups. The point at which this max distance occurs is the optimal cut-off for the model. 

Remember that this is assuming that the cost for each observation is the same. You should ask the business what costs drive the cut-off decision rather than always select the mathematically optimal cut-off.

$$
D = \max_{depth}(TPR - FPR) = \max_{depth}(\text{Sensitivity} + \text{Specificity} - 1)
$$

Note that this is exactly the same as maximizing the Youden index!

::: {.panel-tabset group="language"}
# R

To plot the two cumulative distribution functions as well as the maximum distance between the two CDFs we can use `ksplot`:

```{r}
logit_ks <- ksplot(logit_roc)
logit_ks$"KS Cutoff"
```

:::

### KS-Statistic or Youden?

$D$ test statistic is used for model comparison. However, it is mathematically equivalent to Youden's J statistic. The point at which we have the maximum $D$ statistic is the optimal cutoff.

## Precision vs. Recall

In a precision-recall perspective, our focus is on the 1's.

**Recall** is the same as sensitivity:

$$
TPR = \frac{TP}{TP + FN}
$$

**Precision** is slightly different than specificity:

$$
PPV = \frac{TP}{TP + FP}
$$

## Best Cut-off?

Always consider the cost of false positives and false negatives when doing classification.

When not considering costs, many different techniques to "optimize" cutoff.

$$
F_1 = 2\left(\frac{PPV \cdot TPR}{PPV + TPR}\right)
$$

Precision and recall are weighted equally, so select cut-off that produces highest $F_1$ score.

:::{.panel-tabset group="language"}
# R

```{r}
logit_meas <- measureit(train$p_hat, train$Bonus, measure = c("PREC", "REC", "FSCR"))

fscore_table <- data.frame(Cutoff = logit_meas$Cutoff, FScore = logit_meas$FSCR)

fscore_table %>%
    arrange(desc(FScore)) %>%
    head(1)
```

# Python
:::

## Precision and Lift

Common calculation in marketing. Great for interpretation around validity of model ranking / classifying observations correctly.

$$
\text{Lift} = \frac{PPV}{\pi_1}
$$

-   $\pi_1$ is the proportion of 1's in your original population

The top **depth%** of your customers, based on predicted probability, you get **lift** times as many responses compared to targeting a random sample of **depth%** of your customers.

:::{.panel-tabset group="language"}
# R

```{r}
logit_lift <- gainstable(logit_roc)
logit_lift
```

Bucket 1 contains the top 10% of homes in terms of probability of being bonus eligible. Bucket 10 contains the 10% of homes with the lowest probabilities. The lift amount in the first row represents how much higher our model is able to predict bonus eligible homes than if you were to randomly select from 10% of the population. Response rate (RespRate) is the proportion of responses the model was able to correctly predict.

Cumulative lift (CLift) on the second row is saying our model is able to predict bonus eligible homes at a 2.32 higher rate than if you were to randomly select from 20% of the population.

CCapRate refers to how many 1's we captured within the top $k\%$ of the data. For example, bucket 1 captured 24% of the 1's in the data.
:::

## Accuracy and Error

Be careful selecting models based on accuracy and error. If your data has 10% events and 90% non-events then you can have a 90% accurate model by guessing non-events for every observation.

### Accuracy

$$
\text{Acc} = \frac{TP + TN}{TP + FP + TN + FN}
$$

### Error

$$
\text{Error} = \frac{FP + FN}{n}
$$

# Closing Thoughts On Classification

Classification is a decision that is outside of statistical modeling. Classification assumes cost for each individual is the same. Useful for groups, but be careful about single observation decisions.