---
title: Model Assessment
date: 09/07/2023
categories:
    -   modeling
---

# Setup {.unnumbered}

::: {.panel-tabset}
# R

```{r}
#| warning: false
library(AmesHousing)
library(tidyverse)
library(reticulate)

set.seed(123)

use_condaenv("msa")

ames <- make_ordinal_ames()
ames <- ames |>
    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))
train <- sample_frac(ames, 0.7)

logit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())
```

# Python

```{python}
from statsmodels.genmod.families import Binomial
from statsmodels.genmod.generalized_linear_model import GLM

ames = r.ames
train = r.train
log_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()
```
:::


# Comparing Models

Remember that statistical models are created for two different purposes: estimation and prediction.

Estimation and prediction may not necessarily agree and they can result in tradeoffs of interpretation for predictive power.

# Deviance / Likelihood Measures

AIC and BIC approximate out-of-sample prediction error by applying penalty for model complexity.

-   AIC is a crude, large-sample approximation of leave-one-out cross-validation
-   BIC favors smaller models and penalizes model complexity more

For logistic regression, we also have "pseudo"-$R^2$ quantities. Higher values indicate a "better" model.

## Generalized / Nagelkerkge $R^2$

$$
R_G^2 = 1 - (\frac{L_0}{L_1})^{\frac{2}{n}}
$$

::: {.panel-tabset}
# R

```{r}
library(DescTools)

AIC(logit_model)
BIC(logit_model)
PseudoR2(logit_model, which = "Nagelkerke")
```

# Python

```{python}
print(log_model.aic)
print(log_model.bic_llf)
print(log_model.pseudo_rsquared())
```
:::

# Assessing Predictive Power

Logistic regression was originally designed to rank-order probabilities. It *can* be used as a classification model as well.

You might want that insight into rank-ordering than you might think. An example is when customers are predicted to buy a product but in reality that are on the fence.

## Discrimination vs. Calibration

**Discrimination** is the ability to separate the events from the non-events. How good is a model at distinguishing the 1's from the 0's.

**Calibration** is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.

These two metrics may not agree with each other.

## Coefficient of Discrimination

**Coefficient of determination** is the difference in average predicted probability between 1's and 0's:

$$
D = \bar{\hat{p}}_1 - \bar{\hat{p}}_0
$$

This is a comparison metric to see which model can separate the 1's and 0's better.

::: {.panel-tabset}
# R

```{r}
train$p_hat <- predict(logit_model, type = "response")
p1 <- train$p_hat[train$Bonus == 1]
p0 <- train$p_hat[train$Bonus == 0]

coef_discrim <- mean(p1) - mean(p0)

ggplot(train, aes(p_hat, fill = factor(Bonus))) +
    geom_density(alpha = 0.7) +
    scale_fill_grey() +
    labs(x = "Predicted Probability", fill = "Outcome", title = paste("Coefficient of Discrimination = ", round(coef_discrim, 3), sep = ""))
```

# Python

```{python}
train["p_hat"] = log_model.predict()

p1 = train.loc[train["Bonus"] == 1, "p_hat"]
p0 = train.loc[train["Bonus"] == 0, "p_hat"]

coef_discrim = p1.mean() - p0.mean()
```
:::

# Rank-order Statistics

How well does a model order predictions? Recall concordance. For a pair of subjects with and without the event, the one **with the event** had the **higher** predicted probability.

Discordance is where for a pair of subjects with and without the event, the one **with the event** had the **lower** predicted probability.

## Concordance

Interpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event $Concordance\%$ of the time.

Common metrics based on concordance:

-   C-Statistic: $c = Concordance\% + \frac{1}{2}Tied\%$
-   Somer's D (Gini): $D_{xy} = 2c - 1$
-   Kendall's $\tau_\alpha$: $\tau_\alpha = \frac{\text{#concordant} - \text{#discordant}}{\frac{n(n - 1)}{2}}$

::: {.panel-tabset}
# R

```{r}
library(Hmisc)
somers2(train$p_hat, train$Bonus)
```

# Python
:::

# Assessing Predictive Power

We want our model to correctly classify events and non-events. **Classification** forces the model to predict either 1 or 0 based on whether the predicted probability exceeds some threshold.

Strict classification-based measures completely discard any information about the actual quality of the model's predicted probablities.

![Logistic Discrimination](images/logistic-discrimination.png)

![Classification Table](images/classification-table.png)

## Sensitivity vs. Specificit$$y

Of all the actual 1's how many did you get right (**sensitivity**):

$$
TPR = \frac{TP}{TP + FN}
$$

Of all the actual 0's how many did you get right (**specificity**):

$$
TNR = \frac{TN}{TN + FP}
$$

When we raise the cutoff, we make our model more specific. When we lower the cutoff, we make our model more sensitive.