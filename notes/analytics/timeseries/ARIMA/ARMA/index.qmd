---
title: "ARIMA Models: AR and MA Models"
date: 09/07/2023
---

# Setup {.unnumbered}

```{r}
#| warning: false
library(tidyverse)
library(tseries)
library(forecast)
Y <- read.csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/ar2.csv")
x <- read.csv("https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/MA2.csv")
```

# Notation

-   AR(p) where p refers to the number of autoregressive terms
-   MA(q) where q refers to the number of moving average terms
-   ARMA(p, q) where p refers to AR terms and q refers to MA terms
-   ARIMA(p, d, q) where p refers to AR terms, d refers to number of differences taken (d = 2 means we took a difference of the differences), q refers to MA terms

# Autoregressive Models

Forecast a series based solely on the past values of $Y_t$. We are trying to model the lags of our Y terms.

We are focusing on the most basic case where there is only one lag value of $Y_t$--the AR(1) model.

$$
\begin{align*}
Y_t &= \omega + \phi Y_{t-1} + e_t \\
Y_{t-1} &= \omega + \phi Y_{t-2} + e_{t-1} \\
Y_{t-2} &= \omega + \phi Y_{t-3} + e_{t-2}
\end{align*}
$$

## Correlation Functions for AR(1)

ACF decreases exponentially as the number of lags increases. However, the PACF has a significant spike at the first lag, followed by nothing after.

![AR(1)-ACF](images/AR(1)-ACF.png)

![AR(1)-PACF](images/AR(1)-PACF.png)

Overall, the effect of shocks last over a long period of time. However, the effect of shocks that happened long ago has little effect on the present IF the value for $|\phi| < 1$. Stationarity--the dependence of previous observations declines over time.

-   If $\phi = 1$, then we have a Random Walk and NOT AR model
-   If $\phi > 1$, then today depends on tomorrow which does not make sense

## AR(2) Model

A time series that is a linear function of 2 past values plus error is an AR process of order 2:

$$
Y_t = \omega + \phi_1Y_{t-1} + \phi_2Y_{t-2} + e_t
$$

AR(2) models have a pattern in PACF plots for when it comes to stationarity (2 spikes). The effect of shocks that happened long ago has little effect on the present if the value for $\left| \phi_1 + \phi_2 \right| < 1$

## AR(p) Model

Time series that is a linear function of $p$ past values plus error is called AR process or order $p$.

$$
Y_t = \omega + \phi_1Y_{t-1} + \phi_2Y_{t-2} + \cdots + \phi_pY_{t-p} + e_t
$$

If the model is just an AR model, then the PACF has significant spikes at hte lags up to $p$ lags, followed by nothing after.

::: {.panel-tabset group="language"}
# R

```{r}
Y_ts <- ts(Y)
Y_ARIMA <- Arima(Y_ts, order = c(2, 0, 0))

ggAcf(Y_ARIMA$residuals)
ggPacf(Y_ARIMA$residuals)
```

# Python

:::

The coefficients from the AR(2) model are not very interpretable. If you are reporting to a client you would just say that you fit an AR(2) model.

# Moving Average Models

Moving average models are modeling lags of the error terms--the past error values.

## MA(1)

$$
Y_t = \omega + e_t - \theta e_{t-1}
$$

This is true for all observations (each observation is dependent on the error from the previous observation). Therefore, in an MA(1) model, individual shocks only last for a short time.

In the MA model, we do not have the same restrictions as AR models (but do want them to be invertible).

## Correlation Functions for MA(1)

The ACF has a significant spike at the first lag, followed by nothing after.

PACF decreases exponentially as the number of lags increases. For $q$ > 1, however, these patterns do not imply as the model becomes more complicated.

## MA(q)

Time series that is a linear function of $q$ past errors is a moving average process of order $q$.

$$
Y_t = \omega + e_t - \theta_1e_{t-1} - \theta_2e_{t-2} - \cdots - \theta_qe_{t-q}
$$

## Correlation Functions for MA(q)

ACF has significant spikes at lags up to lag $q$, followed by nothing after.

::: {.panel-tabset group="language"}
# R

MA(2) Model

```{r}
ggAcf(x)
ggPacf(x)
```

```{r}
x_ts <- ts(x)
x_ARIMA <- Arima(x_ts, order = c(0, 0, 2))
summary(x_ARIMA)
ggAcf(x_ARIMA$residuals)
ggPacf(x_ARIMA$residuals)
```

# Python
:::

Note that we can compare the AIC/BIC between AR, MA, or ARIMA models but we cannot compare them to the metrics given by ESM models.

# AR and MA Model Notes

-   Any AR(p) model can be rewritten as an MA($\infty$).
-   If any MA(q) model is invertible, then this MA(q) model can be rewritten as an AR($\infty$).
-   Software should warn you if model is not invertible, if there is no convergence or any other issues... pay attention to the warnings you encounter.
-   Parameters can have different signs based on how it parameterizes equations.

# White Noise

White noise is essentially the independent errors that we are left iwth after modeling the signal.

White noise time series have errors that have the following characteristics:

-   Follow a Normal distribution.
-   Have a mean zero and positive, constant variance.
-   All observations are independent of each other.
-   Autocorrelation and partial autocorrelation functions have a value close to zero at every time point (except for lag of 0).

Goal of modeling time series to to be left with white noise residuals in the time series. If residuals have a "significant" dependence structure, then we can still continue modeling.

## Ljung-Box $\chi^2$ Test for White Noise

Ljung-Box can be applied to original data or to the residuals after fitting a model.

::: {.text-center}
$H_0:$ Series has no autocorrelation.

$H_a:$ One or more autocorrelations up to lag $m$ are not zero.
:::

Essentially, rejecting our null hypothesis means that more modeling should be done.

$$
\chi_m^2 = n(n + 2) \sum_{k=1}^{m} \frac{\beta_k^2}{n - k}
$$

::: {.panel-tabset group="language"}
# R

Looking at original data first: 

```{r}
index1 <- seq(1, 10)
white_lb <- rep(NA, 10)

for (i in 1:10) {
    white_lb[i] <- Box.test(Y, lag = i, type = "Ljung-Box", fitdf = 0)$p.value
}

white_dat <- data.frame(cbind(white_lb, index1))
colnames(white_dat) <- c("pvalues", "Lag")

ggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +
    geom_col() +
    labs(title = "Ljung-Box test p-values", x = "Lags", y = "p-values")
```

Looking at the fitted model:

```{r}
Y_ARIMA <- Arima(Y, order = c(2, 0, 0))
white_lb <- rep(NA, 10)

for (i in 3:10) {
    white_lb[i] <- Box.test(Y_ARIMA$residuals, lag = i, type = "Ljung-Box", fitdf = 2)$p.value
}
white_dat <- data.frame(cbind(white_lb[3:10], index1[3:10]))
colnames(white_dat) <- c("pvalues", "Lag")

ggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +
    geom_col() +
    labs(title = "Ljung-Box test when this is white noise", x = "Lags", y = "p-values")
```

Note that you have to start your tests on the next lag term. In this example we are using AR(2) so our tests start on lag 3.

# Python
:::