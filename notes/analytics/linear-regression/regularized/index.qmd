---
title: Model Building and Scoring for Prediction
date: 07/13/2023
date-modified: 07/23/2023
categories:
    -   modeling
---

# Model Building

Linear regression is a good initial approach to model building, but not the only form of regression.

Linear regression is the best **linear unbiased estimator**.

## Best Linear Unbiased Estimator

$$
\hat{\beta}_j \sim N(\beta_j, s_{\hat{\beta}_j})
$$

On average, coefficients from all samples are centered from the true coefficient. What does it mean to be *best*? If assumptions hold, $s_{\hat{\beta}_j}$ is the minimum variance of all the unbiased estimators.

Put another way: The spread of our guesses is as narrow as it can be.

What if biased estimators had smaller variance?

# Regularized Regression Overview

As the number of variables increases, more problems tend to arise:

-   Assumptions start to fail
-   Multicollinearity concerns
    -   Coefficients can vary widely
    -   Variations lead to overfitting
    -   Higher variance than desired

**Regularized regression** puts constraints on the estimated coefficients in our model and *shrink* these estimates to 0.

Coefficients are biased but potentially improve variance of the model.

Here we are giving up interpretability for better prediction power. We're not guaranteed to predict better, but we have the potential to do so.

![Bias-Variance Tradeoff Example](images/biased-regression.png)

With regularized regression, we move our model farther from the truth, but the precision of our guesses increases. We have to balance moving farther from the "truth" with precision.

There are three common types of regularized regression:

-   Ridge
-   LASSO
-   ElasticNet

## Penalties in Models

Recall OLS regression minimizes the sum of squared errors:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2) = \min(SSE)
$$

Regularized regression introduces a penalty term to the minimization:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \text{Penalty}) = \min(SSE + \text{Penalty})
$$

# Ridge Regression

Ridge regression introduces $L_2$ penalty term:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{p} \hat{\beta}_j^2) = \min(SSE + \lambda\sum_{j=1}^{p} \hat{\beta}_j^2)
$$

-   If $\lambda = 0$, then OLS
-   As $\lambda \rightarrow \infty$, coefficients shrink to 0 but cannot actually go to 0
    -   The only way to counteract $\lambda$ getting bigger is to make the coefficients smaller.

## R Code

In R, all regularized regression functions have to take in a `model.matrix` instead of the usual `lm`

```{r}
#| warning: false

library(tidyverse)
library(AmesHousing)
library(car)

set.seed(123)

ames <- make_ordinal_ames()

ames <- ames %>%
    mutate(id = row_number())

train <- ames %>%
    sample_frac(0.7)

test <- anti_join(ames, train, by = "id")
dim(train)
```

```{r}
train_reg <- train %>%
    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %>%
    replace(is.na(.), 0)

# Leave all continuous variables alone
# Dummy encode all categorical variables
# Still need to factor any numeric categoricals beforehand
# We delete the first column from the model matrix since we don't need the intercept column that model.matrix provides
train_x <- model.matrix(Sale_Price ~ ., data = train_reg)[, -1]
train_y <- train_reg$Sale_Price

test_reg <- test %>%
    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %>%
    replace(is.na(.), 0)

test_x <- model.matrix(Sale_Price ~ ., data = test_reg)[, -1]
test_y <- train_reg$Sale_Price
```

```{r}
library(glmnet)

# alpha controls the option as to which penalty to use
# alpha = 0 uses Ridge
ames_ridge <- glmnet(x = train_x, y = train_y, alpha = 0)
plot(ames_ridge, xvar = "lambda")
```

## Python Code
<!-- TODO: Add Python Code -->
::: callout-caution
TODO: Adding Python regularized regression implementations
:::
```{python}
train = r.train
test = r.test
```

# LASSO Regression

Least absolute shrinkage and selection operator regression introduces an $L_1$ penalty term to the minimization:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda\sum_{j=1}^{p} |\hat{\beta}_j|) = \min(SSE + \lambda\sum_{j=1}^{p} |\hat{\beta}_j|)
$$

-   If $\lambda = 0$, then OLS
-   As $\lambda \rightarrow 0$, coefficients shrink to 0 and can actually become 0

## Differences in Effects

LASSO can delete variables whereas Ridge can only get close to 0. Differences in effects are due to differences in penalty. The deletion of variables from LASSO can actually act as variable selection.

$$
\begin{align*}
\hat{\beta}_{OLS} &= (X^TX)^{-1}X^TY \\
\hat{\beta}_R &= (X^TX + \lambda I)^{-1}X^TY \\
\hat{\beta}_L &= (X^TX)^{-1}(X^TY - \lambda I)
\end{align*}
$$

If $\lambda = X^TY$, $\hat{\beta}_L$ can be 0

## R Code

```{r}
library(glmnet)

# alpha = 1 sets glmnet to use LASSO
ames_lasso <- glmnet(x = train_x, y = train_y, alpha = 1)
plot(ames_lasso, xvar = "lambda")
```

# ElasticNet Regression

Elastic net regression combines both penalty terms in the minimization:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda_1\sum_{j=1}^{p} |\hat{\beta}_j| + \lambda_2\sum_{j=1}^{p} \hat{\beta}_j^2)
$$

The `glmnet` function in R takes a slightly different approach:

$$
\min(\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 + \lambda[\alpha\sum_{j=1}^{p} |\hat{\beta}_j| + (1 - \alpha)\sum_{j=1}^{p}\hat{\beta}_j^2])
$$

-   Any value of `alpha` between 0 and 1 gives a combination of both penalties

# Optimizing Penalties

Need to select $\lambda$ for any of the regularized regression. Don't want to minimize variance to point of overfitting.

Note that none of the regularized regressions care about multicollinearity or model hierarchy. Since we are only looking at the coefficients, if multicollinearity is affecting the coefficients then regularized will most likely filter them out.

## Cross-Validation

Cross-validation is one approach to prevent overfitting when tuning a parameter.

-   Split training data into multiple pieces
-   Build model on majority of pieces
-   Evaluate on remaining piece
-   Repeat process with switching out pieces for building and evaluation

```{r}
# Gives us lambda.min and lambda.1se on the graph
# By default, k-fold = 10
ames_lasso_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 1)
plot(ames_lasso_cv)
```

In regularized regression our evaluation statistic is **mean-squared error**. With each fold, we train a model on the remaining trainning data and calculate the mean-squared error on the fold. We do this for $k$ folds and then calculate the average of all the mean-squared errors to get the final statistic for the model.

When tuning $\lambda$, we will do this across a range of different $\lambda$ values with average mean-squared error being compared throughout the range.

## Important Variables

```{r}
coef(ames_lasso, s = c(ames_lasso_cv$lambda.min, ames_lasso_cv$lambda.1se))
```

# Model Comparisons

When we get to scoring our model on the test set, **do not rerun the algorithm**. We do not want to fit our model to the test dataset. Only use test data to score equations obtained from the final model for comparing.

## Model Metrics

Root MSE:

$$
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

Mean Absolute Error:

$$
MAE = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

Mean Absolute Percentage Error:

$$
MAPE = 100 \cdot \frac{1}{n}\sum_{i=1}^{n} |\frac{y_i - \hat{y}_i}{y_i}
$$