---
title: Diagnostics and Subset Selection
date: 09/05/2023
---

# Setup

::: {.panel-tabset}
# R

```{r}
#| warning: false
library(AmesHousing)
library(tidyverse)
library(reticulate)

set.seed(123)

use_condaenv("msa")

ames <- make_ordinal_ames()
ames <- ames |>
    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))
train <- sample_frac(ames, 0.7)
```

# Python

```{python}
ames = r.ames
train = r.train
```
:::

# Subset Selection Methods

Just like with linear regression, we can use normal stepwise selection techniques (forward, backward, stepwise) to get different models.

::: {.panel-tabset}
# R

```{r}
full_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + factor(Half_Bath) + Lot_Area + factor(Central_Air) + Second_Flr_SF + TotRms_AbvGrd + First_Flr_SF, data = train, family = binomial())

empty_model <- glm(Bonus ~ 1, data = train, family = binomial())

step_model <- step(empty_model, scope = list(lower = formula(empty_model), upper = formula(full_model)), direction = "both")
```
:::

We don't use forward selection as much as it cannot remove variables from your model. Instead, we might favor backward selection or stepwise selection. When we decide on our main effects after data exploration, we can create interactions within our subset and use forward selection to see which interactions stay in our model.

![Interactions with Forward Selection](images/forward-selection-interactions.png)

# P-Value vs. AIC/BIC Metrics

AIC is not necessarily better than p-values when determining variable significance:

$$
AIC = -2\log(L) + 2p
$$

If a model is better with a lower AIC:

$$
-2\log(L_{p+1}) + 2(p + 1) < -2\log(L_p) + 2p
$$

AIC does not adjust for sample size as the significance level, $\alpha$, calculation is determined as:

$$
1 - P(\chi_1^2 > 2) = 0.1573
$$

This is a relatively high significance level and for large datasets does not seem like a good technique for selecting variables.

BIC instead adjusts for sample size:

$$
1 - P(\chi_1^2 > \log(n)) =\cdots
$$

![P-Value vs. BIC Selection](images/pvalue-bic.png)

# Diagnostics

Linear regression residuals have properties useful for model diagnostics. In a binary response model setting, we have residuals but they are not as intuitive.

-   Deviance residuals
-   Partial residuals
-   Pearson residuals
-   Etc.

## Deviance

The model is a summary of a data set. A **saturated** model fits the data perfectly but is not a useful summary. **Deviance** is a measure of how far the fitted model is from the saturated model--the error. Logistic regression minimizes the sum of squared deviances.

Deviance residuals tell us how much each observation reduces the deviance.

## Influence Statistics

-   Cook's D
    -   Measures the overall impact to the coefficients in the model
-   DFBETAS
    -   Measures standardized change in each parameter estimate with deletion of observation
-   DIFCHISQ
    -   Measures change in Pearson Chi-square with deletion of observation
-   DIFDEV
    -   Measures change in deviance with deletion of the observation

::: {.panel-tabset}
# R

```{r}
#| warning: false

library(car)

logit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())

# influence.measures(logit_model)
```

To plot Cook's D:

```{r}
plot(logit_model, 4)
```

To plot DFBETAS:

```{r}
dfbetasPlots(logit_model, terms = "Gr_Liv_Area", id.n = 5, col = ifelse(logit_model$y == 1, "red", "blue"))
```

# Python
:::

