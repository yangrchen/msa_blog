{
  "hash": "6935ceb5edc7400f06dc6954388d544e",
  "result": {
    "markdown": "---\ntitle: Tree-based Models\ndate: 11/06/2023\n---\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# | include: false\nimport pandas as pd\n\ntraining = r.training\ntesting = r.testing\n```\n:::\n\n\n# Decision Tree Review\n\nDecision trees are built by recursively splitting the data into successively purer subsets of data using measures of purity--Gini, entropy, misclassifcation error rate.\n\n![Selecting the Split](images/dt-review-splits.png){#fig-dt-review-splits}\n\n# Bagging\n\n**Bagging** refers to bootstrap aggregation. In order to understand bagging, we need to understanding bootstrapping.\n\n## Bootstrapping\n\nTake random samples of data *with replacement* that are the same size as the original dataset. Some observations will not be sampled which are referred to as **out-of-bag observations**.\n\n![Bootstrapping for 10 Observations](images/bootstrapping-example.png){#fig-bootstrapping-example}\n\nThe out-of-bag observations can be used as a validation set. However, there is no guarantee what the size of each validation set will be.\n\nIt's been proven that a bootstrap sample will contain approximately 63% of observations on average. Sample size is the same as original as some observations are repeated.\n\n## Bagging Process\n\n1.  Take $k$ bootstrap samples of size $n$ from the training data.\n2.  For each of the $k$ samples, create a model using that sample as training data.\n3.  Ensemble the $k$ different models.\n\nBagging aggregates the $k$ models for the full dataset and compares the ensembled values to the actual truth of the original dataset.\n\nThis process can be computationally expensive if we focused on building $k$ complex models. For trees, we will actually just stick to building simple decision tree models with only one split.\n\n## Bagging Example for Trees\n\n1.  Take 10 bootstrap samples.\n2.  Build a tree with only one split. \n3.  Aggregate these rules into a voting ensemble.\n4.  Test performance of the voting ensemble on the whole dataset.\n\n![Bagging Example](images/bagging-example.png){#fig-bagging-example}\n\n## Bagging Summary\n\n-   Improves generalization error on models with high variance\n-   If base classifier is stable (not high variance), bagging can actually make it worse\n-   Does not focus on any particular observations in the training data (unlike boosting)\n\n# Random Forests\n\n**Random forests** are ensembles of decision trees--ensembles work best when they find **different patterns in the data**.\n\nRandom forests also create **random subsets of variables for each split** and **unpruned decision trees** in each ensemble. The idea is that we give different a variables a chance to be a main split on the data. Results from trees are then ensembled together.\n\nFor a regression problem, we are taking the averages of the average predictions made from each model. The number of leaf nodes don't matter since we are averaging the *final predictions* from each model.\n\n## Parameters to Tune\n\n-   Number of trees\n-   Number of variables for each split\n-   Depth of tree (defaults to unpruned)\n\n## Implementing Random Forests\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nlibrary(randomForest)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nrandomForest 4.7-1.1\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nType rfNews() to see new features/changes/bug fixes.\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'randomForest'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    combine\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:ggplot2':\n\n    margin\n```\n:::\n\n```{.r .cell-code}\nrf_ames <- randomForest(\n    Sale_Price ~ ., data = training_df, \n    ntree = 500, # <1>\n    importance = TRUE) # <2>\n```\n:::\n\n1.  `ntree` is the number of trees\n2.  `importance` is a flag to allow for variable importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(rf_ames, main = \"Number of Trees Compared to MSE\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/MSE-plot-1.png){width=672}\n:::\n:::\n\n\nTo get variable importance we can use `varImpPlot`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarImpPlot(rf_ames, sort = TRUE, n.var = 10, main = \"Top 10 Variable Importance\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/var-importance-1.png){width=672}\n:::\n:::\n\n\n-   The x-axis represents how much more MSE your model would gain if you left out this variable\n-   To calculate the MSE increase, the variable is randomly permuted to \"remove\" relationship with target\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.ensemble import RandomForestRegressor\n\ntrain_dummy = pd.get_dummies(training, columns=[\"Street\", \"Central_Air\"])\ny_train = train_dummy[\"Sale_Price\"]\nX_train = train_dummy.loc[:, train_dummy.columns != \"Sale_Price\"]\n\nrf_ames = RandomForestRegressor(n_estimators=100, random_state=12345, oob_score=True)\n\nrf_ames.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(oob_score=True, random_state=12345)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(oob_score=True, random_state=12345)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\nWe can plot the average decrease in impurity in the nodes of the tree:\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nforest_importances = pd.Series(\n    rf_ames.feature_importances_, index=rf_ames.feature_names_in_\n)\n\nfig, ax = plt.subplots()\nforest_importances.plot.bar(ax=ax)\nax.set_title(\"Feature Importances\")\nax.set_ylabel(\"Mean decrease in impurity\")\nfig.tight_layout()\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/py-var-importance-1.png){width=672}\n:::\n:::\n\n\n:::\n\n## Tuning Random Forests\n\nThe number of variables considered for each split is called `mtry` in `caret`. By default, `mtry` is $\\sqrt{p}$, with $p$ being number of variables.\n\nWe use validation (out-of-bag samples) to tune along with number of trees. We are measuring the total amount of error across all the samples we did not take.\n\nFor each `mtry`, the same bootstrap samples are used to give each attempt a fair shot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\ntuneRF(x = training_df[, -1], y = training_df[, 1], plot = TRUE, ntreeTry = 500, stepFactor = 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nmtry = 4  OOB error = 855384447 \nSearching left ...\nmtry = 8 \tOOB error = 890709806 \n-0.04129764 0.05 \nSearching right ...\nmtry = 2 \tOOB error = 908679993 \n-0.06230596 0.05 \n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/tuning-rf-3.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n  mtry  OOBError\n2    2 908679993\n4    4 855384447\n8    8 890709806\n```\n:::\n:::\n\n\n## Variable Selection\n\nRandom forests use all the variables since they are averaged across all the trees used to build the model. Variable selection can be performed by a variety of methods.\n\n-   Many permutations of including / excluding variables\n-   Compare variables to random variable (newer way)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntraining_df <- training_df %>% mutate(random = rnorm(2051))\n\nset.seed(12345)\nrf_ames <- randomForest(Sale_Price ~ ., data = training_df, ntree = 500, mtry = 4, importance = TRUE)\n\nvarImpPlot(rf_ames, sort = TRUE, n.var = 15, main = \"Look for Variables Below Random Variable\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/random-variable-selection-1.png){width=672}\n:::\n:::\n\n\nThe idea is that any variables that are below the completely random variable do not perform as well overall.\n\n## Summary\n\nIf you are building a random forest model for a client, you want to focus on the predictions, results, and *maybe* the variable importance. Beyond that, the interpretability may be too complex.\n\n**Advantages:**\n\n-   Computationally fast\n-   Trees trained simultaneously\n-   Accurate classification model\n-   Variable importance\n-   Missing data is OK \n\n**Disadvantages:**\n\n-   No \"interpretability\" other than variable importance\n-   Tuning parameters\n\n# Interpretability\n\nMost machine learning models are not interpretable in the classical sense. The relationships modeled are **not linear** so the interpretations are much more complicated than a typical regression. \n\nSimilar to GAMs however, we can get a general idea of overall pattern for a predictor variable compared to a target using **partial dependence plots**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npartialPlot(rf_ames, training_df, Year_Built)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/partial-dependence-1.png){width=672}\n:::\n\n```{.r .cell-code}\npartialPlot(rf_ames, training_df, Garage_Area)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/partial-dependence-2.png){width=672}\n:::\n:::\n\n\n# Boosting\n\n**Boosting** is similar to bagging in that we are still taking bootstrapped samples from the dataset. However, unlike bagging, observations are **not sampled randomly**. Boosting assigns weight to each trianing observation and uses the weight as a sampling distribution. In particular, we assign higher weight to the observations that are **harder to classify**.\n\n![Probability of Selection Observation](images/boosting-probability.png){#fig-boosting-probability}\n\nBoosting is trying to learn from its mistakes by selecting the observations that it gets wrong more often. Bagging will build the ensemble model **simultaneously**, but since each model in boosting informs the next we have to build models **sequentially**.\n\nFor our previous tree sample our process is:\n\n1.  Build a tree with only one split\n2.  Start with equal weights for each observation\n3.  Update weights each round based on the classification error\n\n![Three Boosting Rounds](images/three-boosting-rounds.png){#fig-three-boosting-rounds}\n\n# AdaBoost\n\nBoosted ensembles weight the votes of each classifier by a function of their accuracy. If the classifier get the higher weighted observations wrong, it has a higher error rate.\n\nPut simply, more accurate guesses are more important.\n\n## Classifier Weights\n\nObservations are each weighted based on how well they were predicted in the previous round.\n\nLet the weights for each round be denoted as $\\omega_i$ and the predictions for round be $\\hat{y}_{1,i}, \\hat{y}_{2,i}$.\n\nThe prediction for each observation is derived from a classification as follows:\n\n$$\n\\hat{y}_i = \\omega_1 \\hat{y}_{1,i} + \\omega_2 \\hat{y}_{2,i} + \\cdots\n$$\n\n# Gradient Boosting\n\nIdea behind gradient boosting to use simple model to predict the target. With the next model, we are trying to predict the initial model's error, $\\epsilon_1$. Let's say the first variable predicted most of the signal, then by predicting each error we are giving the other variables a chance to predict the signal where the first variable went wrong.\n\nThe idea of sequential building on the errors is why we consider this *boosting*. However, it's different from the previous idea of boosting where we were applying weight classifiers to observations.\n\n$$\ny = f_1(x) + f_2(x) + f_3(x) + \\cdots + f_k(x) + \\epsilon_k\n$$\n\n-   Each function $f_i(x)$ is trying to predict the previous error $\\epsilon_{i-1}$\n\n## Overfitting Protection\n\nGradient boosting regularizes with tunable parameters to prevent overfitting:\n\n1.  Reduce weight of each of the error models for prediction.\n\n$$\ny = f_1(x) + \\eta f_2(x) + \\eta f_3(x) + \\cdots + \\eta f_k(x) + \\epsilon_k\n$$\n\n-   Smaller values of $\\eta$ lead to less overfitting\n\n2.  Number of trees used in the prediction where larger number of trees increases overfitting\n3.  Other parameters introduced over the years...\n\n## Gradient Boosted Trees\n\nGradient boosting yields an **additive ensemble model**. There is no averaging of individual models. Predictions from each model are summed together for final prediction.\n\nThe key to gradient boosting is using weak learners (shallow trees). Although each learner would make poor predictions on their own, their addition provides good predictions.\n\n![Weak Learner Ensemble](images/weak-learner-ensemble.png){#fig-weak-learner-ensemble}\n\n# Gradient Descent\n\nModels are optimized to some form of **loss function**. For example, linear regression and decision trees typically look at minimizing SSE. The SSE represents the overall loss of the model. To find the model with the lowest loss function, we can use **gradient descent**.\n\nGradient descent iteratively updates parameters in order to minimize the loss function by moving int he direction of \"steepest descent\".\n\n![Gradient Descent](images/gradient-descent.png){#fig-gradient-descent}\n\nStep size is updated at each step by multiplying the gradient by a **learning rate**. Without a learning rate, wem igth take steps too big or too small (too long to optimize).\n\n## Stochastic Gradient Descent\n\nNot all loss functions are convex and some have local minima or plateaus that make finding the global minimum difficult.\n\n**Stochastic gradient descent** attempts to solve this by randomly sampling a fraction of the training observations for each tree in the ensemble. This makes the algorithm faster and more reliable, but may not always find the true overall minimum.\n\n## Training a Gradient Boosted Machine\n\nGrid search is very time consuming because of the time it takes to build these models. We can tune parameters one at a time:\n\n1.  Start with relatively high learning rate (default of 0.1 is typically good).\n2.  Determine optimal number of trees for this learning rate.\n3.  Fix tree tuning parameters (number of trees, depth, etc.) and tune learning rate.\n4.  Set learning rate again at this new value and retune tree parameters.\n5.  Try lowering learning rate again to see any improvements.\n\n# XGBoost Introduction\n\n**Extreme gradient boosting** has different advantages over traditional GBM:\n\n1.  Additional regularization parameters to prevent overfitting.\n2.  Settings to stop model assessment when adding more trees isn't helpful.\n3.  Supports parallel processing, but still must be trained sequentially.\n4.  Variety of loss functions.\n5.  Allows generalized linear models as well as tree-based models (all still weak learners).\n6.  Implemented in R, Python, Julia, Scala, Java, C++, etc.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# library(xgboost)\n\n# train_x <- model.matrix(Sale_Price ~ ., data = training)[, -1]\n# train_y <- training$Sale_Price\n\n# set.seed(12345)\n# xgb_ames <- xgboost(data = train_x, label = train_y, subsample = 0.5, nrounds = 100)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom xgboost import XGBRegressor\n\nxgb_ames = XGBRegressor(n_estimators=50, subsample=0.5, random_state=12345)\nxgb_ames.fit(X_train, y_train)\n```\n\n::: {.cell-output-display}\n```{=html}\n<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=50, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=12345, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=None, callbacks=None,\n             colsample_bylevel=None, colsample_bynode=None,\n             colsample_bytree=None, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None, feature_types=None,\n             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n             interaction_constraints=None, learning_rate=None, max_bin=None,\n             max_cat_threshold=None, max_cat_to_onehot=None,\n             max_delta_step=None, max_depth=None, max_leaves=None,\n             min_child_weight=None, missing=nan, monotone_constraints=None,\n             n_estimators=50, n_jobs=None, num_parallel_tree=None,\n             predictor=None, random_state=12345, ...)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    \"n_estimators\": list(range(5, 51, 5)), # <1>\n    \"eta\": [0.1, 0.15, 0.2, 0.25, 0.3], # <2>\n    \"max_depth\": list(range(1, 11)), # <3>\n    \"subsample\": [0.25, 0.5, 0.75, 1] # <4>\n}\n\nxgb = XGBRegressor()\n\ngrid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv = 10)\n# grid_search.fit(X_train, y_train)\n```\n:::\n\n1.  `n_estimators` is the number of trees we will use\n2.  `eta` is the regularization parameter\n3.  `max_depth` is the maximum depth of our trees\n4.  `subsample` is the fraction of our data that we use in stochastic gradient descent\n\n:::\n\n## Variable Importance in XGBoost\n\nXGBoost provides 3 built-in measures of variable importance:\n\n1.  **Gain**: Equivalent metric in random forests\n2.  **Coverage**: Measures relative number of observations influenced by the variable\n3.  **Frequency**: Percentage of splits in the whole ensemble that use this variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set.seed(12345)\n# xgb_ames <- xgboost(data = train_x, label = train_y, subsample = 1, nrounds = 24, eta = 0.25, max_depth = 5)\n# xgb.importance(features_names = colnames(train_x), model = xgb_ames)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# xgb.ggplot.importance(xgb.importance(feature_names = colnames(train_x), model = xgb_ames))\n```\n:::\n\n\nOne of the advantages of XGBoost is that it tries to cluster variables in terms of their importance. There is some notion of statistically different importances between variables.\n\n## Variable Selection\n\nXGBoost uses all variables since they are averages across all the trees used to build the model.\n\nVariable selection can be performed by permutations of including and excluding variables. However, this is extremely time confusing.\n\nSimilarly to the random forest models, we can compare variables to a random variable and potentially exclude variables that end up less important than the random variable.\n\n# Gradient Boosting Summary\n\n**Advantages:**\n\n-   Very accurate\n-   Tend to outperform random forests when properly trained and tuned\n-   Variable importance provided\n\n**Disadvantages:**\n\n-   Lacks \"interpretability\" beyond variable importance\n-   Computationally slower than random forests\n-   More tuning parameters than random forest\n-   Harder to optimize\n-   More sensitive to tuning parameters",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}