{
  "hash": "930fc9cdebb9a9f915b9ab5b4702669f",
  "result": {
    "markdown": "---\ntitle: Generalized Additive Models\ndate: 10/31/2023\n---\n\n\n\n\n# General Structure\n\nGAMs provide a general framework for adding non-linear functions together instead of the typical linear structure.\n\n$$\ny = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p) + \\epsilon\n$$\n\n# Piecewise Linear Regression\n\nWhat if you had a linear relationship between $x$ and $y$, but the slope changes? Different pieces might have a straight-line relationship, but a typical single straight-line model will not be a good fit for this type of data.\n\n![Changing Slopes](images/changing-slopes.png){#fig-changing-slopes}\n\nA model where different straight-line relationships for different intervals in the predictor variable is called the **piecewise linear regression model**. For two slopes, the model follows as:\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2(x_1 - k)x_2 + \\epsilon\n$$\n\n-   $k$ is the knot value for $x_1$\n\nHow do we split up the pieces? $x_2$ depends on how the value of $x_1$ compares to the knot value:\n\n$$\nx_2 = \\begin{cases}\n    1 & x_1 > k \\\\\n    0 & x_1 \\leq k\n\\end{cases}\n$$\n\n![Cement Example](images/cement-example.png){#fig-cement-example}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncement <- read_csv(\"data/cement.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 18 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): BATCH, STRENGTH, RATIO, X2, X2STAR\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncement_lm <- lm(STRENGTH ~ RATIO + X2STAR, data = cement)\nggplot(cement, aes(x = RATIO, y = STRENGTH)) +\n    geom_point() +\n    geom_line(data = cement, aes(x = RATIO, y = cement_lm$fitted.values)) +\n    ylim(0, 6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Extensions - Discontinuous\n\nThe previous approach had piecewise functions that are continuous. The following is the discontinuous version for two straight lines:\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2(x_1 - k)x_2 + \\beta_3x_2 + \\epsilon\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncement_lm <- lm(STRENGTH ~ RATIO + X2STAR + X2, data = cement)\nsummary(cement_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = STRENGTH ~ RATIO + X2STAR + X2, data = cement)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53167 -0.15513  0.06171  0.17239  0.49451 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.04975    0.68558  10.283  6.6e-08 ***\nRATIO       -0.05240    0.01174  -4.463 0.000536 ***\nX2STAR      -0.07888    0.02686  -2.937 0.010830 *  \nX2          -0.60388    0.26877  -2.247 0.041302 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2916 on 14 degrees of freedom\nMultiple R-squared:  0.9548,\tAdjusted R-squared:  0.9451 \nF-statistic: 98.57 on 3 and 14 DF,  p-value: 1.188e-09\n```\n:::\n:::\n\n\nFor $k$ straight lines, we can produce $k - 1$ knot values to model our data.\n\n![Extension - Knots](images/extension-knots.png){#fig-extension-knots}\n\n# MARS (Multivariate Adaptive Regression Splines)\n\nMARS is a generalization of piecewise linear regression. It is a non-parametric regression method that uses a series of nonlinearities and interactions between variables in a additive form. Essentially, uses **piecewise** regression to split into pieces then potentially uses either linear or nonlinear patterns for each piece.\n\nMARS looks for a point in the range of $x$ where two linear functions on either side of the point provides the least squared error.\n\n![Different Knot Values](images/many-knots.png){#fig-many-knots}\n\nAlgorithm continnues on each piece of piecewise function until many knots are found--this will overfit your data. MARS uses a **pruning** algorithm to remove knots that do not improve the model. The way it does this is by comparing the model without the knot to the model with the knot and selects the model which does better on **generalized cross-validation**. This is repeated for each variable we provide to the model.\n\nFor open-source software, the implementation is **EARTH** (Enhanced Adaptive Regression Through Hinges).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(earth)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Formula\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: plotmo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: plotrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: TeachingDemos\n```\n:::\n\n```{.r .cell-code}\nmars1 <- earth(Sale_Price ~ Garage_Area, data = training)\nsummary(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall: earth(formula=Sale_Price~Garage_Area, data=training)\n\n                    coefficients\n(Intercept)           124159.039\nh(286-Garage_Area)       -60.257\nh(Garage_Area-286)       297.277\nh(Garage_Area-521)      -483.642\nh(Garage_Area-576)       733.859\nh(Garage_Area-758)      -356.460\nh(Garage_Area-1043)     -490.873\n\nSelected 7 of 7 terms, and 1 of 1 predictors\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: Garage_Area\nNumber of terms at each degree of interaction: 1 6 (additive model)\nGCV 3427475346    RSS 6.94092e+12    GRSq 0.4492014    RSq 0.4556309\n```\n:::\n:::\n\n\nEach value in the EARTH output represents a knot / hinge value. For example, h(Garage_Area - 286) is the knot value when Garage_Area is 286. A new line is created at this point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(training, aes(x = Garage_Area, y = Sale_Price)) +\n    geom_point() +\n    geom_line(data = training, aes(x = Garage_Area, y = mars1$fitted.values), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nCreating a model on the full data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmars1 <- earth(Sale_Price ~ ., data = training)\nsummary(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall: earth(formula=Sale_Price~., data=training)\n\n                      coefficients\n(Intercept)              319493.46\nCentral_AirY              20289.49\nh(4-Bedroom_AbvGr)         9214.66\nh(Bedroom_AbvGr-4)       -23009.05\nh(Year_Built-1977)         1275.57\nh(2004-Year_Built)         -336.64\nh(Year_Built-2004)         5315.57\nh(13869-Lot_Area)            -2.09\nh(Lot_Area-13869)             0.22\nh(First_Flr_SF-1600)        104.91\nh(2402-First_Flr_SF)        -71.56\nh(First_Flr_SF-2402)       -176.61\nh(1523-Second_Flr_SF)       -53.13\nh(Second_Flr_SF-1523)       426.63\nh(Half_Bath-1)           -45378.31\nh(2-Fireplaces)          -14408.56\nh(Fireplaces-2)          -26072.58\nh(Garage_Area-539)          101.97\nh(Garage_Area-1043)        -294.30\nh(Gr_Liv_Area-2049)          65.21\nh(Gr_Liv_Area-3194)        -159.79\n\nSelected 21 of 24 terms, and 10 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Garage_Area, ...\nNumber of terms at each degree of interaction: 1 20 (additive model)\nGCV 1033819964    RSS 2.036439e+12    GRSq 0.8338641    RSq 0.8402842\n```\n:::\n:::\n\n\nNotice that our output shows how many predictors were used in the model. We also have a list of variable importance. What is going on in the variable importance?\n\n## Variable Importance\n\nThere is one \"subset\" for each model size (1 term, 2 terms, etc.)--the best model of that size. The variable importance is the number of times that variable was used in the best model of that size. The more subsets of models the variable appears in the better the variable.\n\n**Residual sum of squares** is a scaled version of decrease in residual sum of squares relative to the previous subset. GCV is an approximation of RSS on leave-one-out cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevimp(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              nsubsets   gcv    rss\nFirst_Flr_SF        20 100.0  100.0\nSecond_Flr_SF       19  71.7   71.9\nYear_Built          18  50.9   51.3\nGarage_Area         17  34.3   35.0\nFireplaces          16  31.0   31.7\nGr_Liv_Area         15  27.6   28.4\nCentral_AirY        12  20.0   20.9\nBedroom_AbvGr       11  18.1   19.0\nLot_Area            10  16.2   17.2\nHalf_Bath            4   7.4    8.2\n```\n:::\n:::\n\n\nEach variable following the first variable is the amount of improvement in the residual sum of squares relative to the amount of improvement in the first variable. \n\nBe careful, we have no notion of how these variables affect the response just which variables we think are the most important in the model. We trade interpretability for predictive power.\n\n# Smoothing\n\nSmoothing is a way to reduce the variance of our model. In GAMs, we can use **smoothing functions** to approximate a curve representing the signal of our data. One common example is **LOESS (locally estimated scatterplot smoothing)**.\n\n## LOESS\n\nLOESS is a non-parametric regression method that estimates the latent signal in a point-wise fashion. Essnetially, for each point we calculate a local weighted regression of a fixed window size around the point and record the point's prediction along the local regression. However, more weight is assigned to points closer to the point of interest while less weight is assigned to points further away. \n\nLarge window sizes result in higher bias (more points to consider) while lower values will increase variance (less points to consider). The window size is a hyperparameter that we can tune.\n\n![LOESS Visualized](images/loess-visual.png){#fig-loess-visual}\n\n## Smoothing Splines\n\n**Smoothing splines** have a knot at **every single observation** for piecewise regression. We use a penalty parameter to counterbalance the overfitting.\n\nSmoothing splines are optimizing the following equation:\n\n$$\n\\min \\sum_{i=1}^{n} (y_i - s(x_i))^2 + \\lambda \\int s''(t_i)^2 dt\n$$\n\n-   The first sum is the residual sum of squares\n-   The second term is the penalty applied to integral of second derivative of smoothing function\n-   The penalty $\\lambda$ is estimated with an approximation of leave one out cross validation\n\nThe idea behind the second derivative in the penalty term is that it gives us an idea of how fast our slopes (first derivatives) are changing. The larger the second derivative, the faster the slope is changing. We want to penalize large changes in slope to lower the variance of our model.\n\n## Regression Splines\n\nEssentially **regression splines** are a computationally nicer version of smoothing splines.\n\n# Implementing Smoothing\n\n:::{.panel-tabset group=\"language\"}\n# R \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: nlme\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'nlme'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    collapse\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThis is mgcv 1.9-0. For overview type 'help(\"mgcv-package\")'.\n```\n:::\n\n```{.r .cell-code}\ngam1 <- gam(Sale_Price ~ s(Garage_Area), data = training)\nsummary(gam1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Garage_Area)\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   180897       1290   140.2   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df     F p-value    \ns(Garage_Area) 8.134  8.769 192.5  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.451   Deviance explained = 45.3%\nGCV = 3.4301e+09  Scale est. = 3.4148e+09  n = 2051\n```\n:::\n:::\n\n\nWe see a section for coefficients not involved in splines and a section for smoothing terms. The p-value for the spline of `Garage_Area` shows the significance of the variable to the model **as a whole.** These p-values are essentially results from likelihood ratio tests. We can plot the relationship using the `plot` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(gam1, se = TRUE, col = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nWe can also apply this to multiple variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngam2 <- gam(\n    Sale_Price ~ s(Bedroom_AbvGr, k = 5) +\n        s(Year_Built) +\n        s(Mo_Sold) +\n        s(Lot_Area) +\n        s(First_Flr_SF) +\n        s(Second_Flr_SF) +\n        s(Garage_Area) +\n        s(Gr_Liv_Area) +\n        s(TotRms_AbvGrd) +\n        Street +\n        Central_Air +\n        factor(Fireplaces) +\n        factor(Full_Bath) +\n        factor(Half_Bath),\n    method = \"REML\", data = training\n)\nsummary(gam2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Bedroom_AbvGr, k = 5) + s(Year_Built) + s(Mo_Sold) + \n    s(Lot_Area) + s(First_Flr_SF) + s(Second_Flr_SF) + s(Garage_Area) + \n    s(Gr_Liv_Area) + s(TotRms_AbvGrd) + Street + Central_Air + \n    factor(Fireplaces) + factor(Full_Bath) + factor(Half_Bath)\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           136139      19682   6.917 6.20e-12 ***\nStreetPave             27690      12710   2.179   0.0295 *  \nCentral_AirY           18012       3168   5.685 1.50e-08 ***\nfactor(Fireplaces)1    14069       1666   8.443  < 2e-16 ***\nfactor(Fireplaces)2    27137       3146   8.626  < 2e-16 ***\nfactor(Fireplaces)3    15705      10552   1.488   0.1368    \nfactor(Fireplaces)4   -79591      31469  -2.529   0.0115 *  \nfactor(Full_Bath)1     -5341      14528  -0.368   0.7132    \nfactor(Full_Bath)2    -11074      14828  -0.747   0.4552    \nfactor(Full_Bath)3      1225      15787   0.078   0.9382    \nfactor(Full_Bath)4    -16268      24327  -0.669   0.5038    \nfactor(Half_Bath)1      2102       2206   0.953   0.3408    \nfactor(Half_Bath)2    -38508       9111  -4.226 2.48e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                   edf Ref.df       F p-value    \ns(Bedroom_AbvGr) 2.653  3.166  18.787  <2e-16 ***\ns(Year_Built)    6.445  7.543 101.759  <2e-16 ***\ns(Mo_Sold)       1.521  1.875   0.990  0.4500    \ns(Lot_Area)      7.186  8.193  11.726  <2e-16 ***\ns(First_Flr_SF)  8.063  8.765  15.548  <2e-16 ***\ns(Second_Flr_SF) 8.212  8.818   7.806  <2e-16 ***\ns(Garage_Area)   7.426  8.328  21.654  <2e-16 ***\ns(Gr_Liv_Area)   8.546  8.882  14.834  <2e-16 ***\ns(TotRms_AbvGrd) 3.805  4.739   1.921  0.0783 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.851   Deviance explained = 85.6%\n-REML =  23957  Scale est. = 9.2391e+08  n = 2051\n```\n:::\n:::\n\n\nThere are some variables with high p-values that can be removed. The `gam` function has a `select` option which will penalize variable edf values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsel_gam2 <- gam(\n    Sale_Price ~ s(Bedroom_AbvGr, k = 5) +\n        s(Year_Built) +\n        s(Mo_Sold) +\n        s(Lot_Area) +\n        s(First_Flr_SF) +\n        s(Second_Flr_SF) +\n        s(Garage_Area) +\n        s(Gr_Liv_Area) +\n        s(TotRms_AbvGrd) +\n        Street +\n        Central_Air +\n        factor(Fireplaces) +\n        factor(Full_Bath) +\n        factor(Half_Bath),\n    method = \"REML\",\n    select = TRUE,\n    data = training\n)\nsummary(sel_gam2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: gaussian \nLink function: identity \n\nFormula:\nSale_Price ~ s(Bedroom_AbvGr, k = 5) + s(Year_Built) + s(Mo_Sold) + \n    s(Lot_Area) + s(First_Flr_SF) + s(Second_Flr_SF) + s(Garage_Area) + \n    s(Gr_Liv_Area) + s(TotRms_AbvGrd) + Street + Central_Air + \n    factor(Fireplaces) + factor(Full_Bath) + factor(Half_Bath)\n\nParametric coefficients:\n                    Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           139417      19644   7.097 1.76e-12 ***\nStreetPave             27516      12778   2.153   0.0314 *  \nCentral_AirY           17909       3172   5.645 1.89e-08 ***\nfactor(Fireplaces)1    13741       1671   8.223 3.55e-16 ***\nfactor(Fireplaces)2    26998       3170   8.517  < 2e-16 ***\nfactor(Fireplaces)3    17180      10612   1.619   0.1056    \nfactor(Fireplaces)4   -77383      31523  -2.455   0.0142 *  \nfactor(Full_Bath)1     -8354      14483  -0.577   0.5642    \nfactor(Full_Bath)2    -13973      14759  -0.947   0.3439    \nfactor(Full_Bath)3     -1749      15710  -0.111   0.9114    \nfactor(Full_Bath)4    -31031      22964  -1.351   0.1768    \nfactor(Half_Bath)1      2376       2200   1.080   0.2802    \nfactor(Half_Bath)2    -38160       9166  -4.163 3.27e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                      edf Ref.df      F  p-value    \ns(Bedroom_AbvGr) 2.505713      4 15.697  < 2e-16 ***\ns(Year_Built)    6.400950      9 87.241  < 2e-16 ***\ns(Mo_Sold)       0.002762      9  0.000   0.4536    \ns(Lot_Area)      7.242803      9 10.938  < 2e-16 ***\ns(First_Flr_SF)  8.471172      9 19.763  < 2e-16 ***\ns(Second_Flr_SF) 0.949500      9  2.072 8.44e-06 ***\ns(Garage_Area)   6.782102      9 19.327  < 2e-16 ***\ns(Gr_Liv_Area)   8.736843      9 14.613  < 2e-16 ***\ns(TotRms_AbvGrd) 3.248559      9  0.988   0.0215 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.849   Deviance explained = 85.3%\n-REML =  24068  Scale est. = 9.4133e+08  n = 2051\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n# Summary\n\nAdvantages:\n\n-   Allows for non-linear relationships without trying out many transformations\n-   Improved predictions\n-   Still has some \"interpretation\"\n-   Computationally fast\n\nDisadvantages:\n\n-   Can incorporate interactions but can take time\n-   Not good for large numbers of variables\n-   Multicollinearity is still a problem",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}