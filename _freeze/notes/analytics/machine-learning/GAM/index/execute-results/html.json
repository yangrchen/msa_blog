{
  "hash": "26d7da40b986277686f6cb47bed92222",
  "result": {
    "markdown": "---\ntitle: Generalized Additive Models\ndate: 10/31/2023\n---\n\n\n\n\n# General Structure\n\nGAMs provide a general framework for adding non-linear functions together instead of the typical linear structure.\n\n$$\ny = \\beta_0 + f_1(x_1) + f_2(x_2) + \\cdots + f_p(x_p) + \\epsilon\n$$\n\n# Piecewise Linear Regression\n\nWhat if you had a linear relationship between $x$ and $y$, but the slope changes? Different pieces might have a straight-line relationship, but a typical single straight-line model will not be a good fit for this type of data.\n\n![Changing Slopes](images/changing-slopes.png){#fig-changing-slopes}\n\nA model where different straight-line relationships for different intervals in the predictor variable is called the **piecewise linear regression model**. For two slopes, the model follows as:\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2(x_1 - k)x_2 + \\epsilon\n$$\n\n-   $k$ is the knot value for $x_1$\n\nHow do we split up the pieces? $x_2$ depends on how the value of $x_1$ compares to the knot value:\n\n$$\nx_2 = \\begin{cases}\n    1 & x_1 > k \\\\\n    0 & x_1 \\leq k\n\\end{cases}\n$$\n\n![Cement Example](images/cement-example.png){#fig-cement-example}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncement <- read_csv(\"data/cement.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 18 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (5): BATCH, STRENGTH, RATIO, X2, X2STAR\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ncement_lm <- lm(STRENGTH ~ RATIO + X2STAR, data = cement)\nggplot(cement, aes(x = RATIO, y = STRENGTH)) +\n    geom_point() +\n    geom_line(data = cement, aes(x = RATIO, y = cement_lm$fitted.values)) +\n    ylim(0, 6)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Extensions - Discontinuous\n\nThe previous approach had piecewise functions that are continuous. The following is the discontinuous version for two straight lines:\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2(x_1 - k)x_2 + \\beta_3x_2 + \\epsilon\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncement_lm <- lm(STRENGTH ~ RATIO + X2STAR + X2, data = cement)\nsummary(cement_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = STRENGTH ~ RATIO + X2STAR + X2, data = cement)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.53167 -0.15513  0.06171  0.17239  0.49451 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  7.04975    0.68558  10.283  6.6e-08 ***\nRATIO       -0.05240    0.01174  -4.463 0.000536 ***\nX2STAR      -0.07888    0.02686  -2.937 0.010830 *  \nX2          -0.60388    0.26877  -2.247 0.041302 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2916 on 14 degrees of freedom\nMultiple R-squared:  0.9548,\tAdjusted R-squared:  0.9451 \nF-statistic: 98.57 on 3 and 14 DF,  p-value: 1.188e-09\n```\n:::\n:::\n\n\nFor $k$ straight lines, we can produce $k - 1$ knot values to model our data.\n\n![Extension - Knots](images/extension-knots.png){#fig-extension-knots}\n\n# MARS (Multivariate Adaptive Regression Splines)\n\nMARS is a generalization of piecewise linear regression. It is a non-parametric regression method that uses a series of nonlinearities and interactions between variables in a additive form. Essentially, uses **piecewise** regression to split into pieces then potentially uses either linear or nonlinear patterns for each piece.\n\nMARS looks for a point in the range of $x$ where two linear functions on either side of the point provides the least squared error.\n\n![Different Knot Values](images/many-knots.png){#fig-many-knots}\n\nAlgorithm continnues on each piece of piecewise function until many knots are found--this will overfit your data. MARS uses a **pruning** algorithm to remove knots that do not improve the model.\n\nThe pruning is calculated using generalized cross-validation which is a computational shortcut for leave-one-out cross-validation.\n\nFor open-source software, the implementation is **EARTH** (Enhanced Adaptive Regression Through Hinges).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(earth)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Formula\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: plotmo\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: plotrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: TeachingDemos\n```\n:::\n\n```{.r .cell-code}\nmars1 <- earth(Sale_Price ~ Garage_Area, data = training)\nsummary(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall: earth(formula=Sale_Price~Garage_Area, data=training)\n\n                    coefficients\n(Intercept)           124159.039\nh(286-Garage_Area)       -60.257\nh(Garage_Area-286)       297.277\nh(Garage_Area-521)      -483.642\nh(Garage_Area-576)       733.859\nh(Garage_Area-758)      -356.460\nh(Garage_Area-1043)     -490.873\n\nSelected 7 of 7 terms, and 1 of 1 predictors\nTermination condition: RSq changed by less than 0.001 at 7 terms\nImportance: Garage_Area\nNumber of terms at each degree of interaction: 1 6 (additive model)\nGCV 3427475346    RSS 6.94092e+12    GRSq 0.4492014    RSq 0.4556309\n```\n:::\n:::\n\n\nEach value in the EARTH output represents a knot / hinge value. For example, h(Garage_Area - 286) is the knot value when Garage_Area is 286. A new line is created at this point.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(training, aes(x = Garage_Area, y = Sale_Price)) +\n    geom_point() +\n    geom_line(data = training, aes(x = Garage_Area, y = mars1$fitted.values), color = \"blue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nCreating a model on the full data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmars1 <- earth(Sale_Price ~ ., data = training)\nsummary(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall: earth(formula=Sale_Price~., data=training)\n\n                      coefficients\n(Intercept)              319493.46\nCentral_AirY              20289.49\nh(4-Bedroom_AbvGr)         9214.66\nh(Bedroom_AbvGr-4)       -23009.05\nh(Year_Built-1977)         1275.57\nh(2004-Year_Built)         -336.64\nh(Year_Built-2004)         5315.57\nh(13869-Lot_Area)            -2.09\nh(Lot_Area-13869)             0.22\nh(First_Flr_SF-1600)        104.91\nh(2402-First_Flr_SF)        -71.56\nh(First_Flr_SF-2402)       -176.61\nh(1523-Second_Flr_SF)       -53.13\nh(Second_Flr_SF-1523)       426.63\nh(Half_Bath-1)           -45378.31\nh(2-Fireplaces)          -14408.56\nh(Fireplaces-2)          -26072.58\nh(Garage_Area-539)          101.97\nh(Garage_Area-1043)        -294.30\nh(Gr_Liv_Area-2049)          65.21\nh(Gr_Liv_Area-3194)        -159.79\n\nSelected 21 of 24 terms, and 10 of 14 predictors\nTermination condition: Reached nk 29\nImportance: First_Flr_SF, Second_Flr_SF, Year_Built, Garage_Area, ...\nNumber of terms at each degree of interaction: 1 20 (additive model)\nGCV 1033819964    RSS 2.036439e+12    GRSq 0.8338641    RSq 0.8402842\n```\n:::\n:::\n\n\nNotice that our output shows how many predictors were used in the model. We also have a list of variable importance. What is going on in the variable importance?\n\n## Variable Importance\n\nThere is one \"subset\" for each model size (1 term, 2 terms, etc.)--the best model of that size. The variable importance is the number of times that variable was used in the best model of that size. The more subsets of models the variable appears in the better the variable.\n\n**Residual sum of squares** is a scaled version of decrease in residual sum of squares relative to the previous subset. GCV is an approximation of RSS on leave-one-out cross validation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nevimp(mars1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              nsubsets   gcv    rss\nFirst_Flr_SF        20 100.0  100.0\nSecond_Flr_SF       19  71.7   71.9\nYear_Built          18  50.9   51.3\nGarage_Area         17  34.3   35.0\nFireplaces          16  31.0   31.7\nGr_Liv_Area         15  27.6   28.4\nCentral_AirY        12  20.0   20.9\nBedroom_AbvGr       11  18.1   19.0\nLot_Area            10  16.2   17.2\nHalf_Bath            4   7.4    8.2\n```\n:::\n:::\n\n\nEach variable following the first variable is the amount of improvement in the residual sum of squares relative to the amount of improvement in the first variable. \n\nBe careful, we have no notion of how these variables affect the response just which variables we think are the most important in the model. We trade interpretability for predictive power.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}