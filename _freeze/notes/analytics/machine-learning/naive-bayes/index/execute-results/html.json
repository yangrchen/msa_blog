{
  "hash": "18c4211354203c61c17f7e5999e1ea69",
  "result": {
    "markdown": "---\ntitle: Naive Bayes Model\ndate: 11/14/2023\n---\n\n\n\n\nWhen we need to classify observations there are two difference sources of evidence:\n\n-   Similarity to other observations based on certain metrics / variables\n-   Past decisions on classifications of observations like it\n\nThe second approach is a Bayesian approach.\n\nIn @fig-priors, we are establishing what our prior probabilities are based on current observations.\n\n![Naive Bayes Priors](images/priors.png){#fig-priors}\n\nIf we introduce a new observation and take the predefined closest number of observations, then we can get conditional probabilities along with our knowledge of prior probabilities.\n\n![Naive Bayes Conditionals](images/conditionals.png){#fig-conditionals}\n\nWe then multiply the conditionals with the priors and scale the probabilities to sum up to 1. The higher probability wins.\n\n![Naive Bayes Results](images/results.png){#fig-results}\n\n# Assumption\n\nOne of the big assumptions of naive Bayes classification is that **predictor variables are independent in their effects on the classification**.\n\n# Underlying Math\n\n**Posterior probabilities** are the predicted probability of success given values of variables for this observation. The **prior probability** is the probability that an observation has those variable values.\n\nBayesian classifiers are based on Bayes' Theorem. Naive Bayes assumes that the effect of the inputs are independent of one another:\n\nRecall Bayes' Theorem:\n\n$$\nP(y_i | x_1, x_2, \\cdots, x_p) = \\frac{P(y_i)P(x_1, x_2, \\cdots, x_p | y_i)}{P(x_1, x_2, \\cdots, x_p)}\n$$\n\nIf events are independent like Naive Bayes assumes then:\n\n$$\nP(A \\cap B) = P(A) \\cdot P(B)\n$$\n\n$$\nP(A \\cap B|C) = P(A|C) \\cdot P(B|C)\n$$\n\n![Naive Bayes Example](images/medium-blue-example.png){#fig-example}\n\nIf certain values don't occur for all levels of the outcome, then the probability will zero out. We use a Laplace correction to make sure that we can still estimate probabilities.\n\n![Laplace Correction](images/laplace-correction.png){#fig-laplace}\n\n# Naive Bayes Implementation\n\nThe inputs to the Naive Bayes are the same between a classification target or a continuous target. However, the outputs change. We **don't** want to use Naive Bayes for a continuous target.\n\nInputs:\n\n-   **Categorical variables:** Determine probability based on cross-tabulation of each variable with target variable\n-   **Numerical variables:** Determine probability based on either values from a Normal distribution with same mean and std. dev as data or KDE of the data\n\nOutputs:\n\n-   **Classification target:** Probability that each observation belongs to each category of target variable\n-   **Continuous target:** Value of the target variable that is the highest probability. Treats the continuous target as a large number of categories. However, we can't predict any continuous targets outside the range of our data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\nlibrary(e1071)\n\nnb_ames <- naiveBayes(Sale_Price ~ ., data = training, laplace = 0, usekernel = TRUE)\n```\n:::\n\n\n# Summary\n\n**Advantages:**\n\n-   Simple to implement\n-   Good at predictions\n-   Perform best with categorical variables / text\n-   Fast computational time\n-   Robust to noise and irrelevant variables\n\n**Disadvantages:**\n\n-   Independence assumption\n-   Careful about normality assumption for continuous variables\n-   Requires more memory storage than most variables\n-   Trust predicted categories more than probabilities",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}