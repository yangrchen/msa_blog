{
  "hash": "cd33426b696e7883721cad90f182001a",
  "result": {
    "markdown": "---\ntitle: Neural Networks\ndate: 11/13/2023\n---\n\n\n\n\n# Structure\n\nNeural networks are organized as a network of neurons through layers. Input variables are considered neurons on the **bottom layer**. The output variable is considered the neuron on the **head layer**. The layers in between are **hidden layers** which transform the input variables through non-linear activation functions to try and best model the output variable.\n\nA neural network is a linear transformation of nonlinear transformations of our inputs and weights spread out across different layers.\n\n![Neural Net Structure](images/neural-net-structure.png){#fig-nnet-structure}\n\n![Activation Functions](images/activation-functions.png){#fig-activation-functions}\n\n# Backpropagation\n\nThere are two main phases: a forward and backward phase.\n\nIn the forward phase:\n\n1.  Start with randomly initialized weights\n2.  Calculations are passed forward through the network\n3.  Output predicted value computed\n\nIn the backward phase:\n\n1.  Predicted value compared with actual value to compute error\n2.  Work backwards through the network to adjust weights to make the prediction better \n\nWe want to repeat this process until we have some notion of convergence.\n\n# Implementing Neural Nets in R\n\n## Standardization\n\nNeural nets work best when input data **are scaled**. For bell-shaped data, statistical z-scores standardization can work. For severely assymmetric data, midrange standardization works better:\n\n$$\n\\frac{x - \\text{midrange}(x)}{0.5 \\cdot \\text{range}(x)} = \\frac{x - \\frac{(\\max(x) + \\min(x))}{2}}{0.5 \\cdot (\\max(x) - \\min(x))}\n$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncols_to_scale <- c(\"Sale_Price\", \"Bedroom_AbvGr\", \"Year_Built\", \"Mo_Sold\", \"Lot_Area\", \"First_Flr_SF\", \"Second_Flr_SF\", \"Garage_Area\", \"Gr_Liv_Area\", \"TotRms_AbvGrd\")\ntraining <- training %>%\n    mutate(Full_Bath = as.factor(Full_Bath), Half_Bath = as.factor(Half_Bath), Fireplaces = as.factor(Fireplaces))\n\nscaled_training <- training %>%\n    mutate(across(all_of(cols_to_scale), scale))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nnet)\nset.seed(12345)\n\nnn_ames <- nnet(Sale_Price ~ ., data = training, size = 5, linout = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# weights:  116\ninitial  value 79865952890167.062500 \nfinal  value 12750392495886.974609 \nconverged\n```\n:::\n:::\n\n\n## Optimize Number of Hidden Nodes and Decay\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'caret'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\ntune_grid <- expand.grid(\n    .size = c(3, 4, 5, 6, 7),\n    .decay = c(0, 0.5, 1)\n)\n\nset.seed(12345)\n\nnn_ames_caret <- train(Sale_Price ~ ., data = training, method = \"nnet\", tuneGrid = tune_grid, trControl = trainControl(method = \"cv\", number = 10), trace = FALSE, linout = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n```\n:::\n\n```{.r .cell-code}\nnn_ames_caret$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  size decay\n2    3   0.5\n```\n:::\n:::\n\n\n# Variable Selection\n\nNeural networks typically do not care about variable selection. All variables are used in a complicated and mixed way. If you want to do variable selectinon, you can examine weights for each variable, but this is not a clear selection technique.\n\n# Summary\n\n**Advantages:**\n\n-   Usd for categorical / numerical target variables\n-   Capable of modeling complex nonlinear patterns\n-   No assumptions about the data distributions\n\n**Disadvantages:**\n\n-   No insights for variable importance\n-   Extremely computationally intensive\n-   Tuning of parameters\n-   Prone to overfitting training data",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}