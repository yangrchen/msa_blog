{
  "hash": "53b0f7dc7b4cf94ae0e5aa34572c278e",
  "result": {
    "markdown": "---\ntitle: Resampling, Model Selection, and Regularization\ndate: 10/30/2023\n---\n\n\n# Resampling Revisited\n\nRecall that before we do cross-validation, we need to have split off a test set from our overall data. To do cross-validation, we take our training data and split it into $k$ equally-sized groups. For each set, we train the model on the other $k - 1$ sets and validate the model using the selected set. We then average the validation error across all $k$ sets to get our cross-validation error.\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames %>% mutate(id = row_number())\n\nset.seed(4321)\n\n# Split 70% for training, 30% for testing\ntraining <- ames %>% sample_frac(0.7)\ntesting <- anti_join(ames, training, by = \"id\")\n\n# Select a subset of data based on previous data exploration\ntraining <- training %>% select(Sale_Price, Bedroom_AbvGr, Year_Built, Mo_Sold, Lot_Area, Street, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\n\ntraining = r.training\ntesting = r.testing\n```\n:::\n\n:::\n\n# Model Selection\n\nLinear models contain many different models. We should always start by narrowing a list of reasonable predictor variables through exploratory analysis. For explanation and inference, we can use forward, backward, and stepwise selection.\n\nInstead of evaluating on the training set, we evaluate on the validation set for each step.\n\n![Stepwise Selection through Cross-Validation](images/stepwise-selection-cv.png){#fig-stepwise-selection-cv}\n\nIn @fig-stepwise-selection-cv, each numbered row represents training the model at that stage on each of the $k$ folds. The validation error is then averaged across all $k$ folds. The model with the lowest validation error is selected. Since our focus is on prediction, we don't care as much about the typical assumptions of variables. We want to focus on the relationship between the response and the explanatory variable.\n\nAt step 0, for each variable we train 10 models and average their errors. We then have to take the next variable then train another 10 models and average their errors. After evaluating every univariate model, we select the variable with the lowest average error.\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n```\n:::\n\n```{.r .cell-code}\nset.seed(9876)\n\n# nvmax controls what the max number of variables we want to consider in our models\nstep_model <- train(Sale_Price ~ ., data = training, method = \"leapBackward\", tuneGrid = data.frame(nvmax = 1:14), trControl = trainControl(method = \"cv\", number = 10))\nstep_model$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  nvmax\n6     6\n```\n:::\n:::\n\n\nIf we were to see `step_model$results` then each row shows the **average** metric for each metric calculated for the **best model** selected. For example, if we were to use RMSE, then each row would show the average RMSE across all 10 folds.\n\nTo see the actual final model with the best tuned parameters:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(step_model$finalModel)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSubset selection object\n14 Variables  (and intercept)\n              Forced in Forced out\nBedroom_AbvGr     FALSE      FALSE\nYear_Built        FALSE      FALSE\nMo_Sold           FALSE      FALSE\nLot_Area          FALSE      FALSE\nStreetPave        FALSE      FALSE\nCentral_AirY      FALSE      FALSE\nFirst_Flr_SF      FALSE      FALSE\nSecond_Flr_SF     FALSE      FALSE\nFull_Bath         FALSE      FALSE\nHalf_Bath         FALSE      FALSE\nFireplaces        FALSE      FALSE\nGarage_Area       FALSE      FALSE\nGr_Liv_Area       FALSE      FALSE\nTotRms_AbvGrd     FALSE      FALSE\n1 subsets of each size up to 6\nSelection Algorithm: backward\n         Bedroom_AbvGr Year_Built Mo_Sold Lot_Area StreetPave Central_AirY\n1  ( 1 ) \" \"           \" \"        \" \"     \" \"      \" \"        \" \"         \n2  ( 1 ) \" \"           \" \"        \" \"     \" \"      \" \"        \" \"         \n3  ( 1 ) \" \"           \"*\"        \" \"     \" \"      \" \"        \" \"         \n4  ( 1 ) \" \"           \"*\"        \" \"     \" \"      \" \"        \" \"         \n5  ( 1 ) \"*\"           \"*\"        \" \"     \" \"      \" \"        \" \"         \n6  ( 1 ) \"*\"           \"*\"        \" \"     \" \"      \" \"        \" \"         \n         First_Flr_SF Second_Flr_SF Full_Bath Half_Bath Fireplaces Garage_Area\n1  ( 1 ) \"*\"          \" \"           \" \"       \" \"       \" \"        \" \"        \n2  ( 1 ) \"*\"          \"*\"           \" \"       \" \"       \" \"        \" \"        \n3  ( 1 ) \"*\"          \"*\"           \" \"       \" \"       \" \"        \" \"        \n4  ( 1 ) \"*\"          \"*\"           \" \"       \" \"       \" \"        \"*\"        \n5  ( 1 ) \"*\"          \"*\"           \" \"       \" \"       \" \"        \"*\"        \n6  ( 1 ) \"*\"          \"*\"           \" \"       \" \"       \"*\"        \"*\"        \n         Gr_Liv_Area TotRms_AbvGrd\n1  ( 1 ) \" \"         \" \"          \n2  ( 1 ) \" \"         \" \"          \n3  ( 1 ) \" \"         \" \"          \n4  ( 1 ) \" \"         \" \"          \n5  ( 1 ) \" \"         \" \"          \n6  ( 1 ) \" \"         \" \"          \n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n:::\n\n:::\n\n# Two Views of Parameter Tuning\n\n+----------------+----------------------------------------------------------------------------------------------------------------+\n| View           | Points                                                                                                         |\n+================+================================================================================================================+\n| Classical View | -   Use validation to evaluate which model is \"best\" at each step of the procedure                             |\n|                |                                                                                                                |\n|                | -   Final model contains variables remaining at end of procedure                                               |\n|                |                                                                                                                |\n|                | -   Combine training and validation and update parameter estimates on the chosen variables                     |\n+----------------+----------------------------------------------------------------------------------------------------------------+\n| \"Modern\" View  | -   Use validation to evaluate which model is \"best\" at each step of the procedure                             |\n|                |                                                                                                                |\n|                | -   Final model contains same number of variables as model at end of procedure                                 |\n|                |                                                                                                                |\n|                | -   Combine training and validation but do not restrict yourself to any variable, just the number of variables |\n+----------------+----------------------------------------------------------------------------------------------------------------+\n\nIn the classical view, we can take those final variables and retrain the model on the entire training set. In the modern view, we drop cross-validation and see what optimal model for the selected `nvmax` is on the entire training set. The modern view assumes that there is no globally optimal variable even as data changes, but there is an optimal number of variables to use--the best variables can change as data changes.\n\nWhen it comes to retraining the model with new data, the modern view does not need to retune the best number of parameters unless the data *fundamentally* changes. Changes can include new variables or how we view our data to begin with.\n\nClassical view: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_model1 <- glm(Sale_Price ~ First_Flr_SF + Second_Flr_SF + Year_Built + Garage_Area + Bedroom_AbvGr + Fireplaces, data = training)\nsummary(final_model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Sale_Price ~ First_Flr_SF + Second_Flr_SF + Year_Built + \n    Garage_Area + Bedroom_AbvGr + Fireplaces, data = training)\n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.407e+06  6.439e+04 -21.852  < 2e-16 ***\nFirst_Flr_SF   1.128e+02  3.236e+00  34.871  < 2e-16 ***\nSecond_Flr_SF  8.252e+01  2.812e+00  29.342  < 2e-16 ***\nYear_Built     7.256e+02  3.306e+01  21.945  < 2e-16 ***\nGarage_Area    6.012e+01  5.366e+00  11.203  < 2e-16 ***\nBedroom_AbvGr -1.265e+04  1.317e+03  -9.607  < 2e-16 ***\nFireplaces     1.113e+04  1.555e+03   7.157 1.14e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1561167099)\n\n    Null deviance: 1.275e+13  on 2050  degrees of freedom\nResidual deviance: 3.191e+12  on 2044  degrees of freedom\nAIC: 49246\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nModern view:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nempty_model <- glm(Sale_Price ~ 1, data = training)\nfull_model <- glm(Sale_Price ~ ., data = training)\n\nfinal_model2 <- step(empty_model, scope = list(lower = formula(empty_model), upper = formula(full_model)), direction = \"both\", steps = 6, trace = 0)\nsummary(final_model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Sale_Price ~ Gr_Liv_Area + Year_Built + First_Flr_SF + \n    Garage_Area + Bedroom_AbvGr + Fireplaces, data = training)\n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.441e+06  6.451e+04 -22.343  < 2e-16 ***\nGr_Liv_Area    8.116e+01  2.790e+00  29.086  < 2e-16 ***\nYear_Built     7.433e+02  3.313e+01  22.438  < 2e-16 ***\nFirst_Flr_SF   3.053e+01  2.944e+00  10.370  < 2e-16 ***\nGarage_Area    6.110e+01  5.373e+00  11.372  < 2e-16 ***\nBedroom_AbvGr -1.258e+04  1.322e+03  -9.518  < 2e-16 ***\nFireplaces     1.138e+04  1.558e+03   7.305 3.95e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 1569252272)\n\n    Null deviance: 1.2750e+13  on 2050  degrees of freedom\nResidual deviance: 3.2076e+12  on 2044  degrees of freedom\nAIC: 49257\n\nNumber of Fisher Scoring iterations: 2\n```\n:::\n:::\n\n\nWe have a different set of 6 variables under the modern view.\n\n# Regularization\n\n**Regularization** is a tool toc ontrol the complexity/flexibility of a model. We are adding a penalty term to penalize model complexity. Model is biased, but potentially improves variance.\n\n![Bias-Variance Tradeoff](images/bias-variance-tradeoff.png){#fig-bias-variance-tradeoff}\n\nRegularized regression puts constraints on the estimates coefficients in our model and **shrink** these estimates to 0. We have three common methods:\n\n-   LASSO\n-   Ridge\n-   ElasticNet\n\n## Penalties in Models\n\nWe introduce a penalty term into our loss function to penalize coefficients.\n\n$$\n\\min\\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\text{Penalty} \\right) = \\min(\\text{SSE} + \\text{Penalty})\n$$\n\n## Ridge Regression\n\nRidge regression introduces an L2 penalty term to the minimization:\n\n$$\n\\min\\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda\\sum_{i=1}^{p} \\hat{\\beta}_j^2 \\right)\n$$\n\nIf $\\lambda = 0$, then OLS. As $\\lambda \\longrightarrow \\infty$, coefficients shrink to 0 but cannot actually become 0.\n\n## LASSO Regression\n\nLeast aboslute shrinkage and selection operator regression introduces an L1 penalty term to the minimization:\n\n$$\n\\min\\left( \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda\\sum_{j=1}^{n}|\\hat{\\beta}_j|  \\right)\n$$\n\nIf $\\lambda = 0$, then OLS. As $\\lambda \\longrightarrow \\infty$, coefficients shrink to 0 and can become 0.\n\n## ElasticNet\n\nElasticNet combines both Ridge and LASSO based on a new $\\alpha$ parameter. Any calue of $\\alpha$ between 0 and 1 gives a combination of both penalties. We can use cross-validation to tune the $\\alpha$ parameter.\n\n:::{.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\n\nen_model <- train(Sale_Price ~ ., data = training, method = \"glmnet\", tuneGrid = expand.grid(.alpha = seq(0, 1, by = 0.05), .lambda = seq(100, 60000, by = 1000)), trControl = trainControl(method = \"cv\", number = 10))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,\n: There were missing values in resampled performance measures.\n```\n:::\n\n```{.r .cell-code}\nen_model$bestTune\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    alpha lambda\n601   0.5    100\n```\n:::\n:::\n\n\nOur best $\\alpha$ is 0.5 and our best $\\lambda$  is 100.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-7\n```\n:::\n\n```{.r .cell-code}\ntrain_x <- model.matrix(Sale_Price ~ ., data = training)[, -1]\ntrain_y <- training$Sale_Price\n\n# Build with the optimal alpha level\names_en <- glmnet(x = train_x, y = train_y, alpha = 0.5)\nplot(ames_en, xvar = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/glmnet-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n:::\n\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(5)\names_en_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 0.5)\n\nplot(ames_en_cv)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\names_en_cv$lambda.min\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 115.4119\n```\n:::\n\n```{.r .cell-code}\names_en_cv$lambda.1se\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 13269.57\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}