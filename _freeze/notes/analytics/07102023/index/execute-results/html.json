{
  "hash": "81a8cd329c5b4f251d39705588fd26df",
  "result": {
    "markdown": "---\ntitle: Diagnostics\ndate: 07/10/2023\n---\n\n\n# Examining Residuals\n\nRecall the assumptions of linear regression:\n\n-   Mean of the Ys is accurately modeled by linear function of the Xs\n-   $\\varepsilon$ is assumed to be Normal with a mean of 0\n-   $\\varepsilon$ is assumed to have constant variance $\\sigma^2$\n-   Errors are independent\n-   **No perfect collinearity**\n\nWe can investigate many of our assumptions through \nresiduals in residuals vs. fitted values plots.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(httpgd)\nlibrary(reticulate)\n\nuse_condaenv(condaenv = \"blues_clues\", required = TRUE)\n# hgd(port = 8888)\ndata_path <- \"data/Salary.csv\"\nsalary <- read.csv(data_path)\npar(mfrow = c(2, 2))\nsalary_lm <- lm(Salary ~ YearsExperience, data = salary)\n\nggplot(salary_lm, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\nsalary = r.salary\nsalary\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    YearsExperience  Salary\n0               1.1   39343\n1               1.3   46205\n2               1.5   37731\n3               2.0   43525\n4               2.2   39891\n5               2.9   56642\n6               3.0   60150\n7               3.2   54445\n8               3.2   64445\n9               3.7   57189\n10              3.9   63218\n11              4.0   55794\n12              4.0   56957\n13              4.1   57081\n14              4.5   61111\n15              4.9   67938\n16              5.1   66029\n17              5.3   83088\n18              5.9   81363\n19              6.0   93940\n20              6.8   91738\n21              7.1   98273\n22              7.9  101302\n23              8.2  113812\n24              8.7  109431\n25              9.0  105582\n26              9.5  116969\n27              9.6  112635\n28             10.3  122391\n29             10.5  121872\n30             11.2  127345\n31             11.5  126756\n32             12.3  128765\n33             12.9  135675\n34             13.5  139465\n```\n:::\n\n```{.python .cell-code}\n# sns.residplot(data=salary, x=\"YearsExperience\", y=\"Salary\", order=2)\n```\n:::\n\n\n\n# Misspecified Model\n\nWe detect a misspecified model when a pattern is detected \nin the residuals. The model form is incorrect for the data.\n\nWe can potentially resolve this by including polynomial \nterms, interactions, splines, etc.\n\n# Model Hierarchy\n\nWhen adding higher order terms (power terms and/or \ninteractions) you should have **all** lower terms included \nin the model.\n\nIf $x^3$ is in the model, you should have $x$ and $x^2$ in \nthe model as well. If you include an interaction $x_1x_2$ \nin the model, then $x_1$ and $x_2$ should be included.\n\n# Polynomial Regression\n\nPatterns in residual plots of our variable may give us an \nindication to try higher order terms.\n\nIn a polynomial regression, if the higher order term is \nused then we lose the interpretation for that entire variable. \n\nIf we do model selection with higher order terms and the \nhigher orders end up in the final model then we have to \nmake sure to add the lower terms back in to the model.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary_quad <- lm(Salary ~ YearsExperience + I(YearsExperience^2), data = salary)\nsummary(salary_quad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Salary ~ YearsExperience + I(YearsExperience^2), \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9210.7 -4037.8  -467.7  3485.5 11052.0 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          21855.58    3630.80   6.019 1.03e-06 ***\nYearsExperience      11456.37    1217.28   9.411 9.79e-11 ***\nI(YearsExperience^2)  -193.90      84.45  -2.296   0.0284 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5733 on 32 degrees of freedom\nMultiple R-squared:  0.9701,\tAdjusted R-squared:  0.9682 \nF-statistic:   519 on 2 and 32 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nggplot(salary_quad, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n-   In R we use the `I()` function to create a higher \norder term\n\nWhen a straight line is inappropriate, we can consider:\n\n-   Fit a polynomial/more complex regression\n-   Transform the dependent and/or independent\nvariables to obtain linearity\n-   Fit a nonlinear regression model if appropriate\n-   Fit a nonparameetric regression model (e.g. LOESS)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}