{
  "hash": "5e2187776f5c4a10c6bb9c18bfad2cb0",
  "result": {
    "markdown": "---\ntitle: Multiple Linear Regression\ndate: 07/06/2023\ndate-modified: 07/23/2023\n---\n\n\nModels with more than one predictor variable are called **multiple regression models**.\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\varepsilon\n$$\n\n-   Estimate $\\hat{\\beta}_j$ is the predicted average change in $y$ with a one unit in crease in $x_j$ given all other variables are held constant.\n\nWe are still trying to minimize the sum of squared errors:\n\n$$\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nLinear in MLR refers to the linear combination of variables in the model--not how they're visualized in multiple dimensions.\n\nA model like $y = \\beta_0 + \\beta_1x_1 + \\beta_1x_1^2$ is still a linear regression!\n\n# Global F-Test\n\nIn SLR, we could use a single t-test to determine model utility. In MLR, we have the Global F-Test to determine if an overall model is useful for predicting $y$ overall.\n\n::: text-center\n$H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0$\n\n$H_a:$ At least one variable is useful in predicting the target\n:::\n\nThe F-statistic is calculated as:\n\n$$\nF = \\frac{(\\frac{SSR}{k})}{(\\frac{SSE}{n - k - 1})}\n$$\n\n-   $\\frac{SSR}{k}$ is the average amount of variation each variable explains\n-   $\\frac{SSE}{n - k - 1}$ is the average amount of variation left per data point\n\nWhen it comes to comparing which variable is more important. You should **not** look at the parameter estimates. Instead, compare the t-values which do not change based on the scale of the variable.\n\nNote: The Global F-test is actually the same as an ANOVA test but for the weights in our model.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(AmesHousing)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\nset.seed(123)\names <- ames |> mutate(id = row_number())\ntrain <- ames |> sample_frac(0.7)\ntest <- anti_join(ames, train, by = \"id\")\n\names_lm2 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, train)\nsummary(ames_lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-528656  -30077   -1230   21427  361465 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    42562.657   5365.721   7.932 3.51e-15 ***\nGr_Liv_Area      136.982      4.207  32.558  < 2e-16 ***\nTotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56630 on 2048 degrees of freedom\nMultiple R-squared:  0.5024,\tAdjusted R-squared:  0.5019 \nF-statistic:  1034 on 2 and 2048 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ntrain = r.train\names_lm2 = smf.ols(\"Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd\", train).fit()\n\names_lm2.f_pvalue\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.3720518917664e-311\n```\n:::\n:::\n\n\n# Evaluating MLR\n\n## Assumptions\n\n-   The mean of $y$ is accurately modeled by a linear function of the independent variables\n-   $\\varepsilon$ is Normal with a mean of 0\n-   $\\varepsilon$ has a constant variance\n-   The errors are independent\n-   No perfect collinearity\n\n## Multicollinearity\n\n**Multicollinearity** is when predictor variables are correlated with one another.\n\nNo **perfect** collinearity means no predictor variables as a perfect linear combination of each other. In practice, we only care when collinearity has a significant impact.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(ames_lm2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain[\"resid\"] = ames_lm2.resid\ntrain[\"predict\"] = ames_lm2.predict()\nax = sns.relplot(train, x=\"predict\", y=\"resid\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  if pd.api.types.is_categorical_dtype(vector):\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight\n  self._figure.tight_layout(*args, **kwargs)\n```\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.qqplot(train[\"resid\"])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n:::\n\n\n# MLR vs. SLR\n\nWe can investigate more independent variables simultaneously. However, there is increased complexity which makes it difficult to determine which model is \"best\".\n\n# What is MLR Good At?\n\n## Predict\n\nDevelop a model to predict future values of a response variable based on its relationship with other predictor variables.\n\nThe parameters in the model and their statistical significance are secondary importance. Focus is on producing a model that can predict future values well.\n\nWe should take care to be aware of and address overfitting a model in this case.\n\n## Explain\n\nTo develop an understanding of relationships between the response and predictor.\n\nThe statistical significance of coefficients as well as the magnitudes and signs of coefficients are important.\n\n# The $R^2$ Problem\n\nIn MLR, the addition of any variable will make the $R^2$ value increase regardless of the actual strength of the variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_random <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 0, 1), train)\n\nsummary(mlr_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), \n    0, 1), data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-528651  -29901   -1274   21236  359999 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      42589.405   5365.920   7.937 3.38e-15 ***\nGr_Liv_Area                        137.023      4.208  32.565  < 2e-16 ***\nTotRms_AbvGrd                   -10578.883   1370.137  -7.721 1.79e-14 ***\nrnorm(length(Sale_Price), 0, 1)  -1207.529   1269.595  -0.951    0.342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56630 on 2047 degrees of freedom\nMultiple R-squared:  0.5026,\tAdjusted R-squared:  0.5019 \nF-statistic: 689.4 on 3 and 2047 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n$R^2$ increased despite the insignificant variable!\n\n## Adjusted Coefficient of Determination\n\n$R_a^2$ penalizes a model for adding variables that do not provide useful information.\n\n$$\n\\begin{align*}\nR_a^2 &= 1 - [(\\frac{n - 1}{n - k - 1})(\\frac{SSE}{TSS})] \\\\\n&= 1 - [(1 - R^2)(\\frac{n - 1}{n - k - 1})]\n\\end{align*}\n$$\n\n-   $R_a^2 \\leq R^2$\n\nAlthough better at determining utility of model, we lose the interpretation as the coefficient can be negative.\n\n# Categorical Predictors\n\n## Dummy Encoding\n\nWe can encode categorical variables with **dummy encoding**.\n\n$$\nx_1 = \n\\begin{cases}\n1 \\hspace{0.2cm} \\text{if Y}\\\\\n0 \\hspace{0.2cm} \\text{if N}\n\\end{cases}\n$$\n\n$\\beta_1$ will represent the average difference between category Y and N\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\n\nsimple_array = np.array([0, 2, 1])\nencoded_array = np.zeros((simple_array.size, simple_array.max() + 1), dtype=int)\n\nencoded_array[np.arange(simple_array.size), simple_array] = 1\n\nencoded_array\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1, 0, 0],\n       [0, 0, 1],\n       [0, 1, 0]])\n```\n:::\n:::\n\n\n## Effects Encoding\n\n$$\nx_1 =\n\\begin{cases}\n1 \\hspace{0.2cm} \\text{if Y} \\\\\n-1 \\hspace{0.2cm} \\text{if N}\n\\end{cases}\n$$\n\n$\\beta_1$ is the average difference between category Y and **the overall average of categories Y and N**. Essentially, the overall average of all groups in a category variable.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}