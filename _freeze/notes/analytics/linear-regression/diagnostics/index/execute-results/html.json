{
  "hash": "9ef05941ce7bcc04190de70f1b0a715f",
  "result": {
    "markdown": "---\ntitle: Diagnostics\ndate: 07/10/2023\ndate-modified: 07/23/2023\ncategories:\n    -   diagnostics\n---\n\n\n# Examining Residuals\n\nRecall the assumptions of linear regression:\n\n-   Mean of the Ys is accurately modeled by linear function of the Xs\n-   $\\varepsilon$ is assumed to be Normal with a mean of 0\n-   $\\varepsilon$ is assumed to have constant variance $\\sigma^2$\n-   Errors are independent\n-   **No perfect collinearity**\n\nWe can investigate many of our assumptions through residuals in residuals vs. fitted values plots.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(httpgd)\nlibrary(reticulate)\n\nuse_condaenv(condaenv = \"msa\", required = TRUE)\ndata_path <- \"data/Salary.csv\"\nsalary <- read.csv(data_path)\npar(mfrow = c(2, 2))\nsalary_lm <- lm(Salary ~ YearsExperience, data = salary)\n\nggplot(salary_lm, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport seaborn as sns\nfrom pathlib import Path\n\nsalary = r.salary\nsns.residplot(salary, x=\"YearsExperience\", y=\"Salary\", order=2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n# Misspecified Model\n\nWe detect a misspecified model when a pattern is detected in the residuals. The model form is incorrect for the data.\n\nWe can potentially resolve this by including polynomial terms, interactions, splines, etc.\n\n# Model Hierarchy\n\nWhen adding higher order terms (power terms and/or interactions) you should have **all** lower terms included in the model.\n\nIf $x^3$ is in the model, you should have $x$ and $x^2$ in the model as well. If you include an interaction $x_1x_2$ in the model, then $x_1$ and $x_2$ should be included.\n\n# Polynomial Regression\n\nPatterns in residual plots of our variable may give us an indication to try higher order terms.\n\nIn a polynomial regression, if the higher order term is used then we lose the interpretation for that entire variable.\n\nIf we do model selection with higher order terms and the higher orders end up in the final model then we have to make sure to add the lower terms back in to the model.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsalary_quad <- lm(Salary ~ YearsExperience + I(YearsExperience^2), data = salary)\nsummary(salary_quad)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Salary ~ YearsExperience + I(YearsExperience^2), \n    data = salary)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9210.7 -4037.8  -467.7  3485.5 11052.0 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(>|t|)    \n(Intercept)          21855.58    3630.80   6.019 1.03e-06 ***\nYearsExperience      11456.37    1217.28   9.411 9.79e-11 ***\nI(YearsExperience^2)  -193.90      84.45  -2.296   0.0284 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5733 on 32 degrees of freedom\nMultiple R-squared:  0.9701,\tAdjusted R-squared:  0.9682 \nF-statistic:   519 on 2 and 32 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nggplot(salary_quad, aes(x = .fitted, y = .resid)) +\n    geom_point() +\n    geom_hline(yintercept = 0) +\n    labs(x = \"Predicted Values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-3.png){width=672}\n:::\n:::\n\n\n-   In R we use the `I()` function to create a higher order term\n\nWhen a straight line is inappropriate, we can consider:\n\n-   Fit a polynomial/more complex regression\n-   Transform the dependent and/or independent variables to obtain linearity\n-   Fit a nonlinear regression model if appropriate\n-   Fit a nonparametric regression model (e.g. LOESS)\n\n# Lack of Constant Variance\n\nThe random error, $\\varepsilon$, is assumed to have a constant variance, $\\sigma^2$. Under heteroscedasticity, any inferences under traditional assumptions **will be incorrect and our hypothesis tests and confidence intervals will not be valid.**\n\nWe can detect heteroscedasticity by either plotting residuals or using **Spearman Rank Correlation**.\n\n![Lack of Constant Variance](images/heteroscedasticity.png)\n\n## Spearman Rank Correlation\n\nIf Spearman rank coefficient between the absolute value of the residuals and predicted values is:\n\n-   Close to zero: variance is potentially homoscedastic\n-   Positive: variance increases as mean increases\n-   Negative: variance descreases as the mean increases\n\n::: text-center\n$H_0:$ Variance is homoscedastic\n\n$H_a:$ Variance is heteroscedastic\n:::\n\nIf there is a relationship between the absolute value of residuals and predicted value but it is not linear, this test will **not** discover it.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm_var <- lm(Salary ~ YearsExperience, data = salary)\ncor.test(abs(resid(lm_var)), fitted.values(lm_var), method = \"spearman\", exact = T)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(abs(resid(lm_var)), fitted.values(lm_var), method =\n\"spearman\", : Cannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  abs(resid(lm_var)) and fitted.values(lm_var)\nS = 7493, p-value = 0.7779\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n       rho \n-0.0494467 \n```\n:::\n:::\n\n\n# Lack of Normality\n\nCheck that error terms are Normal by examining:\n\n-   Histogram of residuals\n-   Normal probability plot of the residuals (QQ-Plot)\n-   Formal tests for Normality\n\n## Formal Tests\n\n::: text-center\n$H_0:$ Normality\n\n$H_a:$ Not Normal\n:::\n\n**Anderson-Darling** is based on empirical cumulative distribution function of data and gives more weight to the tails.\n\n**Shapiro-Wilk** uses correlation between sample data and normal scores. The Shapiro-Wilk is better for smaller data sets.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(nortest)\nad.test(resid(lm_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tAnderson-Darling normality test\n\ndata:  resid(lm_var)\nA = 0.4791, p-value = 0.2205\n```\n:::\n\n```{.r .cell-code}\nshapiro.test(resid(lm_var))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tShapiro-Wilk normality test\n\ndata:  resid(lm_var)\nW = 0.94679, p-value = 0.09028\n```\n:::\n:::\n\n\n## Accounting for Lack of Normality\n\nDepends on why the assumption was broken:\n\n-   Outliers $\\rightarrow$ Robust Regression\n-   Nonnormal $\\rightarrow$ Transformation Needed\n    -   Can try Box-Cox transformation\n\n## Box-Cox Transformation\n\nBox-Cox developed method to determine best power transformation to induce Normality.\n\n$$\n\\begin{align*}\n(y^{\\lambda} - 1) / \\lambda, \\hspace{0.2cm} \\lambda &\\neq 0 \\\\\n\\log(y), \\hspace{0.2cm} \\lambda &= 0\n\\end{align*}\n$$\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'MASS'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n\n```{.r .cell-code}\nboxcox(lm_var)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}