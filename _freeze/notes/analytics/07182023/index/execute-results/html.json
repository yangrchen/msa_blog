{
  "hash": "29eb00d32c171307df8c61b82b60957e",
  "result": {
    "markdown": "---\ntitle: Introduction to Logistic Regression\ndate: 07/18/2023\n---\n\n\n# Setup\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nuse_condaenv(\"blues_clues\")\n\nset.seed(123)\names <- make_ordinal_ames()\n\names <- ames %>%\n    mutate(id = row_number())\ntrain <- ames %>% sample_frac(0.7)\ntrain <- train %>%\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\n\ntrain = r.train\n\n```\n:::\n\n\nLogistic regression is one of many classification models that can help us predict a specific class in a categorical variable.\n\nOne of the most common targets we will be trying to predict is a binary target variable (Yes / No, 1 / 0).\n\n# Why Linear Regression Doesn't Work for Classification\n\n$$\ny_i = \\beta_0 + \\beta_1x_{1,i} + \\varepsilon_i\n$$\n\n-   If response is categorical, how do you encode the response numerically?\n-   If our regression is prediction 0.5, 1.1, -0.4 what does that mean practically?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlp_model <- lm(Bonus ~ Gr_Liv_Area, data = train)\nwith(train, plot(\n    x = Gr_Liv_Area, y = Bonus,\n    main = \"OLS Regression?\",\n    xlab = \"Greater Living ARea (Sqft)\",\n    ylab = \"Bonus Eligibility\"\n))\nabline(lp_model)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# Binary Logistic Regression\n\n$$\np_i = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i})}}\n$$\n\n-   Predicted probability will always be between 0 and 1\n-   Parameter estimates do not enter model equation linearly\n-   Rate of change of probability varies as the X's vary\n\nDecision probabilities are not linear. Even small changes in X can drastically change the decision.\n\n\n::: {.cell}\n\n```{.python .cell-code}\ndef sigmoid(x):\n    return 1.0 / (1.0 + np.exp(-x))\n\n\nx = np.arange(-10, 10, 0.1)\n\nplt.plot(x, sigmoid(x))\nplt.xlabel(\"x\")\nplt.ylabel(\"sigmoid(x)\")\nplt.title(\"Sigmoid Curve\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n## Logit Link Function\n\nTo create linear model, a logit function is applied to the probabilities:\n\n$$\n\\log(\\frac{p_i}{1 - p_i}) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_k x_{k,i}\n$$\n\n-   $\\frac{p_i}{1 - p_i}$ is the odds of an outcome happening\n-   The relationship between parameters and **logits** are linear\n-   Logits unbounded\n\n## Assumptions\n\n-   Independence of observations\n-   Logit is linearly related to variables\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_logit <- glm(Bonus ~ Gr_Liv_Area, data = train, family = binomial())\n\nsummary(ames_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Bonus ~ Gr_Liv_Area, family = binomial(), data = train)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -6.1348858  0.2757473  -22.25   <2e-16 ***\nGr_Liv_Area  0.0038463  0.0001799   21.38   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1926.4  on 2049  degrees of freedom\nAIC: 1930.4\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\names_logit = smf.logit(\"Bonus ~ Gr_Liv_Area\", data=train).fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.469614\n         Iterations 7\n```\n:::\n\n```{.python .cell-code}\names_logit.summary()\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>Bonus</td>      <th>  No. Observations:  </th>   <td>  2051</td>  \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  2049</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Tue, 18 Jul 2023</td> <th>  Pseudo R-squ.:     </th>   <td>0.3060</td>  \n</tr>\n<tr>\n  <th>Time:</th>                <td>14:14:54</td>     <th>  Log-Likelihood:    </th>  <td> -963.18</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1387.9</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>9.553e-187</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n       <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>   <td>   -6.1349</td> <td>    0.276</td> <td>  -22.248</td> <td> 0.000</td> <td>   -6.675</td> <td>   -5.594</td>\n</tr>\n<tr>\n  <th>Gr_Liv_Area</th> <td>    0.0038</td> <td>    0.000</td> <td>   21.375</td> <td> 0.000</td> <td>    0.003</td> <td>    0.004</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\n# Interpretation of Logistic Regression Coefficients\n\nA $\\hat{\\beta}$ change in the logit is equivalent to $100 \\cdot (e^{\\hat{\\beta}} - 1)\\%$. Why?\n\n$$\n\\frac{p}{1 - p} = e^{\\beta_0 + \\beta_1x}\n$$\n\nWith a unit change in x:\n\n$$\n\\frac{p}{1 - p} e^{\\beta_0 + \\beta_1x + \\beta_1}\n$$\n\nThe odds ratio is then the ratio between the unit change odds and default odds:\n\n$$\n\\text{Odds Ratio} = \\frac{e^{\\beta_0 + \\beta_1x + \\beta_1}}{e^{\\beta_0 + \\beta_1x}} = e^{\\beta_1}\n$$\n\nThe interpretation in terms of a percentage change in expected odds is then:\n\n$$\n100 \\cdot (e^{\\beta_1} - 1)\\%\n$$\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n100 * (exp(cbind(coef(ames_logit), confint(ames_logit))) - 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                              2.5 %      97.5 %\n(Intercept) -99.7834027 -99.8755103 -99.6328865\nGr_Liv_Area   0.3853699   0.3508132   0.4216532\n```\n:::\n:::\n\n\nThe reason we subtract by 1 is that 1 is the center point of odds. We subtract to help center back around 0.\n\nEvery additional square foot of greater living area increases the expected odds of being bonus eligible by 0.38%.\n\nIn practice, a client will care more about this interpretation of how an outcome odds changes rather than the pure technical details behind how a model was built.\n\n## Categorical Variable Interpretation\n\n\n::: {.cell}\n\n```{.r .cell-code}\names_logit2 <- glm(Bonus ~ Gr_Liv_Area + Central_Air + factor(Fireplaces), data = train, family = binomial())\n\n100 * (exp(cbind(coef(ames_logit2), confint(ames_logit2))) - 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                         2.5 %        97.5 %\n(Intercept)         -9.999532e+01  -99.9988238   -99.9843742\nGr_Liv_Area          3.766413e-01    0.3376191     0.4175879\nCentral_AirY         3.429115e+03 1264.8396218 11177.2021479\nfactor(Fireplaces)1  1.670410e+02  108.9856565   241.6360910\nfactor(Fireplaces)2  9.607987e+01   22.4764491   214.9571145\nfactor(Fireplaces)3 -3.914247e+00  -81.9507819   497.4825425\nfactor(Fireplaces)4  8.308750e+05 -100.0000000            NA\n```\n:::\n\n```{.r .cell-code}\nexp(cbind(coef(ames_logit2), confint(ames_logit2)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                        2.5 %       97.5 %\n(Intercept)         4.678435e-05 1.176215e-05 1.562580e-04\nGr_Liv_Area         1.003766e+00 1.003376e+00 1.004176e+00\nCentral_AirY        3.529115e+01 1.364840e+01 1.127720e+02\nfactor(Fireplaces)1 2.670410e+00 2.089857e+00 3.416361e+00\nfactor(Fireplaces)2 1.960799e+00 1.224764e+00 3.149571e+00\nfactor(Fireplaces)3 9.608575e-01 1.804922e-01 5.974825e+00\nfactor(Fireplaces)4 8.309750e+03 1.523733e-25           NA\n```\n:::\n\n```{.r .cell-code}\ncoef(ames_logit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        (Intercept)         Gr_Liv_Area        Central_AirY factor(Fireplaces)1 \n       -9.969961800         0.003759338         3.563632363         0.982231915 \nfactor(Fireplaces)2 factor(Fireplaces)3 factor(Fireplaces)4 \n        0.673351875        -0.039929137         9.025184862 \n```\n:::\n:::\n\n\nHomes with central air increases the expected odds of being bonus eligible by 3416% compared to those without central air. \n\nA more believable way of saying this would be $e^{3.56} = 35.16$ times more likely to be bonus eligible than compared to those without central air.\n\n# Model Assessment\n\nOne main way to evaluate models is to compare every pair of 0's and 1's in the target variable. These pairs are considered **concordant, discordant, or tied.** We want to rank our 1's higher than our 0's.\n\n## Concordant\n\n0 and 1 pair where bonus eligible home (1) has a higher predicted probability than the non-bonus eligible home (0).\n\nDoes not matter what the actual predicted probability values are as long as the bonus eligible home has a higher predicted probability than the non-bonus eligible home.\n\n## Discordant\n\n0 and 1 pair where bonus eligible home (1) has a lower predicted probability than the non-bonus eligible home (0)\n\nModel unsuccessfully ordered the homes.\n\n## Tied Pair\n\n0 and 1 pair where the bonus eligible home has the same predicted probability as the non-bonus eligible home.\n\n## Concordance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\n\nconcordance(ames_logit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nconcordance.lm(object = ames_logit)\n\nn= 2051 \nConcordance= 0.8632 se= 0.007744\nconcordant discordant     tied.x     tied.y    tied.xy \n    877810     138829        601    1083029       2006 \n```\n:::\n:::\n\n\nModel correctly ranks bonus eligible homes ahead of non-bonus eligible homes 86.3% of the time. This does not mean that our model is accurate 86.3% of the time.\n\n# Variable Selection and Regularized Regression\n\nAll of the same approaches to variable selection in linear regression are available for logistic.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_sel_log <- train %>%\n    select(\n        Bonus, Lot_Area, Street, Bldg_Type, House_Style,\n        Overall_Qual, Roof_Style, Central_Air,\n        First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath,\n        Fireplaces, Garage_Area, Gr_Liv_Area,\n        TotRms_AbvGrd\n    ) %>%\n    replace(is.na(.), 0)\n\nfull_model <- glm(Bonus ~ ., data = train_sel_log)\nempty_model <- glm(Bonus ~ 1, data = train_sel_log)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor_model <- step(\n    empty_model,\n    scope = list(\n        lower = empty_model,\n        upper = full_model\n    ),\n    direction = \"forward\",\n    k = log(dim(train_sel_log)[1])\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=2918.59\nBonus ~ 1\n\n                Df Deviance    AIC\n+ Overall_Qual   9   234.74 1453.0\n+ Full_Bath      1   320.51 2030.8\n+ Gr_Liv_Area    1   335.11 2122.1\n+ Garage_Area    1   367.62 2312.0\n+ First_Flr_SF   1   396.15 2465.3\n+ Fireplaces     1   415.26 2561.9\n+ TotRms_AbvGrd  1   422.53 2597.6\n+ House_Style    7   444.22 2745.9\n+ Half_Bath      1   459.40 2769.1\n+ Second_Flr_SF  1   461.26 2777.4\n+ Central_Air    1   473.64 2831.7\n+ Lot_Area       1   478.42 2852.3\n+ Bldg_Type      4   477.21 2870.0\n<none>               495.97 2918.6\n+ Street         1   495.86 2925.8\n+ Roof_Style     5   491.40 2937.7\n\nStep:  AIC=1452.99\nBonus ~ Overall_Qual\n\n                Df Deviance    AIC\n+ Full_Bath      1   203.87 1171.4\n+ Gr_Liv_Area    1   208.62 1218.7\n+ First_Flr_SF   1   218.70 1315.5\n+ Garage_Area    1   223.10 1356.3\n+ TotRms_AbvGrd  1   223.12 1356.5\n+ Fireplaces     1   223.33 1358.5\n+ Lot_Area       1   225.07 1374.3\n+ Bldg_Type      4   227.31 1417.6\n+ Second_Flr_SF  1   230.98 1427.5\n+ Half_Bath      1   231.81 1434.8\n+ Central_Air    1   233.55 1450.2\n<none>               234.74 1453.0\n+ Street         1   234.30 1456.8\n+ House_Style    7   230.28 1467.1\n+ Roof_Style     5   234.18 1486.2\n\nStep:  AIC=1171.42\nBonus ~ Overall_Qual + Full_Bath\n\n                Df Deviance    AIC\n+ Fireplaces     1   193.70 1074.1\n+ First_Flr_SF   1   195.11 1089.0\n+ Gr_Liv_Area    1   196.26 1101.0\n+ Lot_Area       1   197.22 1111.0\n+ Bldg_Type      4   195.24 1113.2\n+ Garage_Area    1   197.63 1115.3\n+ Half_Bath      1   201.31 1153.1\n+ TotRms_AbvGrd  1   202.51 1165.3\n+ Central_Air    1   202.61 1166.4\n<none>               203.87 1171.4\n+ Street         1   203.38 1174.2\n+ Second_Flr_SF  1   203.85 1178.9\n+ House_Style    7   201.34 1199.2\n+ Roof_Style     5   203.40 1204.8\n\nStep:  AIC=1074.14\nBonus ~ Overall_Qual + Full_Bath + Fireplaces\n\n                Df Deviance    AIC\n+ Garage_Area    1   188.96 1030.9\n+ First_Flr_SF   1   189.34 1035.0\n+ Bldg_Type      4   187.45 1037.3\n+ Lot_Area       1   190.10 1043.3\n+ Gr_Liv_Area    1   190.64 1049.1\n+ Half_Bath      1   192.04 1064.0\n<none>               193.70 1074.1\n+ Central_Air    1   193.02 1074.5\n+ Street         1   193.35 1078.0\n+ TotRms_AbvGrd  1   193.42 1078.7\n+ Second_Flr_SF  1   193.69 1081.7\n+ House_Style    7   190.38 1092.0\n+ Roof_Style     5   193.16 1106.5\n\nStep:  AIC=1030.9\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area\n\n                Df Deviance    AIC\n+ Bldg_Type      4   183.72 1003.8\n+ First_Flr_SF   1   186.24 1008.8\n+ Lot_Area       1   186.46 1011.2\n+ Gr_Liv_Area    1   186.87 1015.7\n+ Half_Bath      1   187.55 1023.2\n<none>               188.96 1030.9\n+ Street         1   188.68 1035.5\n+ Central_Air    1   188.76 1036.3\n+ TotRms_AbvGrd  1   188.78 1036.6\n+ Second_Flr_SF  1   188.96 1038.5\n+ House_Style    7   186.42 1056.5\n+ Roof_Style     5   188.20 1060.8\n\nStep:  AIC=1003.78\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type\n\n                Df Deviance     AIC\n+ First_Flr_SF   1   181.17  982.68\n+ Half_Bath      1   181.99  991.99\n+ Gr_Liv_Area    1   182.00  992.09\n+ Lot_Area       1   182.09  993.12\n<none>               183.72 1003.78\n+ Street         1   183.36 1007.35\n+ Central_Air    1   183.60 1009.98\n+ TotRms_AbvGrd  1   183.66 1010.71\n+ Second_Flr_SF  1   183.72 1011.35\n+ House_Style    7   180.41 1019.87\n+ Roof_Style     5   182.90 1032.69\n\nStep:  AIC=982.68\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF\n\n                Df Deviance     AIC\n+ Half_Bath      1   177.46  947.94\n+ Second_Flr_SF  1   180.10  978.18\n+ Lot_Area       1   180.24  979.73\n+ Gr_Liv_Area    1   180.28  980.18\n<none>               181.17  982.68\n+ Street         1   180.77  985.80\n+ House_Style    7   176.95  987.73\n+ Central_Air    1   181.06  989.10\n+ TotRms_AbvGrd  1   181.16  990.19\n+ Roof_Style     5   179.76 1004.77\n\nStep:  AIC=947.94\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF + Half_Bath\n\n                Df Deviance    AIC\n+ Lot_Area       1   176.75 947.31\n<none>               177.46 947.94\n+ TotRms_AbvGrd  1   177.03 950.49\n+ Street         1   177.09 951.18\n+ Central_Air    1   177.43 955.17\n+ Gr_Liv_Area    1   177.46 955.45\n+ Second_Flr_SF  1   177.46 955.56\n+ Roof_Style     5   175.95 968.54\n+ House_Style    7   174.78 970.01\n\nStep:  AIC=947.31\nBonus ~ Overall_Qual + Full_Bath + Fireplaces + Garage_Area + \n    Bldg_Type + First_Flr_SF + Half_Bath + Lot_Area\n\n                Df Deviance    AIC\n<none>               176.75 947.31\n+ TotRms_AbvGrd  1   176.32 949.96\n+ Street         1   176.54 952.49\n+ Central_Air    1   176.72 954.56\n+ Gr_Liv_Area    1   176.73 954.71\n+ Second_Flr_SF  1   176.75 954.89\n+ Roof_Style     5   175.25 968.01\n+ House_Style    7   174.09 969.62\n```\n:::\n:::\n\n\nVariable selection methods are computationally expensive. You should spend time to find a subset of main effects that you want to try *and then* try variable selection.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}