{
  "hash": "a6baaad7ba13ba9414e8eddb56df2a10",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Propensity Score Matching\ndate: 03/13/2024\n---\n\nAlthough randomized controlled trials (RCTs) are the \"gold standard\" for assessing changes in an environment, they may not be possible.\n\n-   Ethics\n-   Costs\n-   Lack of known participant properties\n-   Sample size\n\nIn these situations, we use **propensity score matching** to match pairs of participants that are similar, placing one in group A and one in group B. Propensity score matching also helps to account for confounding factors when we are unsure if there are between two groups.\n\nParticipants are defined by multiple attributes. Rather than choosing through random selection, we can choose two participants that are \"similar\" to one another and place one in group A and one in group B. We use random selection where the correct attributes are not available.\n\nPropensity scoring is calculated from observational data about participants. We split participants based on two participants having common observable characteristics. Rather than using the participant attributes directly, we calculate a propensity score for each particpant. A propensity score is a value where the \"score\" of a participant is determined as a function of the covariates.\n\nThe standard method to reduce our covariates to a single value is to fit a logistic regression model to the covariates, then use the model to convert covariates to a single logit value. \n\n1.  Randomly divide participants into group A and group B, like in A-B testing.\n2.  Choose which vcovariate attributes you will use to calculate a participant's propensity score.\n3.  Fit a logistic regression model using the selected covariates.\n4.  Use the score the compute a propensity score for each participant $p_i$.\n5.  Order participants in both groups by their propensity scores.\n6.  For each participant $p_i$, find their nearest neighbor $n_i$ in the opposite group. If $n_i$ is farther than a threshold value, do not include $p_i$ in the follow-on analysis.\n7.  Store the pair $(p_i, n_i)$ as a matched pair.\n8.  Once all participants are paired or removed, search for significance over the pairs' proportional differences $|\\mu_{p_i} - \\mu_{n_i}|$.\n\n# PSM Code\n\n::: {#64208b36 .cell execution_count=1}\n``` {.python .cell-code}\nimport random\nimport matplotlib.pyplot as plt \nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.datasets import make_classification\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import auc\nfrom sklearn.neighbors import NearestNeighbors\nfrom IPython.display import display\nimport numpy as np\nimport pandas as pd\n\n# Generate noisy data\nX, y = make_classification(\n    n_samples=1000,\n    n_features=4,\n    n_redundant=0,\n    n_classes=2,\n    n_clusters_per_class=1,\n    class_sep=2,\n    flip_y=0.2,\n    weights=[0.5, 0.5],\n)\n\n# Create min-max scaler, normalize data to usage ratios\nscaler = MinMaxScaler()\nnormalized_data = scaler.fit_transform(X)\n\ndata = pd.DataFrame(normalized_data, columns=[\"A\", \"B\", \"C\", \"D\"])\ndata[\"renewal_status\"] = y\ndata[\"treatment\"] = (data[\"A\"] >= 0.4) * 1\n\n# Select covariates for PSM\ncovariates = [\"B\", \"C\", \"D\"]\n\nX = data[covariates]\ny = data[\"treatment\"]\n\n# Fit logit to get coefficients for covariates\nlogit = LogisticRegression()\nlogit.fit(X, y)\n\nPS = logit.predict_proba(X)[:, 1]\nfalse_positive_rate, true_positive_rate, th = roc_curve(y, PS)\n\n# Match treated and control individuals based on propensity score\ntreated_indices = data[data[\"treatment\"] == 1].index\ncontrol_indices = data[data[\"treatment\"] == 0].index\n\nnbrs = NearestNeighbors(n_neighbors=1, algorithm=\"ball_tree\").fit(\n    np.reshape(PS[control_indices], (-1, 1))\n)\ndistances, indices = nbrs.kneighbors(\n    np.reshape(PS[treated_indices], (-1, 1))\n)\nprint(indices)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[133]\n [  8]\n [118]\n [202]\n [274]\n [ 25]\n [141]\n [ 50]\n [271]\n [146]\n [230]\n [147]\n [243]\n [130]\n [ 87]\n [123]\n [202]\n [240]\n [152]\n [ 41]\n [ 32]\n [179]\n [278]\n [154]\n [227]\n [ 16]\n [111]\n [159]\n [127]\n [271]\n [253]\n [240]\n [ 19]\n [201]\n [119]\n [240]\n [ 59]\n [ 64]\n [169]\n [180]\n [281]\n [ 90]\n [143]\n [145]\n [ 31]\n [239]\n [193]\n [214]\n [  4]\n [167]\n [ 20]\n [238]\n [214]\n [239]\n [ 12]\n [ 44]\n [169]\n [225]\n [ 11]\n [103]\n [ 60]\n [  4]\n [271]\n [103]\n [127]\n [159]\n [154]\n [101]\n [ 79]\n [ 29]\n [263]\n [191]\n [193]\n [264]\n [192]\n [184]\n [ 56]\n [ 76]\n [101]\n [188]\n [ 38]\n [106]\n [261]\n [  0]\n [ 22]\n [ 82]\n [111]\n [ 23]\n [169]\n [235]\n [239]\n [159]\n [191]\n [ 41]\n [222]\n [  8]\n [237]\n [169]\n [218]\n [ 50]\n [223]\n [265]\n [180]\n [127]\n [124]\n [ 47]\n [ 69]\n [ 61]\n [128]\n [ 43]\n [ 37]\n [260]\n [193]\n [120]\n [214]\n [ 10]\n [163]\n [ 37]\n [188]\n [ 12]\n [  1]\n [217]\n [273]\n [ 44]\n [141]\n [ 80]\n [238]\n [240]\n [ 41]\n [144]\n [239]\n [ 78]\n [180]\n [ 83]\n [275]\n [246]\n [210]\n [168]\n [239]\n [241]\n [205]\n [265]\n [ 52]\n [238]\n [240]\n [127]\n [222]\n [273]\n [ 53]\n [256]\n [115]\n [ 63]\n [ 22]\n [ 40]\n [238]\n [183]\n [178]\n [260]\n [ 92]\n [191]\n [ 19]\n [103]\n [270]\n [253]\n [252]\n [ 60]\n [186]\n [167]\n [104]\n [104]\n [193]\n [259]\n [245]\n [  0]\n [  1]\n [ 99]\n [ 57]\n [152]\n [  6]\n [179]\n [ 47]\n [114]\n [165]\n [193]\n [251]\n [153]\n [205]\n [228]\n [252]\n [101]\n [  0]\n [132]\n [ 26]\n [266]\n [216]\n [264]\n [270]\n [125]\n [ 27]\n [170]\n [197]\n [ 11]\n [ 12]\n [ 14]\n [ 48]\n [ 70]\n [211]\n [162]\n [255]\n [244]\n [ 69]\n [122]\n [184]\n [225]\n [223]\n [180]\n [ 44]\n [228]\n [ 50]\n [103]\n [ 74]\n [154]\n [146]\n [165]\n [241]\n [ 12]\n [129]\n [262]\n [123]\n [171]\n [114]\n [ 85]\n [132]\n [114]\n [ 68]\n [264]\n [251]\n [  8]\n [203]\n [202]\n [103]\n [ 92]\n [ 49]\n [206]\n [115]\n [192]\n [ 11]\n [ 60]\n [240]\n [276]\n [241]\n [235]\n [ 48]\n [ 14]\n [156]\n [192]\n [100]\n [127]\n [ 12]\n [180]\n [ 42]\n [101]\n [ 63]\n [101]\n [103]\n [ 43]\n [212]\n [281]\n [104]\n [112]\n [ 55]\n [ 70]\n [152]\n [281]\n [220]\n [ 26]\n [ 12]\n [  0]\n [ 68]\n [  8]\n [243]\n [202]\n [ 69]\n [147]\n [ 16]\n [ 47]\n [167]\n [ 11]\n [ 50]\n [ 10]\n [192]\n [146]\n [ 12]\n [  6]\n [227]\n [122]\n [260]\n [ 44]\n [123]\n [ 56]\n [142]\n [ 93]\n [ 63]\n [223]\n [116]\n [111]\n [141]\n [112]\n [103]\n [ 47]\n [  0]\n [ 65]\n [ 70]\n [194]\n [ 49]\n [211]\n [223]\n [169]\n [ 70]\n [270]\n [213]\n [251]\n [256]\n [ 66]\n [146]\n [271]\n [121]\n [188]\n [114]\n [ 46]\n [  2]\n [ 81]\n [108]\n [175]\n [  8]\n [139]\n [271]\n [ 13]\n [272]\n [ 45]\n [202]\n [144]\n [221]\n [251]\n [218]\n [216]\n [ 31]\n [ 22]\n [102]\n [ 48]\n [127]\n [ 70]\n [140]\n [235]\n [100]\n [281]\n [  5]\n [253]\n [179]\n [255]\n [223]\n [121]\n [256]\n [134]\n [187]\n [175]\n [ 61]\n [219]\n [144]\n [187]\n [139]\n [ 44]\n [ 37]\n [ 16]\n [161]\n [228]\n [ 74]\n [238]\n [251]\n [185]\n [252]\n [ 59]\n [ 48]\n [190]\n [135]\n [  5]\n [125]\n [156]\n [ 45]\n [ 41]\n [167]\n [225]\n [ 16]\n [132]\n [252]\n [271]\n [230]\n [ 28]\n [259]\n [103]\n [176]\n [ 47]\n [202]\n [ 89]\n [  1]\n [111]\n [ 22]\n [ 50]\n [ 16]\n [ 40]\n [188]\n [115]\n [101]\n [ 18]\n [177]\n [ 57]\n [251]\n [ 29]\n [278]\n [260]\n [212]\n [116]\n [205]\n [241]\n [252]\n [ 31]\n [111]\n [103]\n [119]\n [243]\n [ 49]\n [114]\n [132]\n [192]\n [191]\n [261]\n [222]\n [ 76]\n [134]\n [144]\n [277]\n [137]\n [270]\n [155]\n [125]\n [228]\n [251]\n [149]\n [159]\n [ 99]\n [255]\n [260]\n [ 69]\n [107]\n [196]\n [ 12]\n [238]\n [100]\n [ 41]\n [ 50]\n [253]\n [ 55]\n [ 53]\n [132]\n [218]\n [230]\n [221]\n [158]\n [180]\n [159]\n [ 70]\n [186]\n [ 22]\n [116]\n [183]\n [205]\n [150]\n [176]\n [  1]\n [ 75]\n [ 47]\n [193]\n [183]\n [211]\n [ 33]\n [ 49]\n [239]\n [240]\n [215]\n [ 43]\n [156]\n [  7]\n [ 91]\n [ 32]\n [ 16]\n [187]\n [275]\n [230]\n [ 57]\n [238]\n [275]\n [139]\n [154]\n [139]\n [ 60]\n [195]\n [242]\n [183]\n [155]\n [173]\n [100]\n [ 95]\n [ 45]\n [148]\n [103]\n [125]\n [ 46]\n [243]\n [114]\n [239]\n [ 90]\n [ 26]\n [253]\n [260]\n [219]\n [227]\n [264]\n [160]\n [  7]\n [112]\n [ 97]\n [ 50]\n [165]\n [203]\n [ 80]\n [260]\n [187]\n [ 57]\n [ 42]\n [253]\n [ 12]\n [ 43]\n [229]\n [184]\n [ 28]\n [263]\n [178]\n [107]\n [255]\n [264]\n [190]\n [ 33]\n [239]\n [ 34]\n [ 40]\n [238]\n [195]\n [260]\n [ 70]\n [253]\n [255]\n [244]\n [172]\n [252]\n [179]\n [  5]\n [162]\n [147]\n [123]\n [ 50]\n [197]\n [110]\n [186]\n [111]\n [250]\n [186]\n [260]\n [132]\n [  7]\n [215]\n [156]\n [111]\n [148]\n [ 40]\n [270]\n [ 56]\n [121]\n [139]\n [202]\n [ 97]\n [ 90]\n [212]\n [ 41]\n [139]\n [260]\n [  6]\n [176]\n [141]\n [ 49]\n [117]\n [261]\n [172]\n [240]\n [253]\n [ 30]\n [175]\n [ 62]\n [183]\n [215]\n [121]\n [120]\n [183]\n [175]\n [278]\n [110]\n [139]\n [ 19]\n [178]\n [158]\n [140]\n [101]\n [165]\n [160]\n [180]\n [202]\n [ 63]\n [240]\n [251]\n [202]\n [ 93]\n [278]\n [ 63]\n [111]\n [205]\n [271]\n [274]\n [166]\n [ 23]\n [225]\n [148]\n [ 51]\n [265]\n [ 88]\n [167]\n [ 16]\n [164]\n [  1]\n [ 24]\n [ 99]\n [274]\n [245]\n [236]\n [ 50]\n [243]\n [ 78]\n [120]\n [ 80]\n [ 91]\n [ 76]\n [101]\n [130]\n [ 73]\n [ 22]\n [212]\n [215]\n [107]\n [237]\n [179]\n [205]\n [ 24]\n [ 41]\n [ 41]\n [143]\n [ 14]\n [179]\n [179]\n [218]\n [ 47]\n [ 47]\n [158]\n [197]\n [138]\n [132]\n [ 12]\n [ 14]\n [ 83]\n [ 13]\n [ 23]\n [ 59]\n [161]\n [220]\n [137]\n [ 17]\n [ 81]\n [172]\n [271]\n [106]\n [273]\n [ 36]\n [ 58]\n [132]\n [ 41]\n [100]\n [241]\n [219]\n [ 86]\n [ 50]\n [271]\n [154]\n [236]\n [132]\n [146]\n [247]\n [273]\n [234]\n [193]\n [259]\n [229]]\n```\n:::\n:::\n\n\nThe code implements PSM to test whether a four-product dataset will generate higher renewal rates when the usage of the first product A is greater than 40%.\n\n1.  Create random data with four products A, B, C, D; a binary column indicating whether a customer renewed their subscription `renewal_status`, and a binary column identifying customers with product A usage over 40%.\n2.  Select covariates B, C, and D and fit a logit to produce an outcome treatment variable to predict whether a customer will have a usage for product A above or below 40%.\n3.  Given the logit probabilities, we pair customers in the control and treatment groups with similar probabilities using k-nearest neighbors.\n4.  We compare the mean renewal rate for control and treatment groups to determine the predicted renewal rate when product A usage is above 40%.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}