{
  "hash": "c0c1871c1b88321a5f557e0962cc0c16",
  "result": {
    "markdown": "---\ntitle: \"Introduction to Logistic Regression Review\"\ndate: 08/22/2023\n---\n\n\nBinary classification is a **supervised** algorithm where we are trying to predict one of two target outcomes. Binary classification is one of the most common type of business problems that need solving.\n\n-   Targeted Marketing\n-   Churn Prediction\n-   Probability of Default\n-   Fraud Detection\n\nWe will use the Ames dataset again for binary logistic regression.\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames |>\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\ntrain <- sample_frac(ames, 0.7)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\names = r.ames\n```\n:::\n\n\n:::\n\nWhat is regression actually doing? \n\nModeling the *expected* response conditional on the predictors. For a binary response, $y_i$, the expected value is the probability of the event:\n\n$$\nE(y_i) = P(y_i = 1) = p_i\n$$\n\nWhy can't we model the following:\n\n$$\np_i = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n$$\n\nProbabilities are bounded but linear functions can take on any value. Relationship between probabilities and X is usually nonlinear. Properties of OLS do not hold.\n\n# Logistic Regression Model\n\n$$\np_i = \\frac{1}{1 + e^{-z}}\n$$\n\n-   $z$ is the linear equation with the $\\beta_k$\n-   Predicted probability will always be between 0 and 1\n-   Parameter estimates do not enter the model linearly\n-   Rate of change of the probability varies as the X's vary\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\n  \nx = np.linspace(-10, 10, 100)\nz = 1/(1 + np.exp(-x))\n  \nplt.plot(x, z)\nplt.xlabel(\"x\")\nplt.ylabel(\"Sigmoid(X)\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nTo convert back to a linear model, we use the logit function:\n\n$$\n\\log(\\frac{p_i}{1 - p_i}) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n$$\n\nRelationship between parameters and logits are linear. Logits are unbounded.\n\n# Coefficient Interpretations\n\nWith a $\\hat{\\beta}$ change in the logit, we can interpret the change in odds as $e^{\\hat{\\beta}}$ or as a percentage $100 \\cdot (e^{\\hat{\\beta}} - 1)\\%$\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_model <- glm(Bonus ~ Gr_Liv_Area + factor(Central_Air), data = train, family = binomial(link = \"logit\"))\nsummary(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Bonus ~ Gr_Liv_Area + factor(Central_Air), family = binomial(link = \"logit\"), \n    data = train)\n\nCoefficients:\n                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)          -1.035e+01  6.422e-01  -16.12  < 2e-16 ***\nGr_Liv_Area           4.112e-03  1.962e-04   20.96  < 2e-16 ***\nfactor(Central_Air)Y  3.952e+00  5.180e-01    7.63 2.35e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1808.8  on 2048  degrees of freedom\nAIC: 1814.8\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.formula.api as smf\n\ntrain = r.train\nlogit_model_py = smf.logit('Bonus ~ Gr_Liv_Area + C(Central_Air)', data=train).fit()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOptimization terminated successfully.\n         Current function value: 0.440967\n         Iterations 8\n```\n:::\n\n```{.python .cell-code}\nlogit_model_py.summary()\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>Logit Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>Bonus</td>      <th>  No. Observations:  </th>   <td>  2051</td>  \n</tr>\n<tr>\n  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  2048</td>  \n</tr>\n<tr>\n  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n</tr>\n<tr>\n  <th>Date:</th>            <td>Tue, 05 Sep 2023</td> <th>  Pseudo R-squ.:     </th>   <td>0.3484</td>  \n</tr>\n<tr>\n  <th>Time:</th>                <td>14:15:03</td>     <th>  Log-Likelihood:    </th>  <td> -904.42</td> \n</tr>\n<tr>\n  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1387.9</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.064e-210</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n           <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>           <td>  -10.3546</td> <td>    0.642</td> <td>  -16.124</td> <td> 0.000</td> <td>  -11.613</td> <td>   -9.096</td>\n</tr>\n<tr>\n  <th>C(Central_Air)[T.Y]</th> <td>    3.9519</td> <td>    0.518</td> <td>    7.629</td> <td> 0.000</td> <td>    2.937</td> <td>    4.967</td>\n</tr>\n<tr>\n  <th>Gr_Liv_Area</th>         <td>    0.0041</td> <td>    0.000</td> <td>   20.959</td> <td> 0.000</td> <td>    0.004</td> <td>    0.004</td>\n</tr>\n</table>\n```\n:::\n:::\n\n:::\n\nNote that in the output the significance of the `Central_Air` factor is only saying that `Central_Air` is significantly different from the **reference** level (no central air).\n\n![Odds Ratio for Central Air](images/log-odds-interpretation.png)\n\nIn a business context, you don't have to necessarily drill down into the specifics of the odds ratio. Your goal is to convey the magnitude of the odds ratio to create a strategy.\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp(cbind(coef(logit_model), confint(logit_model)))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWaiting for profiling to be done...\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n                                         2.5 %       97.5 %\n(Intercept)          3.184558e-05 8.233966e-06 1.041852e-04\nGr_Liv_Area          1.004121e+00 1.003745e+00 1.004517e+00\nfactor(Central_Air)Y 5.203450e+01 2.058035e+01 1.620722e+02\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nodds_ratio = np.exp(logit_model_py.params)\nodds_ratio\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept               0.000032\nC(Central_Air)[T.Y]    52.034498\nGr_Liv_Area             1.004121\ndtype: float64\n```\n:::\n:::\n\n:::\n\n## Amount to Double the Odds\n\nWorking through math bckwards allows us to see what increase in square footage is needed for an expected doubling of odds:\n\n$$\n\\text{Double Odds} = \\frac{\\log(2)}{\\beta}\n$$\n\nAny odds ratio equal to 1 has no association. Lower than 1 means that the group in denominator has higher odds of the event. Greater than 1 means group in numerator has higher odds of the event.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}