{
  "hash": "6d0d61c36c0f8def2117ab004998a010",
  "result": {
    "markdown": "---\ntitle: Categorical Data Analysis\ndate: 07/17/2023\ndate-modified: 07/23/2023\n---\n\n\nRecall: categorical variables are data whose measurement scale is inherently categorical.\n\nTwo types: nominal and ordinal. Nominal has no logical ordering and ordianl has a logical ordering.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\nuse_condaenv(\"msa\")\names <- make_ordinal_ames()\n\names <- ames %>%\n    mutate(id = row_number())\n\ntrain <- ames %>%\n    sample_frac(0.7)\ntest <- anti_join(ames, train, by = \"id\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\n```\n:::\n\n\n# Examining Categorical Variables\n\nBy examining the distributions of categorical variables, you can\n\n-   Determine the frequencies of data values\n-   Recognize possible associations among variables\n\nAn association exists between two categorical variables if the dist. of one variable changes when the level of the other variable changes.\n\nIf no association exists, the distribution of the first variable is the same regardless of the level of the other variable.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train$Central_Air)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   N    Y \n 147 1904 \n```\n:::\n\n```{.r .cell-code}\nggplot(train) +\n    geom_bar(mapping = aes(x = Central_Air))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\ntable(train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n   0    1 \n1211  840 \n```\n:::\n\n```{.r .cell-code}\nggplot(train) +\n    geom_bar(mapping = aes(x = Bonus))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n\ntrain = r.train\n\ntrain[\"Bonus\"] == np.where(train[\"Sale_Price\"] > 175000, 1, 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0       True\n1       True\n2       True\n3       True\n4       True\n        ... \n2046    True\n2047    True\n2048    True\n2049    True\n2050    True\nName: Bonus, Length: 2051, dtype: bool\n```\n:::\n\n```{.python .cell-code}\ntrain[\"Bonus\"].value_counts()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBonus\n0.0    1211\n1.0     840\nName: count, dtype: int64\n```\n:::\n\n```{.python .cell-code}\nax = sns.countplot(x=\"Bonus\", data=train, color=\"Blue\")\nax.set(\n    xlabel=\"Bonus Eligible\", ylabel=\"Frequency\", title=\"Bar Graph of Bonus Eligibility\"\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n# Tests of Association\n\nHow much of a change in the distribution is required to believe there actually is a difference?\n\n::: text-center\n$H_0:$ There is no association\n\n$H_a:$ There is an association\n:::\n\n## $\\chi^2$ Distribution\n\nCharacteristics of the distribution:\n\n-   Bounded below by 0\n-   Right skewed\n-   One set of degrees of freedom\n\n$$\n\\chi_P^2 = \\sum_{i=1}^{R}\\sum_{j=1}^{C} \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}\n$$\n\n-   Summing over all the rows and columns and checking the difference between the observed frequencies and expected frequencies\n-   $d.f.$ equals (# Rows - 1)(# Columns - 1)\n\n![](images/expected-cell-counts.png)\n\nTo calculate expected cell counts, we take the proportion of column values and multiply them by the row totals to get that entry's expected count.\n\n## Pearson Chi-Square Test\n\nThink of Pearson Chi-Square Test as the categorical counterpart to Pearson correlation test with continuous variables.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's Chi-squared test with Yates' continuity correction\n\ndata:  table(train$Central_Air, train$Bonus)\nX-squared = 90.686, df = 1, p-value < 2.2e-16\n```\n:::\n:::\n\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import chi2_contingency\n\nchi2_contingency(pd.crosstab(train[\"Central_Air\"], train[\"Bonus\"]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nChi2ContingencyResult(statistic=90.6859058522855, pvalue=1.6838843977848939e-21, dof=1, expected_freq=array([[  86.79522184,   60.20477816],\n       [1124.20477816,  779.79522184]]))\n```\n:::\n:::\n\n\n## Likelihood Ratio Test\n\n$$\n\\chi_{LR}^2 = 2 \\cdot \\sum_{i=1}^{R}\\sum_{j=1}^{C} Obs_{i,j} \\cdot \\log(\\frac{Obs_{i,j}}{Exp_{i,j}})\n$$\n\n-   $d.f.$ equals (# Rows - 1)(# Columns - 1)\n\n## Assumptions\n\nBoth of the tests have a sample size requirement. The sample size requirement is **80% or more of the cells** in the cross-tabulation table need **expected** count larger than 5.\n\n## Fisher's Exact Test\n\nWhen we do not meet the assumption we use the **Fisher's exact test** that calculates all possible permutations of data.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfisher.test(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tFisher's Exact Test for Count Data\n\ndata:  table(train$Central_Air, train$Bonus)\np-value < 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n  9.213525 69.646380\nsample estimates:\nodds ratio \n  22.16545 \n```\n:::\n:::\n\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import fisher_exact\n\nfisher_exact(pd.crosstab(train[\"Central_Air\"], train[\"Bonus\"]))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSignificanceResult(statistic=22.18334892422825, pvalue=1.0365949500671678e-27)\n```\n:::\n:::\n\n\n## Ordinal Compared to Nominal\n\nPearson and Likelihood Ratio tests can handle any type of categorical variable. However, ordinal variables provide extra information due to order mattering.\n\nWe can test for ordinal variables against other ordinal variables with **Mantel-Haenszel Chi-Square Test**. This test checks whether two ordinal variables have a linear relationship as compared to just a general one.\n\nHowever, if you are comparing nominal to ordinal, you have to stick to a general test of association with Pearson's chi-square.\n\n## Mantel-Haenszel Chi-Square Test\n\n::: text-center\n$H_0:$ There is no linear association\n\n$H_a:$ There is a linear association\n:::\n\n$$\n\\chi_{MH}^2 = (n - 1)r^2\n$$\n\n-   $d.f.$ equals 1\n\nJust because you fail to reject in Mantel-Haenszel it **does not** mean that there is no association. There is just no linear association\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vcdExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: vcd\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: grid\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: gnm\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'vcdExtra'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following object is masked from 'package:dplyr':\n\n    summarise\n```\n:::\n\n```{.r .cell-code}\nCMHtest(table(train$Central_Air, train$Bonus))$table[1, ]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       Chisq           Df         Prob \n9.230619e+01 1.000000e+00 7.425180e-22 \n```\n:::\n:::\n\n\nOne thing to keep in mind about this function is that it orders the variables alphanumerically. For more than two categories in a variable you need to make sure that the values reflect the correct order (e.g. encoding strings as ordered numerics).\n\n# Measures of Association\n\nChi-square tests can determine whether an association exists. They cannot measure the strength of the association.\n\nCommon measures of association:\n\n-   Odds Ratios (Only for 2x2 tables)\n-   Cramer's V (Any size table)\n-   Spearman's Correlation (Ordinal vs. ordinal)\n\n## Odds Ratios\n\n<!-- TODO: Add example of odds ratio and explaining why interpretation reverse is also true -->\n\nOdds ratios indicates how much more likely, with respect to odds, a certain event occurs in one group relative to its occurrence in another group.\n\n**Odds** of an event occurring is not the same as probability.\n\n$$\n\\text{Odds} = \\frac{p}{1 - p}\n$$\n\n## Cramer's V\n\n$$\nV = \\sqrt{\\frac{(\\frac{\\chi_P^2}{n})}{\\min(\\text{Rows} - 1, \\text{Columns} - 1)}}\n$$\n\n-   Bounded between 0 and 1 (-1 and 1 for 2x2) where closer to 0 the weaker the relationship\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vcd)\nassocstats(table(train$Central_Air, train$Bonus))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                     X^2 df P(> X^2)\nLikelihood Ratio 121.499  1        0\nPearson           92.351  1        0\n\nPhi-Coefficient   : 0.212 \nContingency Coeff.: 0.208 \nCramer's V        : 0.212 \n```\n:::\n:::\n\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats.contingency import association\n\nassociation(pd.crosstab(train['Central_Air'], train['Bonus']), method = 'cramer')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.21219662657018892\n```\n:::\n:::\n\n\n## Spearman's Correlation\n\nMeasures the strength of association between two ordinal variables. Calculated with Pearson's correlation on the ranks of the observations instead of the values of the observations.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(\n    x = as.numeric(ordered(train$Central_Air)),\n    y = as.numeric(ordered(train$Bonus)),\n    method = \"spearman\"\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in cor.test.default(x = as.numeric(ordered(train$Central_Air)), :\nCannot compute exact p-value with ties\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tSpearman's rank correlation rho\n\ndata:  as.numeric(ordered(train$Central_Air)) and as.numeric(ordered(train$Bonus))\nS = 1132826666, p-value < 2.2e-16\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.2121966 \n```\n:::\n:::\n\n\n-   As you increase the number of `Central_Air` the number of `Bonus` increases\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.stats import spearmanr\n\nspearmanr(train['Central_Air'], train['Bonus'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSignificanceResult(statistic=0.21219662657018892, pvalue=2.604243816433701e-22)\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}