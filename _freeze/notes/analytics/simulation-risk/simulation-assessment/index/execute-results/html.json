{
  "hash": "0189e645be3634967f5553cc955eb1c6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Theory and Model Assessment Through Simulation\ndate: 02/05/2024\ndate-modified: 02/12/2024\n---\n\n\n# Theory Assessment\n\nA **closed form solution** to a mathematical / statistical distribution problem means tha tyou can mathematicaly calculate the distribution.\n\nReal world data can be very complicated and changing based on many different inputs which each have their own distribution. Simulation can reveal and approximation of these output distributions.\n\n## Example: Central Limit Theorem\n\nAssume you do not know the Central Limit Theorem, but you want to understand the sampling distribution of sample means. You take samples of size 10, 50, 100 from the following three population distributions and calculate the sample means:\n\n1.  Normal Distribution\n2.  Uniform Distribution\n3.  Exponential Distribution\n\nWhat is the sampling distribution of sample means from each of these distributions and sample sizes?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_size <- 10\nsim_size <- 10000\n\nX1 <- matrix(\n    data = rnorm(n = sample_size * sim_size, mean = 2, sd = 5),\n    nrow = sim_size,\n    ncol = sample_size,\n    byrow = TRUE,\n)\nX2 <- matrix(\n    data = runif(n = sample_size * sim_size, min = 5, max = 105),\n    nrow = sim_size,\n    ncol = sample_size,\n    byrow = TRUE,\n)\nX3 <- matrix(\n    data = (rexp(n = sample_size * sim_size) + 3),\n    nrow = sim_size,\n    ncol = sample_size,\n    byrow = TRUE,\n)\n\nmean_X1 <- apply(X1, 1, mean)\nmean_X2 <- apply(X2, 1, mean)\nmean_X3 <- apply(X3, 1, mean)\n```\n:::\n\n\n## Omitted Variable Bias\n\nWhat if you leave out a variable in a linear regressio that should have been in the model? From the primer, we learned that it would change the variance and bias of the coefficients in the model depending on if the variable left out was correlated.\n\nWhat if you wanted to know how bad it could get?\n\n### Process\n\nBuild the following regression model:\n\n$$\nY = -13 + 1.21X_1 + 3.45X_2 + \\epsilon\n$$\n\n-   Assume errors are Normal with mean 0 and standard deviation 1.5\n-   Assume the predictors follow standard Normal distributions\n\nBuild 10,000 linear regressions (each of sample size 50) and record the coefficients from the regression model when one of the variables is omitted.\n\nLook at the following:\n\n1.  Distribution of coefficient in the model\n    1.  What if the omitted variable isn't correlated with the others? **Unbiased, more variance**\n    2.  What if the omitted variable is correlated with the others? **Biased, more variance**\n2.  How many times did you incorrectly NOT reject the null hypothesis on the coefficient in each of these scenarios?\n\n![Omitted Variable Bias](images/omitted-var.png){#fig-omitted-var}\n\nIn @fig-omitted-var, the second chart shows that $X_1$ is trying to account for the information that $X_2$ had since $X_2$ was dropped. The third chart shows that $X_1$ has a wider standard error which now covers 0--$X_1$ might not be as important to the model after all!\n\n![Omitted Variable Bias 2](images/omitted-var-2.png){#fig-omitted-var-2}\n\nIn @fig-omitted-var-2, the third chart shows that we are now more likely to incorrectly fail to reject the null hypothesis on the $X_1$ coefficient.\n\n# Target Shuffling\n\n**Target shuffling** is when you randomly reorder the target variable values among the sample, while keeping the predictor variable values fixed.\n\nWe are breaking the relationship between our response and our fixed predictors. When we shuffle our data, we want to see if the random model performs better than our original model. If there are many cases where the random model performs better, then we might question how good our original model is.\n\nBuild a model from each of the reshuffled targets and record some measurement of model success ($R_A^2$, c, MAPE, etc.). This should remove the pattern from the data, but some pattern **may exist due to randomness**. Look at distribution of all measurements of model success and find your value from the true model!\n\n![Target Shuffling](images/target-shuffling.png){#fig-target-shuffling}\n\nIf your true model line ends up in the middle, this is typically indicative of a data problem rather than a model issue.\n\n## Example: Fake Data\n\nRandomly generated 8 variables that follow a Normal distribution with mean of 0 and standard deviation of 8.\n\nDefined relationship with target variable:\n\n$$\ny = 5 + 2x_2 - 3x_8 + \\epsilon\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfake <- data.frame(matrix(rnorm(n = 100 * 8), nrow = 100, ncol = 8)) # Create 8 columns for 8 different predictors\nerr <- rnorm(n = 100, mean = 0, sd = 8) # Create 100 error terms for each observation we have\nY <- 5 + 2 * fake$X2 - 3 * fake$X8 + err # Establish the response variable\nfake <- cbind(fake, err, Y)\n\nsim <- 1000\n\nY_shuffle <- matrix(0, nrow = 100, ncol = sim) # Establish our shuffled target simulations\n\n# For each simulation, shuffle the indices of the actual Y's and assign them to the columns of Y_shuffle\nfor (i in 1:sim) {\n    uniform <- runif(100)\n    Y_shuffle[, i] <- Y[order(uniform)]\n}\n\nY_shuffle <- data.frame(Y_shuffle)\ncolnames(Y_shuffle) <- paste(\"Y.\", seq(1:sim), sep = \"\")\n\nfake <- data.frame(fake, Y_shuffle)\n\nr_sq_a <- rep(0, sim)\nfor (i in 1:sim) {\n    r_sq_a[i] <- summary(lm(fake[, 10 + i] ~ fake$X1 + fake$X2 + fake$X3 + fake$X4 + fake$X5 + fake$X6 + fake$X7 + fake$X8))$adj.r.squared\n}\ntrue_rsq_a <- summary(lm(fake$Y ~ fake$X1 + fake$X2 + fake$X3 + fake$X4 + fake$X5 + fake$X6 + fake$X7 + fake$X8))$adj.r.squared\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(r_sq_a, true_rsq_a, breaks = 50, col = \"blue\", main = \"Distribution of Adjusted R-Squared Values\", xlab = \"Adjusted R-Squared\")\nabline(v = true_rsq_a, col = \"red\", lwd = 2)\nmtext(\"True Model\", at = true_rsq_a, col = \"red\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nTarget shuffling is meant to be done on an entire process, not just the final model. You want every variable to have a chance so this means target shuffling on the train-test split, every model considered, etc. You should automate this process to do this. At the very least, run target shuffling on the model selection process.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}