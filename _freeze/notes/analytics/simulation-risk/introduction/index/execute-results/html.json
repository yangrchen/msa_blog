{
  "hash": "2c9dce90e7824a2a74d299691d869969",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Main Concepts of Simulation\ndate: 01/29/2024\ndate-modified: 02/02/2024\n---\n\n\n# Varying Inputs\n\nUp until this point we have been assuming an unrealistic view of the real world--certainty. Inputs and coefficients in a problem are rarely fixed quantities in the real world.\n\n# Monte Carlo Simulations\n\n**Simulations** help us determine not only the full array of outcomes of a given decision, but the probabilities of these outcomes occurring.\n\n![Monte Carlo Simulation](images/monte-carlo-simulations.png){#fig-monte-carlo}\n\n## What-If Analysis\n\nEach input inside of a model is assigned a range of possible values--**probability distribution of the inputs**.\n\nWe analyze what happens to the decision from our model under all of these possible scenarios. Simulation analysis describes not only the outcomes of certain decisions, but also the probability distribution of those outcomes.\n\n## Outcome Distribution\n\nAfter the simulation analysis, the focus then turns to the probability distribution of the outcomes.\n\nDescribe the characteristics of this new distribtuion--mean, variance, skewness, kurtosis, percentiles, etc.\n\n## Selecting Distributions\n\nWhen designing your simulations the biggest choice comes from the decision of the distribution on the inputs that vary.\n\n1.  Common Probability Distribution\n2.  Historical (Empirical) Distribution\n3.  Hypothesized Future Distribution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- rnorm(n = 10000, mean = 0.0879, sd = 0.1475)\nP0 <- 1000\nP1 <- P0 * (1 + r)\n\nhist(P1, breaks = 50, main = \"One Year Value Distribution\", xlab = \"Final Value\")\nabline(v = 1000, col = \"red\", lwd = 2)\nmtext(\"Initial Inv.\", at = 1000, cl = \"red\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in mtext(\"Initial Inv.\", at = 1000, cl = \"red\"): \"cl\" is not a\ngraphical parameter\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n# Distribution Selection\n\n## Common Probability Distributions\n\nCommon discrete distributions:\n\n1.  Uniform Distribution\n2.  Poisson Distribution\n\nCommon continuous distributions:\n\n1.  Continuous Uniform Distribution\n2.  Triangular Distribution\n3.  Student's t-Distribution\n4.  Lognormal Distribution\n5.  Normal Distribution\n6.  Exponential Distribution\n7.  Chi-Square Distribution\n8.  Beta Distribution\n\nThere is also a triangle distribution that takes three parameters:\n\n-   What do you think is the worst outcome?\n-   What do you expect to happen?\n-   What do you think is the best outcome?\n\n## Historical (Empirical) Distributions\n\nIf you are unsure of the distribution of the data you are trying to simulate, you can estimate is using **kernel density estimation**.\n\nKernel density is a non-parametric method of estimating distributions of data through smoothing our of data values.\n\n$$\n\\hat{f}(x) = \\frac{1}{nh}\\sum_{i=1}^{n} \\kappa \\left( \\frac{x - x_i}{h} \\right)\n$$\n\n-   $h$ is the bandwidth\n\nIf we use a Normal kernel, then we essentially create a Normal distribution around the density of each point and we sum up the distributions to build the density curve. Using KDE, we build variability around our data to fill out the possible other values of our distribution.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(quantmod)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: xts\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: TTR\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n\n\n:::\n\n```{.r .cell-code}\ntickers <- \"^GSPC\"\n\n# Retrieves the stock data associated with the tickers\ngetSymbols(tickers)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"GSPC\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate yearly period returns on the closing price\ngspc_r <- periodReturn(GSPC$GSPC.Close, period = \"yearly\")\n\n# Plot the distribution of the returns\nhist(gspc_r, main = \"Historical S&P500\", xlab = \"S&P500 Annual Returns\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the kernel density along with the optimal bandwidth value\ndensity_GSPC <- density(gspc_r)\ndensity_GSPC\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\n\tdensity.default(x = gspc_r)\n\nData: gspc_r (18 obs.);\tBandwidth 'bw' = 0.08189\n\n       x                  y           \n Min.   :-0.63052   Min.   :0.003044  \n 1st Qu.:-0.33747   1st Qu.:0.204250  \n Median :-0.04442   Median :0.422917  \n Mean   :-0.04442   Mean   :0.852074  \n 3rd Qu.: 0.24862   3rd Qu.:1.643093  \n Max.   : 0.54167   Max.   :2.304845  \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ks)\n\n# Random kernel density estimation, with n = 1000 sample size\nest_GSPC <- rkde(fhat = kde(gspc_r, h = density_GSPC$bw), n = 1000)\nhist(est_GSPC, breaks = 50, main = \"KDE of Historical S&P500\", xlab = \"S&P500 Annual Returns\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- est_GSPC\nP0 <- 1000\nP1 <- P0 * (1 + r)\n```\n:::\n\n\nOnce you have the Kernel density function, you can sample from this density function. \n\n1.  If you have large sample sizes, your bandwidth can be smaller and your estimates more accurate.\n2.  If you have small sample sizes, your bandwidth increases and estimates are more smoothed.\n\n## Hypothesized Future Distribution\n\nYou might know of an upcoming change that will occur so that the past information is not going to be the future distribution.\n\nExample: The volatility of the market is forecasted to increase, so instead of a standard deviation of 14.75% it is 18.25%.\n\nIn these situations, you select any distribution of choice.\n\n# Compounding and Correlations\n\nComplication arises when you are simulating multiple inputs changing at the same time. Even when the distributions of these inputs are the same, final result cna still be hard to mathematically calculate--benefit of simulation.\n\n1.  When a constant is added to a random variable then the distribution is the same, only shifted by the constant.\n2.  The addition of many distributions that are the same is rarely the same shape of distribution--exception would be **independent** Normal distributions.\n3.  The product of many distributions that are the same is rarely the same shape of distribution--exception would be **independent** lognormal distributions.\n\n## Example\n\nYou want to invest $1,000 in the US stock market for thirty years. You invest in a mutual fund that tries to produce the same return as the S&P500 Index.\n\n$$\nP_t = P_0 * (1 + r_{0,1})(1 + r{1,2})(1 + r_{2,3})\\cdots(1 + r_{t-1,t})\n$$\n\n-   Assume annual returns follow a Normal distribution with historical mean of 8.79% and std. dev of 14.75% every year.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create an initial vector that can store 10000 simulations\nP30 <- rep(0, 10000)\nfor (i in 1:10000) {\n    # Start with our initial $1,000 investment\n    P0 <- 1000\n\n    # Sample a rate from the random Normal distribution\n    r <- rnorm(n = 1, mean = 0.0879, sd = 0.1475)\n\n    Pt <- P0 * (1 + r)\n\n    # For the next 30 years, draw from a random Normal and calculate the returns\n    for (j in 1:29) {\n        r <- rnorm(n = 1, mean = 0.0879, sd = 0.1475)\n        Pt <- Pt * (1 + r)\n    }\n    P30[i] <- Pt\n}\n\nhist(P30, breaks = 50, main = \"30 Year Value Distribution\", xlab = \"Final Value\")\nabline(v = 1000, col = \"red\", lwd = 2)\nmtext(\"Initial Inv.\", at = 1000)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Correlated Inputs\n\nNot all inputs are independent of each other. Need to simulate random variables that have correlation with each other.\n\n### Example\n\nYou want to invest $1,000 in the US stock market or US Treasury bond for thirty years. You invest a certain percentage in a mutual fund that tries to produce the same return as the S&P500 Index and the rest in US Treasury bonds.\n\nTreasury bonds perceived as safer investment so when stock market does poorly more people invest in bonds--negatively correlated.\n\n$$\n\\begin{align*}\n&P_{t,S} = P_{0,S} * (1 + r_{0,1})(1 + r_{1,2})(1 + r_{2,3})\\cdots(1 + r_{t-1,t}) \\\\\n&P_{t,B} = P_{0,B} * (1 + r_{0,1})(1 + r_{1,2})(1 + r_{2,3})\\cdots(1 + r_{t-1,t}) \\\\\n&P_t = P_{t,S} + P_{t,B}\n\\end{align*}\n$$\n\n-   Assume mutual fund N(8.79%, 14.75%)\n-   Assume Treasury Bond Normal(4.00%, 7.00%)\n-   Assume correlation of -0.2\n\nIn this example, the correlation is Pearson's correlation between the two variables.\n\nOne way to \"add\" correlation to data is to multiple correlation into the data through matrix multiplication.\n\n![Adding Correlation](images/adding-correlation.png){#fig-adding-correlation}\n\nFor multiple variables at the same time, we can use the variance matrix:\n\n![Adding Correlation Multiple](images/adding-correlation-multi.png){#fig-adding-correlation-multi}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvalue_r <- rep(0, 10000)\nR <- matrix(data = cbind(1, -0.2, -0.2, 1), nrow = 2)\nU <- t(chol(R))\nperc_B <- 0.5\nperc_S <- 0.5\ninitial <- 1000\n\nstandardize <- function(x) {\n    x.std <- (x - mean(x)) / sd(x)\n    return(x.std)\n}\n\ndestandardize <- function(x.std, x) {\n    x.old <- (x.std * sd(x)) + mean(x)\n    return(x.old)\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfor (j in 1:10000) {\n    S_r <- rnorm(n = 30, mean = 0.0879, sd = 0.1475)\n    B_r <- rnorm(n = 30, mean = 0.04, sd = 0.07)\n    both_r <- cbind(standardize(S_r), standardize(B_r))\n\n    # Matrix multiply our decomposition with the standardized columns\n    SB_r <- U %*% t(both_r)\n    SB_r <- t(SB_r)\n\n    final_SB_r <- cbind(destandardize(SB_r[, 1], S_r), destandardize(SB_r[, 2], B_r))\n\n    Pt_B <- initial * perc_B\n    Pt_S <- initial * perc_S\n\n    for (i in 1:30) {\n        Pt_B <- Pt_B * (1 + final_SB_r[i, 2])\n        Pt_S <- Pt_S * (1 + final_SB_r[i, 1])\n    }\n    value_r[j] <- Pt_B + Pt_S\n}\n```\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}