{
  "hash": "d7da31cd9efb46735c6fbe18bca28a3b",
  "result": {
    "markdown": "---\ntitle: Seasonality Models\ndate: 09/27/2023\ndate-modified: 10/06/2023\n---\n\n\n# Setup {.unnumbered}\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(aTSA)\nlibrary(tseries)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\nusairlines <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv\")\npassenger <- ts(usairlines$Passengers, start = 1990, frequency = 12)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sma\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\n\nusair = r.usairlines\ndf = pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')\nusair.index = pd.to_datetime(df)\n\ntrain = usair.head(207)\ntest = usair.tail(12)\n```\n:::\n\n\n:::\n\n# Review\n\n## Exponential Smoothing Model Forecasts\n\nIn the short term, exponential smoothing models can be great at forecasting one-step ahead. For seasonal models, the one-step ahead is a whole season ahead.\n\n## Stationarity\n\nStationary means that any time could be the mean. Eventually our data converges to a mean.\n\nWe need consistency of mean and variance. If there are significant changes in mean (trending) or seasonality then the data is **NOT** stationary.\n\n## ARIMA Models\n\nAR is forecasting a series based on the past values in the series--called **lags**. With AR terms, the actual past observations inform the prediction in the next time period.\n\nMA is forecasting a series based solely on past errors--called **error lags**. With MA terms, the mistake you made in the last observation informs what your prediction is in the next time period.\n\nTypically, we write ARIMA as:\n\n$$\n\\text{ARIMA}(p, d, q)\n$$\n\n-   $p$ is the number of AR terms\n-   $d$ is the number of first differences\n-   $q$ is the number of MA terms\n\n## Seasonality\n\nThere is no formal test for seasonality. You have to plot your overall data to see if you believe seasonality exists.\n\n# Using the US Passengers Data\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- subset(passenger, end = length(passenger) - 12)\ntest <- subset(passenger, start = length(passenger) - 11)\n\ndecomp_stl <- stl(train, s.window = 7)\nplot(decomp_stl)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/train-test-split-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n\n:::\n\nTo decide how large our test set should be, we use the length of forecast we are interested in. In this case, we are interested in the next year forecast so the length of test will be 12 months.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhwes_usair_train <- hw(train, seasonal = \"multiplicative\", initial = \"optimal\", h = 12)\nautoplot(hwes_usair_train) +\n    autolayer(fitted(hwes_usair_train), series = \"fitted\") +\n    ylab(\"Airlines Passengers\") +\n    geom_vline(xintercept = 2007.25, color = \"orange\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/holt-winters-model-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhw_error <- test - hwes_usair_train$mean\n\nhw_mae <- mean(abs(hw_error))\nhw_mape <- mean(abs(hw_error) / abs(test)) * 100\n```\n:::\n\n\nIs this a good MAPE? We don't know, but exponential smoothing models make a good baseline to compare other models to.\n\n# Python\n\n:::\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n# Python\n\n:::\n\n# Seasonality\n\n**Seasonality** is the component of time series that reprsents the effects of seasonal variation. However, just because you have something that repeats every year (e.g. chocolate sales in February) does not mean that it is seasonal. A **seasonal period ($S$)** is the length of time that the season occurs. For monthly data, $S = 12$.\n\nSeasonal data means that no matter where you are in a season, that seasonal wave will repeat itself.\n\nBy defiinition, seasonal data is not stationary. Mathematically, stationarity is if you take any window of time to your data, the average stays the same. See @fig-stationary-windows for an example.\n\n![Seasonality is Not Stationary](images/stationary-windows.png){#fig-stationary-windows}\n\n# Seasonal ARIMA Models\n\nSimilar to trend, seasonality can be solved with deterministic or stochastic solutions.\n\n**Deterministic** uses seasonal dummy variables, Fourier transforms, and predictor variables. **Stochastic** uses seasonal differences. We always have to make our data stationary first before modeling.\n\n# Seasonal Unit-Root Testing\n\nWe can perform the **Canova-Hansen** test to evaluate whether a unit root exists for seasonal data.\n\n:::{.text-center}\n$H_0$: Deterministic Seasonality (Differencing will not help)\n\n$H_a$: Stochastic Seasonality (Differencing needed)\n:::\n\nNo good formal tests for seasons beyond 24 time periods. Frequency is only a notion of how many times the data should \"repeat\" itself. Every time we take a difference we essentially create a whole new distribution. Taking one difference and taking one difference of up to 12 lags are not similar.\n\nIf we face this situation, then we should try both paths and see which model predicts better.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>% nsdiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(\"Airlines Passengers\" = train, \"Annual change in Passengers\" = diff(train, 12)) %>%\n    autoplot(facets = TRUE) +\n    labs(x = \"Time\", y = \"\") +\n    ggtitle(\"Comparison of Difference Data to Original\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotting-seasonal-diff-1.png){width=672}\n:::\n:::\n\n\nThis result tells us that we should take one **seasonal** difference. After we take our seasonal difference (12 lags) then we need to check for **regular** differences afterwards.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>%\n    diff(lag = 12) %>%\n    ndiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nThis result tells us that we should take 0 regular differences after taking the seasonal difference. No trend, no seasonality, so we believe our data is now stationary.\n\n# Python\n\n:::\n\n# Deterministic Solutions\n\n## Seasonal Dummy Variables\n\nFor a time series with $S$ periods within a season, there will be **S - 1** dummy variables, one for each period (and one accounted for with the intercept).\n\n-   Monthly Data\n    -   One dummy variable for each month (S = 12)\n-   Weekly Data\n    -   One dummy variable for each day of week (S = 7)\n-   Hourly Data\n    -   One dummy variable for each hour (S = 24)\n\n### Example Model\n\n$$\nY_t = \\beta_0 + \\beta_1JAN + \\beta_2FEB + \\cdots + \\beta_11NOV + e_t\n$$\n\n-   $\\beta_0 + \\beta_M$ is the effect of the $M$th month\n-   $\\beta_0$ is the effect of December\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmonth <- rep(0, length(train))\nmonth <- month + 1:12\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in month + 1:12: longer object length is not a multiple of shorter\nobject length\n```\n:::\n\n```{.r .cell-code}\nM <- factor(month)\nM <- relevel(M, ref = \"12\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nseason_lm <- lm(train ~ M)\nsummary(season_lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = train ~ M)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-16257.2  -6848.9    484.4   6368.3  14699.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  48761.5     2002.3  24.353   <2e-16 ***\nM1           -4461.9     2792.1  -1.598   0.1116    \nM2           -5433.9     2792.1  -1.946   0.0531 .  \nM3            4099.8     2792.1   1.468   0.1436    \nM4             814.9     2831.7   0.288   0.7738    \nM5            1951.8     2831.7   0.689   0.4915    \nM6            4844.6     2831.7   1.711   0.0887 .  \nM7            7504.9     2831.7   2.650   0.0087 ** \nM8            7297.4     2831.7   2.577   0.0107 *  \nM9           -3242.5     2831.7  -1.145   0.2536    \nM10           1064.1     2831.7   0.376   0.7075    \nM11          -1268.2     2831.7  -0.448   0.6548    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8256 on 195 degrees of freedom\nMultiple R-squared:  0.2097,\tAdjusted R-squared:  0.1651 \nF-statistic: 4.703 on 11 and 195 DF,  p-value: 2.227e-06\n```\n:::\n:::\n\n\nWe can model with a deterministic solution by supplying a model matrix of our dummy-encoded months. `seasonal = FALSE` because we are trying to account for seasonality manually with our deterministic approach.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm_matrix <- model.matrix(~M)\ntrend <- 1:length(train)\n\nsd_arima <- auto.arima(train, xreg = m_matrix[, 2:12], method = \"ML\", seasonal = FALSE)\nsummary(sd_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: train \nRegression with ARIMA(1,1,1) errors \n\nCoefficients:\n         ar1      ma1     drift          M1          M2         M3         M4\n      0.4291  -0.7971  120.7017  -3947.9999  -5040.3341  4372.9579  1776.4469\ns.e.  0.1142   0.0772   47.0804    485.1759    583.2678   625.8855   653.3705\n             M5         M6         M7         M8          M9        M10\n      2774.4117  5538.2230  8074.3636  7745.4694  -2914.0941  1275.6541\ns.e.   664.7051   667.9359   665.0342   655.1107    633.9315   589.7377\n             M11\n      -1168.3771\ns.e.    486.9793\n\nsigma^2 = 3751113:  log likelihood = -1844.41\nAIC=3718.82   AICc=3721.34   BIC=3768.74\n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set -13.57427 1865.286 1119.092 -0.1821873 2.399031 0.4156612\n                     ACF1\nTraining set -0.002821366\n```\n:::\n:::\n\n\nThe `auto.arima` function built a linear regression and then an ARIMA(1, 1, 1) on the residuals from the linear regression.\n\n# Python\n\n:::\n\n### Advantages and Disadvantages\n\nAdvantages:\n\n-   Interpretability on the effects of different parts of the season\n-   Straight forward to implement\n\nDisadvantages:\n\n-   Especially long or complex seasons are hard to deal with\n    -   More than 24 periods in a season is computationally burdensome\n    -   Some seasons are complex (365.25 days in a year, 52.17 weeks in a year)\n-  Seasonal effects remain constant \n\n## Fourier Transforms\n\nFourier showed that series of **sine and cosine terms** of the right frequencies approximate periodic series.\n\n![Periodic Series](images/fourier-waves.png){#fig-periodic-series}\n\nThe idea is that we add Fourier variables to a regression model predicting the target to remove the seasonal pattern.\n\n![Fourier Variables](images/fourier-variables.png){#fig-fourier-variables}\n\nIn @fig-fourier-variables, $S$ is the frequency or length of the season. The multiplier refers to how many times we want the curves to repeat per season. We are going to continue adding *pairs* of cosine and sine terms until we have one half of a season. With odd-number frequencies, we go up to $\\text{floor}(\\frac{S}{2})$ terms.\n\nIf you add the same number of Fourier variables as you have seasonal dummy variables, you will get the same predictions. However, typically do not need all the Fourier variables, especially with large values of $S$.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplots <- list()\nfor (i in seq(6)) {\n    fit <- auto.arima(train,\n        xreg = fourier(train, K = i),\n        seasonal = FALSE, lambda = NULL\n    )\n\n    plots[[i]] <- autoplot(forecast(fit, xreg = fourier(train, K = i, h = 12))) +\n        xlab(paste(\"K=\", i, \"   BIC=\", round(fit[[\"bic\"]], 2))) +\n        ylab(\"\") +\n        ylim(30000, 80000)\n}\ngridExtra::grid.arrange(\n    plots[[1]], plots[[2]], plots[[3]],\n    plots[[4]], plots[[5]], plots[[6]],\n    nrow = 3\n)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe $K$ parameter controls how many *pairs* of Fourier terms we are adding. We can record the BIC values to see how many pairs of terms provides our best model. If the best number of pairs is $\\frac{S}{2}$, then one term in the last pair will be left out which is the intercept.\n\n:::\n\n### Advantages and Disadvantages\n\nAdvantages:\n\n-   Can handle long and complex seasonality\n    -   If multiple seasons, just add more Fourier variables to account for them\n\nDisadvantages:\n\n-   Trial and error for \"right\" amount of Fourier variables to use\n-   No interpretable value\n-   Effect of season remains constant\n\n## Predictor Variables for Seasonality\n\nLast approach to account for seasonality in data is to use other predictor variables that have matching season.\n\nModeling these variables against the target might remove seasonality.\n\n### Advantages and Disadvantages\n\nAdvantages:\n\n-   Can handle long and complex seasonality\n    -   If multiple seasons, just add more variables to account for them\n-   Interpretation still holds\n    -   Can easily measure and interpret effects from these variables\n\nDisadvantages:\n\n-   Trial and error for \"right\" variables to use\n-   Might not have predictor variables to use in this context\n\n# Modeling Residuals\n\nAfter removing seasonality through deterministic approaches, the residuals are modeled with Seasonal ARIMA models. We still might need seasonal effects even though season is removed.\n\nIf you have data that you believe might show trend AND seasonality. You should address seasonality first as it might take care of trend. Addressing trend first will never address seasonality as well.\n\n# Stochastic Solution (Differencing)\n\nWith seasonal differences, you are taking the difference between some observation $Y_t$ and the observation one season prior $Y_{t-S}$.\n\nAfter removing the seasonality through stochastic approaches, the remaining differences are modeled with Seasonal ARIMA models:\n\n$$\nY_t - Y_{t-S} = \\textcolor{red}{W_t}\n$$\n\n-   $W_t$ is modeled with Seasonal ARIMA\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>%\n    diff(lag = 12) %>%\n    ggtsdisplay()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nOur seasonal length, $S$, is 12 so we take a difference with `lag = 12`. In practice, we want at least 3 seasons in our data because differencing completely removes the first season.\n\n## Limitations\n\nHard to evaluate stochastic effects for long and complex seasons. Most statistical tests cannot evaluate seasons past 12 or 24 time periods.\n\nFor long and complex seasons it's best to approach them with deterministic solutions.\n\n# Seasonal ARIMA\n\n$$\n\\text{ARIMA}(p, d, q)(P, D, Q)_S\n$$\n\nSeasonal ARIMA has an extra set of terms: $P, D, Q$ and $S$. $S$ represents the length of the season so if we take any seasonal differences we are taking the difference between the current observations and the observations $S$ time periods back. See @fig-seasonal-ARIMA-example for an example of a seasonal $\\text{ARIMA}(1, 0, 1)(2, 1, 0)_{12}$\n\n![Seasonal ARIMA Example](images/seasonal-ARIMA-notation.png){#fig-seasonal-ARIMA-example}\n\nSeasonal ARIMA models have the same structure and approach as regular ARIMA with AR and MA patterns in the PACF and ACF.\n\nThe pattern is on the **seasonal lag** instead of the individual lags.\n\n:::{layout-ncol=2}\n![ARIMA(0, 0, 0)(0, 0, 1), S = 12 Plots](images/s-arima-pacf.png){#fig-Q1}\n\n![ARIMA(0, 0, 0)(1, 0, 0), S = 12 Plots](images/s-arima-acf.png){#fig-P1}\n:::\n\nIn order to determine P, D, Q terms, look for spikes at the $S$ lag term. Figuring out p, d, q terms is the same as the regular ARIMA.\n\nNote that in the `Arima` function, the length of seasonality is inferred from the `ts` object we supply to it.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>%\n    Arima(order = c(1, 0, 0), seasonal = c(1, 1, 1)) %>%\n    residuals() %>%\n    ggtsdisplay()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ns_arima <- Arima(train, order = c(1, 0, 0), seasonal = c(1, 1, 1))\nsummary(s_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: train \nARIMA(1,0,0)(1,1,1)[12] \n\nCoefficients:\n         ar1    sar1    sma1\n      0.9056  0.0917  -0.672\ns.e.  0.0364  0.1091   0.093\n\nsigma^2 = 4126436:  log likelihood = -1763.94\nAIC=3535.88   AICc=3536.09   BIC=3548.97\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE      MASE       ACF1\nTraining set 338.6503 1956.379 1156.221 0.5565257 2.418163 0.4294517 -0.2622466\n```\n:::\n\n```{.r .cell-code}\n# Check residuals provides us the Ljung-Box test\ncheckresiduals(s_arima)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,0)(1,1,1)[12]\nQ* = 45.934, df = 21, p-value = 0.001304\n\nModel df: 3.   Total lags used: 24\n```\n:::\n:::\n\n\nWe can also use `auto.arima` to check for an automated seasonal ARIMA:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns_arima <- auto.arima(train, method = \"ML\", seasonal = TRUE)\nsummary(s_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: train \nARIMA(1,0,1)(0,1,1)[12] with drift \n\nCoefficients:\n         ar1      ma1     sma1     drift\n      0.8800  -0.2962  -0.6785  124.9788\ns.e.  0.0454   0.0950   0.0600   23.6330\n\nsigma^2 = 3639517:  log likelihood = -1751.67\nAIC=3513.34   AICc=3513.66   BIC=3529.7\n\nTraining set error measures:\n                    ME    RMSE     MAE        MPE     MAPE      MASE       ACF1\nTraining set -4.332616 1832.54 1055.07 -0.1745474 2.217472 0.3918815 0.01300462\n```\n:::\n\n```{.r .cell-code}\ncheckresiduals(s_arima)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tLjung-Box test\n\ndata:  Residuals from ARIMA(1,0,1)(0,1,1)[12] with drift\nQ* = 21.957, df = 21, p-value = 0.402\n\nModel df: 3.   Total lags used: 24\n```\n:::\n:::\n\n\n## Multiple Differences\n\nModels can contain both unit roots and seasonal unit roots. After removing the seasonal unit root through differencing to get $W_t$, ordinary differences can be calculated.\n\n### Limitations\n\n-   Hard to evaluate stochastic effects for long and complex seasons\n-   Most statistical tetss for stochastic vs. deterministic can not handle past 12 or 24 periods in season\n-   For long / complex seasons, best to approach with deterministic solutions\n\n# Multiplicative vs. Additive\n\n![Multiplicative vs. Additive](images/mult-vs-add.png){#fig-mult-vs-add}\n\n## Backshift Operator (B)\n\nThe backshift operator is the mathematical operator to convert observations to their lags:\n\n$$\n\\begin{align*}\n\\text{B}(Y_t) &= Y_{t-1} \\\\\n\\text{B}^2(Y_t) &= \\text{B}(Y_{t-1}) = Y_{t-2}\n\\end{align*}\n$$\n\n## Structures to Seasons\n\nAdditive:\n\n$$\n\\begin{align*}\n(1 - \\alpha_1\\text{B} - \\alpha_2\\text{B}^{12})Y_t &= e_t \\\\\nY_t - \\alpha_1\\text{B}(Y_t) - \\alpha_2\\text{B}^{12}(Y_t) &= e_t \\\\\nY_t - \\alpha_1Y_{t-1} - \\alpha_2Y_{t-12} = e_t\n\\end{align*}\n$$\n\nMultiplicative:\n\n$$\n\\begin{align*}\n(1 - \\alpha_1\\text{B})(1 - \\alpha_2\\text{B}^{12})Y_t &= e_t \\\\\n(1 - \\alpha_1\\text{B} - \\alpha_2\\text{B}^{12} + \\alpha_1\\alpha_2\\text{B}^{13})Y_t &= e_t\n\\end{align*}\n$$\n\nWhat do the correlation plots look like for additive vs. multiplicative seasonality?\n\n![Multiplicative vs. Additive Correlation Plots](images/mult-vs-add-plots.png){#fig-mult-vs-add-plots}\n\nIn @fig-mult-vs-add-plots, we can see that for multiplicative seasonality there are spikes around the lag at $S = 12$. It does not matter which directions the spikes are moving in, just as long as there are spikes adjacent to the seasonal lag.\n\nBy default, R assumes multiplicative seasonality. If you want to force additive, then you have to 0 out the seasonal terms and then manually assign all the lags you do want:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns_arima <- Arima(\n    train,\n    order = c(1, 0, 13),\n    seasonal = c(0, 1, 0),\n    fixed = c(NA, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, NA, NA),\n    method = \"ML\"\n)\nsummary(s_arima)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: train \nARIMA(1,0,13)(0,1,0)[12] \n\nCoefficients:\n         ar1      ma1  ma2  ma3  ma4  ma5  ma6  ma7  ma8  ma9  ma10  ma11\n      0.9679  -0.3698    0    0    0    0    0    0    0    0     0     0\ns.e.  0.0237   0.0880    0    0    0    0    0    0    0    0     0     0\n         ma12    ma13\n      -0.6612  0.2490\ns.e.   0.0626  0.0766\n\nsigma^2 = 3792723:  log likelihood = -1755.53\nAIC=3521.07   AICc=3521.39   BIC=3537.43\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE    MAPE      MASE       ACF1\nTraining set 216.2511 1870.713 1094.634 0.3140324 2.29433 0.4065768 0.02300017\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}