{
  "hash": "a2cd226a70592b0805c87fcd1fd64c22",
  "result": {
    "markdown": "---\ntitle: Seasonality Models\ndate: 09/27/2023\n---\n\n\n# Setup {.unnumbered}\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(aTSA)\nlibrary(tseries)\nlibrary(forecast)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\nusairlines <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/usairlines.csv\")\npassenger <- ts(usairlines$Passengers, start = 1990, frequency = 12)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sma\nfrom statsmodels.tsa.seasonal import STL\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt, ExponentialSmoothing\n\nusair = r.usairlines\ndf = pd.date_range(start='1/1/1990', end='3/1/2008', freq='MS')\nusair.index = pd.to_datetime(df)\n\ntrain = usair.head(207)\ntest = usair.tail(12)\n```\n:::\n\n\n:::\n\n# Review\n\n## Exponential Smoothing Model Forecasts\n\nIn the short term, exponential smoothing models can be great at forecasting one-step ahead. For seasonal models, the one-step ahead is a whole season ahead.\n\n## Stationarity\n\nStationary means that any time could be the mean. Eventually our data converges to a mean.\n\nWe need consistency of mean and variance. If there are significant changes in mean (trending) or seasonality then the data is **NOT** stationary.\n\n## ARIMA Models\n\nAR is forecasting a series based on the past values in the series--called **lags**. With AR terms, the actual past observations inform the prediction in the next time period.\n\nMA is forecasting a series based solely on past errors--called **error lags**. With MA terms, the mistake you made in the last observation informs what your prediction is in the next time period.\n\nTypically, we write ARIMA as:\n\n$$\n\\text{ARIMA}(p, d, q)\n$$\n\n-   $p$ is the number of AR terms\n-   $d$ is the number of first differences\n-   $q$ is the number of MA terms\n\n## Seasonality\n\nThere is no formal test for seasonality. You have to plot your overall data to see if you believe seasonality exists.\n\n# Using the US Passengers Data\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- subset(passenger, end = length(passenger) - 12)\ntest <- subset(passenger, start = length(passenger) - 11)\n\ndecomp_stl <- stl(train, s.window = 7)\nplot(decomp_stl)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/train-test-split-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n\n:::\n\nTo decide how large our test set should be, we use the length of forecast we are interested in. In this case, we are interested in the next year forecast so the length of test will be 12 months.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhwes_usair_train <- hw(train, seasonal = \"multiplicative\", initial = \"optimal\", h = 12)\nautoplot(hwes_usair_train) +\n    autolayer(fitted(hwes_usair_train), series = \"fitted\") +\n    ylab(\"Airlines Passengers\") +\n    geom_vline(xintercept = 2007.25, color = \"orange\", linetype = \"dashed\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/holt-winters-model-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhw_error <- test - hwes_usair_train$mean\n\nhw_mae <- mean(abs(hw_error))\nhw_mape <- mean(abs(hw_error) / abs(test)) * 100\n```\n:::\n\n\nIs this a good MAPE? We don't know, but exponential smoothing models make a good baseline to compare other models to.\n\n# Python\n\n:::\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n# Python\n\n:::\n\n# Seasonality\n\n**Seasonality** is the component of time series that reprsents the effects of seasonal variation. However, just because you have something that repeats every year (e.g. chocolate sales in February) does not mean that it is seasonal. A **seasonal period ($S$)** is the length of time that the season occurs. For monthly data, $S = 12$.\n\nSeasonal data means that no matter where you are in a season, that seasonal wave will repeat itself.\n\nBy defiinition, seasonal data is not stationary. Mathematically, stationarity is if you take any window of time to your data, the average stays the same. See @fig-stationary-windows for an example.\n\n![Seasonality is Not Stationary](images/stationary-windows.png){#fig-stationary-windows}\n\n# Seasonal ARIMA Models\n\nSimilar to trend, seasonality can be solved with deterministic or stochastic solutions.\n\n**Deterministic** uses seasonal dummy variables, Fourier transforms, and predictor variables. **Stochastic** uses seasonal differences. We always have to make our data stationary first before modeling.\n\n# Seasonal Unit-Root Testing\n\nWe can perform the **Canova-Hansen** test to evaluate whether a unit root exists for seasonal data.\n\n:::{.text-center}\n$H_0$: Deterministic Seasonality (Differencing will not help)\n\n$H_a$: Stochastic Seasonality (Differencing needed)\n:::\n\nNo good formal tests for seasons beyond 24. Every time we take a difference we essentially create a whole new distribution. Taking one difference and taking one difference of up to 12 lags are not similar.\n\nIf we face this situation, then we should try both paths and see which model predicts better.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>% nsdiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncbind(\"Airlines Passengers\" = train, \"Annual change in Passengers\" = diff(train, 12)) %>%\n    autoplot(facets = TRUE) +\n    labs(x = \"Time\", y = \"\") +\n    ggtitle(\"Comparison of Difference Data to Original\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotting-seasonal-diff-1.png){width=672}\n:::\n:::\n\n\nThis result tells us that we should take one **seasonal** difference. After we take our seasonal difference (12 lags) then we need to check for **regular** differences afterwards.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain %>%\n    diff(lag = 12) %>%\n    ndiffs()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n:::\n\n\nThis result tells us that we should take 0 regular differences after taking the seasonal difference. No trend, no seasonality, so we believe our data is now stationary.\n\n# Python\n\n:::\n\n# Deterministic Solutions\n\n## Seasonal Dummy Variables\n\nFor a time series with $S$ periods within a season, there will be **S - 1** dummy variables, one for each period (and one accounted for with the intercept).\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}