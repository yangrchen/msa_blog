{
  "hash": "5ee641dd04a9e8250164457de3fa3b84",
  "result": {
    "markdown": "---\ntitle: \"ARIMA Models: AR and MA Models\"\ndate: 09/07/2023\ndate-modified: 09/19/2023\n---\n\n\n# Setup {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tseries)\nlibrary(forecast)\nY <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/ar2.csv\")\nx <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/MA2.csv\")\n```\n:::\n\n\n# Notation\n\n-   AR(p) where p refers to the number of autoregressive terms\n-   MA(q) where q refers to the number of moving average terms\n-   ARMA(p, q) where p refers to AR terms and q refers to MA terms\n-   ARIMA(p, d, q) where p refers to AR terms, d refers to number of differences taken (d = 2 means we took a difference of the differences), q refers to MA terms\n\n# Autoregressive Models\n\nForecast a series based solely on the past values of $Y_t$. We are trying to model the lags of our Y terms.\n\nWe are focusing on the most basic case where there is only one lag value of $Y_t$--the AR(1) model. In daily data, today depends on yesterday, yesterday depends on the day before.\n\n$$\n\\begin{align*}\nY_t &= \\omega + \\phi Y_{t-1} + e_t \\\\\nY_{t-1} &= \\omega + \\phi Y_{t-2} + e_{t-1} \\\\\nY_{t-2} &= \\omega + \\phi Y_{t-3} + e_{t-2}\n\\end{align*}\n$$\n\nThe relationship goes back in time. The model assumes that the correlation structure exists throughout the whole series. \n\nThe $\\omega$ in the AR model refers to the average of the series whereas in the Random Walk equation the $\\omega$ refers to drift. Whenever you are not taking a difference, the model will try to fit around the mean.\n\n## Difference Between ESM and AR(1)\n\nESMs are weighting the previous information with the average model. AR(1) only takes into account the previous value. AR(1) uses a restricted maximum likelihood estimator to estimate the weights. \n\n## Correlation Functions for AR(1)\n\nACF decreases exponentially as the number of lags increases. However, the PACF has a significant spike at the first lag, followed by nothing after.\n\n![AR(1)-ACF](images/AR(1)-ACF.png){#fig-ar1-acf}\n\n![AR(1)-PACF](images/AR(1)-PACF.png){#fig-ar1-pacf}\n\n$|\\phi| < 1$ has to be true for stationarity. Overall, the effect of shocks last over a long period of time. However, the effect of shocks that happened long ago has little effect on the present IF the value for $|\\phi| < 1$. Stationarity--the dependence of previous observations declines over time.\n\n-   If $\\phi = 1$, then we have a Random Walk and NOT an AR model\n    -   $Y_t = 1 \\cdot Y_{t-1} + e_t$ which is the Random Walk equation\n-   If $\\phi > 1$, then today depends on tomorrow which does not make sense\n\nIf we have a random walk then our correlation graphs are useless in determining correlation structure. See @fig-rw-acf for an example of a Random Walk ACF.\n\n![Random Walk ACF](images/randomwalk-autocorrelation.png){#fig-rw-acf}\n\n## AR(2) Model\n\nA time series that is a linear function of 2 past values plus error is an AR process of order 2:\n\n$$\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + e_t\n$$\n\nAR(2) models have a pattern in PACF plots for when it comes to stationarity (2 spikes). The effect of shocks that happened long ago has little effect on the present if the value for $\\left| \\phi_1 + \\phi_2 \\right| < 1$. We have more restrictions on our $\\phi$ weights.\n\n## AR(p) Model\n\nTime series that is a linear function of $p$ past values plus error is called AR process or order $p$.\n\n$$\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\cdots + \\phi_pY_{t-p} + e_t\n$$\n\nIf the model is just an AR model, then the PACF has significant spikes at hte lags up to $p$ lags, followed by nothing after.\n\nThere are more complicated restrictions on $\\phi_i$'s. If your software warns you about convergence issues or warnings, then you have to try a different model.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY_ts <- ts(Y)\nY_ARIMA <- Arima(Y_ts, order = c(2, 0, 0))\n\nggAcf(Y_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(Y_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\nNotice that we are doing the correlation functions on the **residuals**. If you have modeled the correlation structure, then you should only have small amounts of noise left. Residual analysis gives us an idea of whether we have more modeling to do: if large spikes in our residuals exist, then we have not modeled all the autocorrelation. \n\n# Python\n\n:::\n\nThe coefficients from AR models are **not very interpretable**. If you are reporting to a client you would just say that you fit an AR(p) model. Note that we can still overfit in ARMA models. We prefer parsimonious, simpler models over complicated models that capture every bit of autocorrelation.\n\n# Moving Average Models\n\nMoving average models are modeling lags of the error terms--the past error values.\n\n## MA(1)\n\n$$\nY_t = \\omega + e_t - \\theta e_{t-1}\n$$\n\nThis is true for all observations (each observation is dependent on the error from the previous observation). Therefore, in an MA(1) model, individual shocks only last for a short time.\n\nIn the MA model, we do not have the same restrictions as AR models (but do want them to be invertible).\n\n## Correlation Functions for MA(1)\n\nThe ACF has a significant spike at the first lag, followed by nothing after.\n\nPACF decreases exponentially as the number of lags increases. For $q$ > 1, however, these patterns do not apply as the model becomes more complicated.\n\n## MA(q)\n\nTime series that is a linear function of $q$ past errors is a moving average process of order $q$.\n\n$$\nY_t = \\omega + e_t - \\theta_1e_{t-1} - \\theta_2e_{t-2} - \\cdots - \\theta_qe_{t-q}\n$$\n\n## Correlation Functions for MA(q)\n\nACF has significant spikes at lags up to lag $q$, followed by nothing after.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nWe can plot the ACF and PACF for a simulated MA(2) model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n\nIn a purely MA(2) model, we see the two spikes in the first two lags in the ACF plot.\n\nNote that in `ggplot` the plots have lag 0 truncated since it does not provide us any valuable information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_ts <- ts(x)\nx_ARIMA <- Arima(x_ts, order = c(0, 0, 2))\nsummary(x_ARIMA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: x_ts \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n          ma1     ma2    mean\n      -0.2460  0.4772  0.0250\ns.e.   0.0857  0.0923  0.0567\n\nsigma^2 = 0.2207:  log likelihood = -65.1\nAIC=138.2   AICc=138.63   BIC=148.62\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0008828966 0.4627151 0.3808289 74.99115 114.4434 0.5453401\n                     ACF1\nTraining set -0.002299708\n```\n:::\n\n```{.r .cell-code}\nggAcf(x_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(x_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n# Python\n\n:::\n\nWe don't have any large spikes in our residual plots so our model effectively captures the autocorrelation structure.\n\nNote that we can compare the AIC/BIC between AR, MA, or ARIMA models but we cannot compare them to the metrics given by ESM models as there may be different ways the metrics are being calculated across different algorithms.\n\n# AR and MA Model Notes\n\n-   Any AR(p) model can be rewritten as an MA($\\infty$).\n-   If any MA(q) model is invertible, then this MA(q) model can be rewritten as an AR($\\infty$).\n-   Software should warn you if model is not invertible, if there is no convergence, or any other issues. You should change your model if this is the case.\n-   Parameters can have different signs based on how it parameterizes equations.\n\n## PACF/ACF for Identifying Model Terms\n\n**PACF is useful for identifying the order of AR terms** in a time series model. PACF measures **the direct relationship** between the current value at time $t$ and a value at time $t - k$ without the influence of other lags. \n\nHowever, **MA processes do not exhibit a direct relationship** between the current observations and its past values. MA processes involve linear combinations of error terms with coefficients applied to past error terms. This makes it difficult to assess the relationship using PACF because the influence of past values is not straightforward.\n\nInstead, we use **ACF to identify the order of MA terms**. If we see a signficant spike at lag $k$, then we may want to consider adding in $q = k$ terms to the MA portion of our model.\n\n# White Noise\n\nWhite noise is essentially the independent errors that we are left with after modeling the signal.\n\nWhite noise time series have errors that have the following characteristics:\n\n-   Follow a Normal distribution.\n-   Have a mean zero and positive, constant variance.\n-   All observations are independent of each other.\n-   Autocorrelation and partial autocorrelation functions have a value close to zero at every time point (except for lag of 0).\n\nGoal of modeling time series to to be left with white noise residuals in the time series. If residuals have a \"significant\" dependence structure, then we can still continue modeling.\n\n## Ljung-Box $\\chi^2$ Test for White Noise\n\nLjung-Box can be applied to original data or to the residuals after fitting a model.\n\n::: {.text-center}\n$H_0:$ Series has no autocorrelation.\n\n$H_a:$ One or more autocorrelations up to lag $m$ are not zero.\n:::\n\nEssentially, rejecting our null hypothesis means that more modeling should be done. When looking at nonseasonal data, we typically go back around 10 lags.\n\n$$\n\\chi_m^2 = n(n + 2) \\sum_{k=1}^{m} \\frac{\\beta_k^2}{n - k}\n$$\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nFor `Box.test`, there is a parameter `fitdf` that you want to specify. The degrees of freedom change depending on what kind of vector we are trying to test. If `fitdf = 0` then we are specifying the test on the original time series data. If `fitdf = p + q` (where p is number of AR terms and q is number of MA terms) then we are specifying the test on the residuals of our fitted model.\n\nLooking at original data first: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nindex1 <- seq(1, 10)\nwhite_lb <- rep(NA, 10)\n\nfor (i in 1:10) {\n    white_lb[i] <- Box.test(Y, lag = i, type = \"Ljung-Box\", fitdf = 0)$p.value\n}\n\nwhite_dat <- data.frame(cbind(white_lb, index1))\ncolnames(white_dat) <- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test p-values\", x = \"Lags\", y = \"p-values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooking at the fitted model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY_ARIMA <- Arima(Y, order = c(2, 0, 0))\nwhite_lb <- rep(NA, 10)\n\nfor (i in 3:10) {\n    white_lb[i] <- Box.test(Y_ARIMA$residuals, lag = i, type = \"Ljung-Box\", fitdf = 2)$p.value\n}\n\n# We index our lags at the p+q+1 lag.\n# For an AR(2), we index at 3\nwhite_dat <- data.frame(cbind(white_lb[3:10], index1[3:10]))\ncolnames(white_dat) <- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test when this is white noise\", x = \"Lags\", y = \"p-values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNote that you have to start your tests on the next lag term than the number of lags in your model. In this example we are using AR(2) so our tests start on lag 3. In general, the Ljung-Box autocorrelation terms are undefined up to $p + q$ terms.\n\n# Python\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}