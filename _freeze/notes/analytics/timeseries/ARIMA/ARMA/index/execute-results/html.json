{
  "hash": "d3235c4048019d8a0ca028e80f07ef32",
  "result": {
    "markdown": "---\ntitle: \"ARIMA Models: AR and MA Models\"\ndate: 09/07/2023\n---\n\n\n# Setup {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tseries)\nlibrary(forecast)\nY <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/ar2.csv\")\nx <- read.csv(\"https://raw.githubusercontent.com/sjsimmo2/TimeSeries/master/MA2.csv\")\n```\n:::\n\n\n# Notation\n\n-   AR(p) where p refers to the number of autoregressive terms\n-   MA(q) where q refers to the number of moving average terms\n-   ARMA(p, q) where p refers to AR terms and q refers to MA terms\n-   ARIMA(p, d, q) where p refers to AR terms, d refers to number of differences taken (d = 2 means we took a difference of the differences), q refers to MA terms\n\n# Autoregressive Models\n\nForecast a series based solely on the past values of $Y_t$. We are trying to model the lags of our Y terms.\n\nWe are focusing on the most basic case where there is only one lag value of $Y_t$--the AR(1) model.\n\n$$\n\\begin{align*}\nY_t &= \\omega + \\phi Y_{t-1} + e_t \\\\\nY_{t-1} &= \\omega + \\phi Y_{t-2} + e_{t-1} \\\\\nY_{t-2} &= \\omega + \\phi Y_{t-3} + e_{t-2}\n\\end{align*}\n$$\n\n## Correlation Functions for AR(1)\n\nACF decreases exponentially as the number of lags increases. However, the PACF has a significant spike at the first lag, followed by nothing after.\n\n![AR(1)-ACF](images/AR(1)-ACF.png)\n\n![AR(1)-PACF](images/AR(1)-PACF.png)\n\nOverall, the effect of shocks last over a long period of time. However, the effect of shocks that happened long ago has little effect on the present IF the value for $|\\phi| < 1$. Stationarity--the dependence of previous observations declines over time.\n\n-   If $\\phi = 1$, then we have a Random Walk and NOT AR model\n-   If $\\phi > 1$, then today depends on tomorrow which does not make sense\n\n## AR(2) Model\n\nA time series that is a linear function of 2 past values plus error is an AR process of order 2:\n\n$$\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + e_t\n$$\n\nAR(2) models have a pattern in PACF plots for when it comes to stationarity (2 spikes). The effect of shocks that happened long ago has little effect on the present if the value for $\\left| \\phi_1 + \\phi_2 \\right| < 1$\n\n## AR(p) Model\n\nTime series that is a linear function of $p$ past values plus error is called AR process or order $p$.\n\n$$\nY_t = \\omega + \\phi_1Y_{t-1} + \\phi_2Y_{t-2} + \\cdots + \\phi_pY_{t-p} + e_t\n$$\n\nIf the model is just an AR model, then the PACF has significant spikes at hte lags up to $p$ lags, followed by nothing after.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY_ts <- ts(Y)\nY_ARIMA <- Arima(Y_ts, order = c(2, 0, 0))\n\nggAcf(Y_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(Y_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-2.png){width=672}\n:::\n:::\n\n\n# Python\n\n:::\n\nThe coefficients from the AR(2) model are not very interpretable. If you are reporting to a client you would just say that you fit an AR(2) model.\n\n# Moving Average Models\n\nMoving average models are modeling lags of the error terms--the past error values.\n\n## MA(1)\n\n$$\nY_t = \\omega + e_t - \\theta e_{t-1}\n$$\n\nThis is true for all observations (each observation is dependent on the error from the previous observation). Therefore, in an MA(1) model, individual shocks only last for a short time.\n\nIn the MA model, we do not have the same restrictions as AR models (but do want them to be invertible).\n\n## Correlation Functions for MA(1)\n\nThe ACF has a significant spike at the first lag, followed by nothing after.\n\nPACF decreases exponentially as the number of lags increases. For $q$ > 1, however, these patterns do not imply as the model becomes more complicated.\n\n## MA(q)\n\nTime series that is a linear function of $q$ past errors is a moving average process of order $q$.\n\n$$\nY_t = \\omega + e_t - \\theta_1e_{t-1} - \\theta_2e_{t-2} - \\cdots - \\theta_qe_{t-q}\n$$\n\n## Correlation Functions for MA(q)\n\nACF has significant spikes at lags up to lag $q$, followed by nothing after.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nMA(2) Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggAcf(x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(x)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nx_ts <- ts(x)\nx_ARIMA <- Arima(x_ts, order = c(0, 0, 2))\nsummary(x_ARIMA)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nSeries: x_ts \nARIMA(0,0,2) with non-zero mean \n\nCoefficients:\n          ma1     ma2    mean\n      -0.2460  0.4772  0.0250\ns.e.   0.0857  0.0923  0.0567\n\nsigma^2 = 0.2207:  log likelihood = -65.1\nAIC=138.2   AICc=138.63   BIC=148.62\n\nTraining set error measures:\n                       ME      RMSE       MAE      MPE     MAPE      MASE\nTraining set 0.0008828966 0.4627151 0.3808289 74.99115 114.4434 0.5453401\n                     ACF1\nTraining set -0.002299708\n```\n:::\n\n```{.r .cell-code}\nggAcf(x_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggPacf(x_ARIMA$residuals)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n:::\n\n\n# Python\n:::\n\nNote that we can compare the AIC/BIC between AR, MA, or ARIMA models but we cannot compare them to the metrics given by ESM models.\n\n# AR and MA Model Notes\n\n-   Any AR(p) model can be rewritten as an MA($\\infty$).\n-   If any MA(q) model is invertible, then this MA(q) model can be rewritten as an AR($\\infty$).\n-   Software should warn you if model is not invertible, if there is no convergence or any other issues... pay attention to the warnings you encounter.\n-   Parameters can have different signs based on how it parameterizes equations.\n\n# White Noise\n\nWhite noise is essentially the independent errors that we are left iwth after modeling the signal.\n\nWhite noise time series have errors that have the following characteristics:\n\n-   Follow a Normal distribution.\n-   Have a mean zero and positive, constant variance.\n-   All observations are independent of each other.\n-   Autocorrelation and partial autocorrelation functions have a value close to zero at every time point (except for lag of 0).\n\nGoal of modeling time series to to be left with white noise residuals in the time series. If residuals have a \"significant\" dependence structure, then we can still continue modeling.\n\n## Ljung-Box $\\chi^2$ Test for White Noise\n\nLjung-Box can be applied to original data or to the residuals after fitting a model.\n\n::: {.text-center}\n$H_0:$ Series has no autocorrelation.\n\n$H_a:$ One or more autocorrelations up to lag $m$ are not zero.\n:::\n\nEssentially, rejecting our null hypothesis means that more modeling should be done.\n\n$$\n\\chi_m^2 = n(n + 2) \\sum_{k=1}^{m} \\frac{\\beta_k^2}{n - k}\n$$\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nLooking at original data first: \n\n\n::: {.cell}\n\n```{.r .cell-code}\nindex1 <- seq(1, 10)\nwhite_lb <- rep(NA, 10)\n\nfor (i in 1:10) {\n    white_lb[i] <- Box.test(Y, lag = i, type = \"Ljung-Box\", fitdf = 0)$p.value\n}\n\nwhite_dat <- data.frame(cbind(white_lb, index1))\ncolnames(white_dat) <- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test p-values\", x = \"Lags\", y = \"p-values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nLooking at the fitted model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nY_ARIMA <- Arima(Y, order = c(2, 0, 0))\nwhite_lb <- rep(NA, 10)\n\nfor (i in 3:10) {\n    white_lb[i] <- Box.test(Y_ARIMA$residuals, lag = i, type = \"Ljung-Box\", fitdf = 2)$p.value\n}\nwhite_dat <- data.frame(cbind(white_lb[3:10], index1[3:10]))\ncolnames(white_dat) <- c(\"pvalues\", \"Lag\")\n\nggplot(white_dat, aes(x = factor(Lag), y = pvalues)) +\n    geom_col() +\n    labs(title = \"Ljung-Box test when this is white noise\", x = \"Lags\", y = \"p-values\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nNote that you have to start your tests on the next lag term. In this example we are using AR(2) so our tests start on lag 3.\n\n# Python\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}