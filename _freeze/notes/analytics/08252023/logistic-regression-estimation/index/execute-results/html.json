{
  "hash": "b2f38c25045e83736057fec2036f7bb2",
  "result": {
    "markdown": "---\ntitle: Estimation Methods for Logistic Regression\ndate: 08/25/2023\n---\n\n\nIn logistic regression, the assumptions of residual Normality and constant variance are violated. OLS is not the best method for parameter estimation.\n\n# Maximum Likelihood Estimation\n\nEstimates are obtained with **maximum likelihood estimation (MLE)**. Likelihood function measures how probable a specific grid of $\\beta$ values is to have produced you data.\n\nFor a binomial target variable:\n\n$$\nL(\\beta's|y, x_1, x_2, \\cdots) = \\prod_{i=1}^n p_i^{y_i}(1 - p_i)^{1 - y_i}\n$$\n\n-   $p_i = \\frac{1}{1 + e^{-z}}$\n\nWithin this combination of inputs and outputs, what is the best combination of $\\beta$ to get our data.\n\nWe can work with the log of the likelihood function:\n\n$$\n\\log(L) = \\sum_{i=1}^n \\left[y_i\\log(p_i) + (1- y_i)\\log(1 - p_i)\\right]\n$$\n\n## Likelihood Ratio Tests\n\nLikelihood estimation allows us to do hypothesis testing. If extra predictors don't add information, then a model that includes them shouldn't be substantially more likely than the moel that doesn't include them.\n\n**Likelihood Ratio Test (LRT)** compares the full and reduced models.\n\n![Likelihood Ratio Test](images/likelihood-ratio-test.png)\n\n$L_0$ is the reduced model and $L_1$ is the full model.\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames |>\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\ntrain <- sample_frac(ames, 0.7)\n\nlogit_model <- glm(Bonus ~ Gr_Liv_Area + factor(Central_Air), data = train, family = binomial(link = \"logit\"))\n\nlogit_model_r <- glm(Bonus ~ 1, data = train, family = binomial(link = \"logit\"))\nanova(logit_model, logit_model_r, test = \"LRT\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: Bonus ~ Gr_Liv_Area + factor(Central_Air)\nModel 2: Bonus ~ 1\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1      2048     1808.8                          \n2      2050     2775.8 -2  -966.96 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\nimport statsmodels.formula.api as smf\nimport scipy as sp\n\names = r.ames\ntrain = r.train\nlogit_model = GLM.from_formula(\n    \"Bonus ~ Gr_Liv_Area + C(Central_Air)\", data=train, family=Binomial()\n).fit()\n\nreduced_ll = logit_model.llf\nfull_ll = (\n    smf.logit(\"Bonus ~ Gr_Liv_Area + C(Central_Air) + C(Fireplaces)\", data=train)\n    .fit()\n    .llf\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWarning: Maximum number of iterations has been exceeded.\n         Current function value: 0.425703\n         Iterations: 35\n\n/opt/homebrew/Caskroom/miniconda/base/envs/msa/lib/python3.11/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n  warnings.warn(\"Maximum Likelihood optimization failed to \"\n```\n:::\n\n```{.python .cell-code}\nLR_stat = -2 * (reduced_ll - full_ll)\np_val = sp.stats.chi2.sf(LR_stat, 4)\np_val\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n8.181135706933704e-13\n```\n:::\n:::\n\n\n:::\n\nIn this example, you can think of LRT as comparing these two equations:\n\n$$\n\\begin{align*}\nL_1 &= \\beta_0 + \\beta_1GLA + \\beta_2CA + \\beta_3F1 \\beta_4F_2 + \\cdots \\\\\nL_0 &= \\beta_0 + \\beta_1GLA + \\beta_2CA\n\\end{align*}\n$$\n\nAlways check the difference in degrees of freedom to double-check if you are comparing the right number of variables (how many levels are in the additional variable you are including?).\n\n## Categorical P-Values\n\nFor categorical variables with more than 2 levels we shouldn't evaluate the significance of the entire variable with the individual p-values. Use LRT to compare model with and without the categorical variable since LRT compares the model with ALL the levels included against the model with ALL the levels not included.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_model_f <- glm(Bonus ~ Gr_Liv_Area + factor(Central_Air) + factor(Fireplaces), data = train, family = binomial(link = \"logit\"))\ncar::Anova(logit_model_f, test = \"LR\", type = \"III\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table (Type III tests)\n\nResponse: Bonus\n                    LR Chisq Df Pr(>Chisq)    \nGr_Liv_Area           565.89  1  < 2.2e-16 ***\nfactor(Central_Air)    86.81  1  < 2.2e-16 ***\nfactor(Fireplaces)     62.61  4  8.181e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\n# Assumptions\n\nUnlike categorical variables, we need to test for linearity of the continuous variables with the logit.\n\n## General Additive Model (GAM)\n\nThe idea: We want to fit the best curve for our target and then we run a statistical test to see if it our fitted curve is any better than just a straight line relationship. If it is better, then our assumption of linearity is not met.\n\nTraditional logistic regression model:\n\n$$\n\\log(odds) = \\beta_0 + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n$$\n\nGAM logistic regression model:\n\n$$\n\\log(odds) = \\beta_0 + f_1(x_{1,i}) + \\cdots + f_k(x_{k,i})\n$$\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(mgcv)\nfit_gam <- gam(Bonus ~ s(Gr_Liv_Area) + factor(Central_Air), data = train, family = binomial(link = \"logit\"), method = \"REML\")\n\nsummary(fit_gam)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFamily: binomial \nLink function: logit \n\nFormula:\nBonus ~ s(Gr_Liv_Area) + factor(Central_Air)\n\nParametric coefficients:\n                     Estimate Std. Error z value Pr(>|z|)    \n(Intercept)           -4.4616     0.5033  -8.864  < 2e-16 ***\nfactor(Central_Air)Y   3.4882     0.4911   7.103 1.22e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n                 edf Ref.df Chi.sq p-value    \ns(Gr_Liv_Area) 6.221  7.232  380.4  <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =   0.43   Deviance explained =   39%\n-REML = 859.46  Scale est. = 1         n = 2051\n```\n:::\n\n```{.r .cell-code}\nplot(fit_gam)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nThe spline p-value is not telling us whether our assumption is met or not. It tells us whether or not the splined variable is significant in the model.\n\n`edf` in the splined variable is the polynomial degree that the fit thinks we should have. In theory, if the relationship was close to a straight line then `edf` would be close to 1.\n\nHow do we actually test if our variable satisfies the linearity assumption?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(logit_model, fit_gam, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAnalysis of Deviance Table\n\nModel 1: Bonus ~ Gr_Liv_Area + factor(Central_Air)\nModel 2: Bonus ~ s(Gr_Liv_Area) + factor(Central_Air)\n  Resid. Df Resid. Dev     Df Deviance  Pr(>Chi)    \n1    2048.0     1808.8                              \n2    2042.8     1692.3 5.2212   116.58 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n:::\n:::\n\n\nIf our p-value is significant, then it means our two models are significantly different. If our two models are significantly different, then our curve model is providing more information than a straight line. Assumption is not met.\n\nIn conclusion, high p-value means our assumption is met, else not met.\n:::\n\n### Linearity Assumption Failed?\n\n1.  Use GAM logistic model instead with more limited interpretation on variables that break assumption\n2.  Bin the continuous variables that break assumption (keeps interpretation)\n\n![Binning Continuous Variable](images/continuous-binning.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n    mutate(Gr_Liv_Area_BIN = cut(Gr_Liv_Area, breaks = c(-Inf, 1000, 1500, 3000, 4500, Inf)))\n\nlogit_model_bin <- glm(Bonus ~ factor(Gr_Liv_Area_BIN) + factor(Central_Air), data = train, family = binomial())\nsummary(logit_model_bin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = Bonus ~ factor(Gr_Liv_Area_BIN) + factor(Central_Air), \n    family = binomial(), data = train)\n\nCoefficients:\n                                       Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                             -8.8210     1.1065  -7.972 1.56e-15 ***\nfactor(Gr_Liv_Area_BIN)(1e+03,1.5e+03]   4.5121     1.0052   4.489 7.16e-06 ***\nfactor(Gr_Liv_Area_BIN)(1.5e+03,3e+03]   6.6437     1.0049   6.611 3.81e-11 ***\nfactor(Gr_Liv_Area_BIN)(3e+03,4.5e+03]  21.1646   363.8508   0.058  0.95361    \nfactor(Gr_Liv_Area_BIN)(4.5e+03, Inf]    5.5986     1.7331   3.230  0.00124 ** \nfactor(Central_Air)Y                     3.2224     0.4734   6.807 9.95e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2775.8  on 2050  degrees of freedom\nResidual deviance: 1892.0  on 2045  degrees of freedom\nAIC: 1904\n\nNumber of Fisher Scoring iterations: 14\n```\n:::\n:::\n\n\nNote that binning a continuous variable results in an *ordinal* variable.\n\n# Predicted Values\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnew_ames <- data.frame(Gr_Liv_Area = c(1500, 2000, 2250, 2500, 3500), Central_Air = c(\"N\", \"Y\", \"Y\", \"N\", \"Y\"))\n\nnew_ames <- data.frame(new_ames, \"Pred\" = predict(logit_model, newdata = new_ames, type = \"response\"))\nprint(new_ames)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Gr_Liv_Area Central_Air       Pred\n1        1500           N 0.01498152\n2        2000           Y 0.86084436\n3        2250           Y 0.94534188\n4        2500           N 0.48167577\n5        3500           Y 0.99966165\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}