{
  "hash": "7b6c75338770318f942a31b669509768",
  "result": {
    "markdown": "---\ntitle: Ordinary Least Squares Regression\ndate: 06/30/2023\ndate-modified: 07/23/2023\n---\n\n\n# Setup {.unnumbered}\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"true\"}\nlibrary(reticulate)\nuse_condaenv(\"blues_clues\")\n```\n:::\n\n\n# Pearson's Correlation\n\nPearson's correlation measures *linear* relationships for continuous variables. \n\n![Pearson's Correlation Scenarios](images/pearson.png)\n\n## Hypothesis Test for Correlation\n\nParameter representing population correlation is $\\rho$ and is estimated by $r$\n\n::: text-center\n$H_0: \\rho = 0$\n\n$H_a: \\rho \\neq 0$\n:::\n\nHowever, rejecting $H_0$ only means that $\\rho$ is not exactly 0 so we need to see if the relationship is practically significant.\n\nNote that outliers affect correlation and correlation *does not* imply causation.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(AmesHousing)\n\nset.seed(123)\n\names <- make_ordinal_ames()\names <- ames |> mutate(id = row_number())\ntrain <- ames |> sample_frac(0.7)\ntest <- anti_join(ames, train, by = \"id\")\n\ndim(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2051   82\n```\n:::\n\n```{.r .cell-code}\ndim(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 879  82\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(train$Gr_Liv_Area, train$Sale_Price)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  train$Gr_Liv_Area and train$Sale_Price\nt = 44.185, df = 2049, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6756538 0.7200229\nsample estimates:\n     cor \n0.698509 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(train[, c(\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\")])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Year_Built Total_Bsmt_SF First_Flr_SF Gr_Liv_Area Sale_Price\nYear_Built     1.0000000     0.4037104    0.3095407   0.2454325  0.5668889\nTotal_Bsmt_SF  0.4037104     1.0000000    0.8120419   0.4643838  0.6276502\nFirst_Flr_SF   0.3095407     0.8120419    1.0000000   0.5707205  0.6085229\nGr_Liv_Area    0.2454325     0.4643838    0.5707205   1.0000000  0.6985090\nSale_Price     0.5668889     0.6276502    0.6085229   0.6985090  1.0000000\n```\n:::\n:::\n\n\nWe can also generate a plot matrix of the variable associations with `pairs`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npairs(train[, c(\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\")])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\ntrain = r.train\n\nnp.corrcoef(train[\"Gr_Liv_Area\"], train[\"Sale_Price\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1.        , 0.69850904],\n       [0.69850904, 1.        ]])\n```\n:::\n\n```{.python .cell-code}\nnp.corrcoef(\n    train[[\"Year_Built\", \"Total_Bsmt_SF\", \"First_Flr_SF\", \"Gr_Liv_Area\", \"Sale_Price\"]],\n    rowvar=False,\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1.        , 0.40371038, 0.3095407 , 0.24543253, 0.56688895],\n       [0.40371038, 1.        , 0.81204187, 0.46438378, 0.62765021],\n       [0.3095407 , 0.81204187, 1.        , 0.57072054, 0.60852293],\n       [0.24543253, 0.46438378, 0.57072054, 1.        , 0.69850904],\n       [0.56688895, 0.62765021, 0.60852293, 0.69850904, 1.        ]])\n```\n:::\n:::\n\n\n## Correlation Does NOT Imply Causation\n\nA strong correlation does not mean that a change in one variable causes a change in the other. Correlations can be misleading if both variables are affected by other variables.\n\n# Simple Linear Regression\n\n$$\ny = \\beta_0 + \\beta_1x_i + e_i\n$$\n\nIn SLR, correlation is not equal to slope. Two pairs of variables can have the same correlation coeff, but different linear relationships. \n\n-   $\\beta_0 + \\beta_1x_1$ makes up the deterministic component\n-   $\\beta_0$ is the intercept estimate\n-   $\\beta_1$ is the slope estimate\n\n## Explained vs. Unexplained Variability\n\nWe are trying to explain variation in the response variable. We can't explain all of it due to random, uncontrollable error but we can model it.\n\n![Variability Explained in SLR](images/variability.png)\n\nWith linear regression, we are trying to minimize a **loss function** called **sum of squared errors**. This is essentially measuring the difference between our predictions and the actual response values we observed in the data. \n\nWe square the differences so they don't cancel each other out and we have a loss that we can optimize our model on. \n\n$$\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) ^2\n$$\n\nSSE makes up the amount of unexplained variability in our model.\n\n## The Baseline Model\n\n::: text-center\n$H_0: \\beta_1 = 0$\n\n$H_a: \\beta_1 \\neq 0$\n:::\n\nFor SLR, the global F-Test, parameter t-test and the test of Pearson's correlation **are all equivalent**.\n\nWhen we can't reject the null hypothesis we are saying that the independent variable doesn't explain any of the variability in the response.\n\n## Assumptions of Simple Linear Regression\n\n-   Linearity of the mean\n    -   As I change values in the independent variable, the line should go through the different means of the response linearly\n    -   If violated, misspecified model\n-   Errors are normally distributed\n    -   If violated, our test results are erroneous\n-   Errors have equal variance (homoskedasticity)\n    -   If violated, standard errors are compromised\n-   Errors are independent\n    -   If violated, standard errors are compromised\n\nThere is also an assumption of no perfect collinearity. Under multicollinearity, we can't believe in our parameter estimates. The parameter estimates would be biased as there are multiple variables supplying the same information.\n\n### Testing of Assumptions\n\n-   Normality can be verified using a histogram, QQ-Plot or normality test\n-   Equal variances can be verified through residuals versus predicted values\n-   Independence can look at residual plots for potential autocorrelation\n-   Linearity in the mean can be tested through a residual plot and finding that there is no pattern in residual plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslr <- lm(Sale_Price ~ Gr_Liv_Area, data = train)\npar(mfrow = c(2, 2))\n\nplot(slr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(slr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-478762  -30030   -1405   22273  335855 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14045.872   3942.503   3.563 0.000375 ***\nGr_Liv_Area   110.726      2.506  44.185  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57430 on 2049 degrees of freedom\nMultiple R-squared:  0.4879,\tAdjusted R-squared:  0.4877 \nF-statistic:  1952 on 1 and 2049 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.formula.api as smf\n\nmodel_slr = smf.ols(\"Sale_Price ~ Gr_Liv_Area\", data=train).fit()\n\nmodel_slr.pvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept       3.754768e-04\nGr_Liv_Area    4.195282e-300\ndtype: float64\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}