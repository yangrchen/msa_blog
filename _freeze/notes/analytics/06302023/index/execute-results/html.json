{
  "hash": "c13e510621634edca079c2e002f5f827",
  "result": {
    "markdown": "---\ntitle: Ordinary Least Squares Regression\ndate: 06.30.2023\n---\n\n\n# Pearson's Correlation\n\nPearson's correlation measures *linear* relationships.\n\n![Pearson's Correlation Scenarios](images/pearson.png)\n\n## Hypothesis Test for Correlation\n\nParameter representing population correlation is $\\rho$ and is estimated by $r$\n\n$H_0: \\rho = 0$\n\nHowever, rejecting $H_0$ only means that $\\rho$ is not exactly 0 so we need to see if the relationship is practically significant.\n\nNote that outliers affect correlation and correlation *does not* imply causation.\n\n## Test of Correlation in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(AmesHousing)\n\names <- make_ordinal_ames()\nset.seed(123)\names <- ames |> mutate(id = row_number())\ntrain <- ames |> sample_frac(0.7)\ntest <- anti_join(ames, train, by = \"id\")\n\ndim(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2051   82\n```\n:::\n\n```{.r .cell-code}\ndim(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 879  82\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(train$Gr_Liv_Area, train$Sale_Price)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  train$Gr_Liv_Area and train$Sale_Price\nt = 44.185, df = 2049, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.6756538 0.7200229\nsample estimates:\n     cor \n0.698509 \n```\n:::\n:::\n\n\n## Pearson in Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"data/AmesHousing.csv\")\n\ntrain, test = train_test_split(data, test_size=0.3, random_state=123)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nnp.corrcoef(train['Gr Liv Area'], train['SalePrice'])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1.        , 0.69584201],\n       [0.69584201, 1.        ]])\n```\n:::\n\n```{.python .cell-code}\nnp.corrcoef(train[['Year Built', 'Total Bsmt SF', '1st Flr SF', 'Gr Liv Area']], rowvar=False)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\narray([[1.        , 0.42722628, 0.32717559, 0.23863599],\n       [0.42722628, 1.        , 0.80658329, 0.45383515],\n       [0.32717559, 0.80658329, 1.        , 0.57135986],\n       [0.23863599, 0.45383515, 0.57135986, 1.        ]])\n```\n:::\n:::\n\n\n## Correlation Does NOT Imply Causation\n\nA strong correlation does not mean that a change in one variable causes a change in the other.\n\n# Simple Linear Regression\n\n$$\ny = \\beta_0 + \\beta_1x_i + e_i\n$$\n\nIn SLR, correlation is not equal to slope. Two pairs of variables can have the same correlation coeff, but different linear relationships.\n\n-   $\\beta_0$ is the intercept estimate\n-   $\\beta_1$ is the slope estimate\n\n## Explained vs. Unexplained Variability\n\nWe are trying to explain variation in the response variable. We can't explain all of it due to random, uncontrollable error but we can model it.\n\n![Variability Explained in SLR](images/variability.png)\n\nWith linear regression, we are trying to minimize a **loss function** called **sum of squared errors**:\n\n$$\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i) ^2\n$$\n\n-   This makes up the amount of unexplained variability in our model\n\n## The Baseline Model\n\n::: {.text-center}\n$H_0: \\beta_1 = 0$\n\n$H_a: \\beta_1 \\neq 0$\n:::\n\nFor SLR, the global F-Test, parameter t-test and the test of Pearson's correlation are all equivalent.\n\nWhen we can't reject the null hypothesis we are essentially saying that the independent variable doesn't explain any of the variability in the response.\n\n## Assumptions of Simple Linear Regression\n\n-   Linearity of the mean\n    -   As I change values in the independent variable, the line should go through the mean of the response linearly\n-   Errors are normally distributed\n-   Errors have equal variance (homoskedasticity)\n-   Errors are independent\n\n### Testing of Assumptions\n\n-   Normality can use a histogram, QQ-Plot or normality test\n-   Equal variances can use residuals versus predicted values\n-   Independence can look at residual plots for potential autocorrelation\n-   Linearity in the mean can be tested through a residual plot and finding that there is no pattern in residual plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslr <- lm(Sale_Price ~ Gr_Liv_Area, data = train)\npar(mfrow = c(2, 2))\n\nplot(slr)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(slr)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-478762  -30030   -1405   22273  335855 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 14045.872   3942.503   3.563 0.000375 ***\nGr_Liv_Area   110.726      2.506  44.185  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 57430 on 2049 degrees of freedom\nMultiple R-squared:  0.4879,\tAdjusted R-squared:  0.4877 \nF-statistic:  1952 on 1 and 2049 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.formula.api as smf\n\ntrain = train.rename(columns={\"Gr Liv Area\": \"Gr_Liv_Area\"})\nmodel_slr = smf.ols(\"SalePrice ~ Gr_Liv_Area\", data=train).fit()\n\nmodel_slr.pvalues\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIntercept       8.540663e-05\nGr_Liv_Area    6.980208e-297\ndtype: float64\n```\n:::\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}