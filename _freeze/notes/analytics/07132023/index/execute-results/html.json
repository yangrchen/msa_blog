{
  "hash": "36f99df24cdeaa87bef0143119677324",
  "result": {
    "markdown": "---\ntitle: Model Building and Scoring for Prediction\ndate: 07/13/2023\nengine: knitr\neditor: visual\neditor_options:\n    chunk_output_type: inline\n---\n\n\n# Model Building\n\nLinear regression is a good initial approach to model building, but not the only form of regression.\n\nLinear regression is the best **linear unbiased estimator**.\n\n## Best Linear Unbiased Estimator\n\n$$\n\\hat{\\beta}_j ~ N(\\beta_j, s_{\\hat{\\beta}_j})\n$$\n\nOn average, coefficients from all samples are centered from the true coefficient. What does it mean to be *best*? If assumptions hold, $s_{\\hat{\\beta}_j}$ is the minimum variance of all the unbiased estimators.\n\nPut another way: The spread of our guesses is as narrow as it can be.\n\nWhat if biased estimators had smaller variance?\n\n# Regularized Regression Overview\n\nAs the number of variables increases, more problems tend to arise:\n\n-   Assumptions start to fail\n-   Multicollinearity concerns\n    -   Variations lead to overfitting\n    -   Higher variance than desired\n\n**Regularized regression** puts constraints on the estimated coefficients in our model and *shrink* these estimates to 0.\n\nCoefficients are biased but potentially improve variance of the model.\n\nHere we are giving up interpretability for better prediction power. We're not guaranteed to predict better, but we have the potential to do so.\n\n![Bias-Variance Tradeoff Example](images/biased-regression.png)\n\nWith regularized regression, we move our model farther from the truth, but the precision of our guesses increases. We have to balance moving farther from the \"truth\" with precision.\n\nThere are three common types of regularized regression:\n\n-   Ridge\n-   LASSO\n-   ElasticNet\n\n## Penalties in Models\n\nRecall OLS regression minimizes the sum of squared errors:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2) = \\min(SSE)\n$$\n\nRegularized regression introduces a penalty term to the minimization:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\text{Penalty}) = \\min(SSE + \\text{Penalty})\n$$\n\n# Ridge Regression\n\nRidge regression introduces $L_2$ penalty term:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda\\sum_{j=1}^{p} \\hat{\\beta}_j^2) = \\min(SSE + \\lambda\\sum_{j=1}^{p} \\hat{\\beta}_j^2)\n$$\n\n-   If $\\lambda = 0$, then OLS\n-   As $\\lambda \\rightarrow \\infty$, coefficients shrink to 0\n    -   The only way to counteract $\\lambda$ getting bigger is to make the coefficients smaller.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n```\n:::\n\n```{.r .cell-code}\nset.seed(123)\n\names <- make_ordinal_ames()\n\names <- ames %>%\n    mutate(id = row_number())\n\ntrain <- ames %>%\n    sample_frac(0.7)\n\ntest <- anti_join(ames, train, by = \"id\")\ndim(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2051   82\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_reg <- train %>%\n    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %>%\n    replace(is.na(.), 0)\n\n# Leave all continuous variables alone\n# Dummy encode all categorical variables\n# Still need to factor any numeric categoricals beforehand\n# We delete the first column from the model matrix since we don't need the intercept column that model.matrix provides\ntrain_x <- model.matrix(Sale_Price ~ ., data = train_reg)[, -1]\ntrain_y <- train_reg$Sale_Price\n\ntest_reg <- test %>%\n    select(Sale_Price, Lot_Area, Street, Bldg_Type, House_Style, Overall_Qual, Roof_Style, Central_Air, First_Flr_SF, Second_Flr_SF, Full_Bath, Half_Bath, Fireplaces, Garage_Area, Gr_Liv_Area, TotRms_AbvGrd) %>%\n    replace(is.na(.), 0)\n\ntest_x <- model.matrix(Sale_Price ~ ., data = test_reg)[, -1]\ntest_y <- train_reg$Sale_Price\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: Matrix\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Matrix'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoaded glmnet 4.1-7\n```\n:::\n\n```{.r .cell-code}\n# alpha controls the option as to which penalty to use\names_ridge <- glmnet(x = train_x, y = train_y, alpha = 0)\nplot(ames_ridge, xvar = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain = r.train\ntest = r.test\n```\n:::\n\n\n# LASSO Regression\n\nLeast absolute shrinkage and selection operator regression introduces an $L_1$ penalty term to the minimization:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda\\sum_{j=1}^{p} |\\hat{\\beta}_j|) = \\min(SSE + \\lambda\\sum_{j=1}^{p} |\\hat{\\beta}_j|)\n$$\n\n-   If $\\lambda = 0$, then OLS\n-   As $\\lambda \\rightarrow 0$, coefficients shrink to 0\n\n## Differences in Effects\n\nLASSO can delete variables whereas Ridge can only get close to 0. Differences in effects are due to differences in penalty. The deletion of variables can actually act as variable selection.\n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\names_lasso <- glmnet(x = train_x, y = train_y, alpha = 1)\nplot(ames_lasso, xvar = \"lambda\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# ElasticNet Regression\n\nElastic net regression combines both penalty terms in the minimization:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda_1\\sum_{j=1}^{p} |\\hat{\\beta}_j| + \\lambda_2\\sum_{j=1}^{p} \\hat{\\beta}_j^2)\n$$\n\nThe `glmnet` function in R takes a slightly different approach:\n\n$$\n\\min(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda[\\alpha\\sum_{j=1}^{p} |\\hat{\\beta}_j| + (1 - \\alpha)\\sum_{j=1}^{p}\\hat{\\beta}_j^2])\n$$\n\n-   Any value of `alpha` between 0 and 1 gives a combination of both penalties\n\n# Optimizing Penalties\n\nNeed to select $\\lambda$ for any of the regularized regression. Don't want to minimize variance to point of overfitting.\n\nNote that none of the regularized regressions care about multicollinearity or model hierarchy. Since we are only looking at the coefficients, if multicollinearity is affecting the coefficients then regularized will most likely filter them out.\n\n## Cross-Validation\n\nCross-validation is one approach to prevent overfitting when tuning a parameter.\n\n-   Split training data into multiple pieces\n-   Build model on majority of pieces\n-   Evaluate on remaining piece\n-   Repeat process with switching out pieces for building and evaluation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Gives us lambda.min and lambda.1se on the graph\n# By default, k-fold = 10\names_lasso_cv <- cv.glmnet(x = train_x, y = train_y, alpha = 1)\nplot(ames_lasso_cv)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n## Important Variables\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(ames_lasso, s = c(ames_lasso_cv$lambda.min, ames_lasso_cv$lambda.1se))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n37 x 2 sparse Matrix of class \"dgCMatrix\"\n                                       s1            s2\n(Intercept)                  4.809883e+04  8.326132e+04\nLot_Area                     5.455632e-01  3.727557e-01\nStreetPave                   7.742774e+03  .           \nBldg_TypeTwoFmCon           -9.791571e+03  .           \nBldg_TypeDuplex             -2.380411e+04  .           \nBldg_TypeTwnhs              -1.755640e+04 -1.552627e+03\nBldg_TypeTwnhsE             -8.901776e+03  .           \nHouse_StyleOne_and_Half_Unf  1.006755e+04  .           \nHouse_StyleOne_Story         2.100854e+04  .           \nHouse_StyleSFoyer            3.314566e+04  .           \nHouse_StyleSLvl              9.806126e+03  .           \nHouse_StyleTwo_and_Half_Fin -2.786798e+04  .           \nHouse_StyleTwo_and_Half_Unf -8.735039e+03  .           \nHouse_StyleTwo_Story         .             .           \nOverall_Qual.L               2.270985e+05  1.963587e+05\nOverall_Qual.Q               1.004889e+05  1.061646e+05\nOverall_Qual.C               1.201336e+04  .           \nOverall_Qual^4              -9.446123e+02  .           \nOverall_Qual^5              -2.520393e+04 -5.544525e+03\nOverall_Qual^6              -8.477324e+03  .           \nOverall_Qual^7              -2.899510e+03  .           \nOverall_Qual^8               .             .           \nOverall_Qual^9              -7.317252e+01  .           \nRoof_StyleGable             -1.083345e+03  .           \nRoof_StyleGambrel            .             .           \nRoof_StyleHip                4.183190e+03  3.243469e+03\nRoof_StyleMansard           -4.033550e+04  .           \nRoof_StyleShed              -2.192610e+04  .           \nCentral_AirY                 1.360755e+04  1.155248e+04\nFirst_Flr_SF                 4.577067e-02  1.421832e+01\nSecond_Flr_SF                4.756848e+00  .           \nFull_Bath                    1.503856e+04  4.316036e+03\nHalf_Bath                    1.073726e+04  1.820427e+02\nFireplaces                   8.158764e+03  6.631559e+03\nGarage_Area                  3.623071e+01  4.285732e+01\nGr_Liv_Area                  4.233104e+01  3.285089e+01\nTotRms_AbvGrd               -7.343589e+02  .           \n```\n:::\n:::\n\n\nRegularized regression \n\n# Model Comparisons\n\nWhen we get to scoring our model on the test set, **do not rerun the algorithm**. We do not want to fit our model to the test dataset. Only use test data to score equations obtained from the final model for comparing.\n\n## Model Metrics\n\nRoot MSE:\n\n$$\nRMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n$$\n\nMean Absolute Error:\n\n$$\nMAE = \\frac{1}{n}\\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n$$\n\nMean Absolute Percentage Error:\n\n$$\nMAPE = 100 \\cdot \\frac{1}{n}\\sum_{i=1}^{n} |\\frac{y_i - \\hat{y}_i}{y_i}\n$$",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}