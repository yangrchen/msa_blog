{
  "hash": "9b220c30d3184c9f84640514be77693c",
  "result": {
    "markdown": "---\ntitle: Classification and Regression Trees (CART)\ndate: 10/02/2023\n---\n\n\nA decision tree has a **root node** and a series of nodes splitting off from the node based on rules on the variables in the data. We typically use a binary tree as it shows empirically good results than $n$-ary tree.\n\nLeaf nodes are the final output of decision tree models.\n\n![Decision Tree Visualized](images/decision-tree.png){#fig-decision-tree}\n\nNotes on decision trees:\n\n-   Interpretable\n-   List of decisions that predict the outcome that are easy to implement\n-   Allow for nonlinear associations\n-   Allow for interactions\n-   Can handle missing values\n-   Are greedy algorithms (picks the \"best\" split and settles on that split without going back)\n\n# Classification Trees\n\nClassification trees have an ordinal or categorical target. To determine the quality of a node, we use **purity**.\n\nPurity of a node is looking at how \"homogeneous\" the node is with respect to the target variable. @fig-purity shows an example of how purity is measured for the root node and one of the child nodes.\n\n![Classification Tree Purity](images/decision-tree-purity.png){#fig-purity}\n\n# Building the Tree Overview\n\n<!-- Need to put mermaid tables here -->\nA tree is built by recursively partioning the training data into successively *purer* subsets. Partiioning is done according to some condition which results in a binary split.\n\nFor categorical predictors, we need to put categories into two groups. An example for a predictor with A, B, and C levels is A vs. B and C or B vs. A and C.\n\nFor ordinal and quantitative variables, need to find the best value to split on. Data is \"binned\" into two groups and need to determine best way to bin based on the target.\n\n# Selecting the Best Split\n\n![Measures of Purity](images/purity-scale.png){#fig-purity-scale}\n\nFor a binary tree, a 50-50 split is the lowest purity we could have. The highest purity is 1.00 which means the node only contains a single level.\n\nHow do we choose the best split?\n\nLet $p(i|t)$ represent the the fraction of records belonging to class $i$ at a given node $t$. Let $c$ be the number of classes in a target variable.\n\nTwo common measures for **impurity** are **entropy** and **Gini's impurity**.\n## Entropy\n\n$$\n-\\sum_{i=1}^c p(i|t)\\log_2p(i|t)\n$$\n\n## Gini's Impurity\n\n$$\n1 - \\sum_{i=1}^{c} [p(i|t)]^2\n$$\n\n## Gain\n\nThe impurity of nodes is involved in calculating the **gain** of a node. Gain is measuring how much impurity we have reduced due to the split. We want gain to be high as it represents a significant difference in impurity.\n\n$$\n\\Delta = I(t) = (\\frac{n_L}{n}I(t_L) + \\frac{n_R}{n}I(t_R))\n$$\n\n-   $\\Delta$ is the gain\n-   $I(t)$ is the impurity of a parent node\n-   $I(t_L)$  and $I(t_R)$ are impurity in the left and right children nodes, respectively\n\nGain uses a weighted average between the impurity of the two children nodes.\n\n# Building Process\n\n1.  Compute the gain for all possible splits and select the best one.\n2.  Repeat process recursively until some stopping condition is met\n    1.  No splits meet some minimum Gain\n    2.  All leaves have some minimum number of observations\n    3.  A stopping condition is a way of *prepruning* the tree\n3.  Prune Tree\n\nPruning a tree refers to removing leaves / nodes in a bottom-up fashion, cutting splits with lowest gain first, while optimizing performance on validation data.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\nIn R, the children nodes are numbered $2n$ (left) and $2n + 1$ (right). The root node starts at number 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\nset.seed(7515)\n\nload(\"data/breast_cancer.Rdata\")\nperm <- sample(1:699)\nBC_randomOrder <- BCdata[perm, ]\ntrain <- BC_randomOrder[1:floor(0.75 * 699), ]\ntest <- BC_randomOrder[(floor(0.75 * 699) + 1):699, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBC_tree <- rpart(Target ~ . - ID, data = train, method = \"class\", parms = list(split = \"gini\"))\nBC_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 524 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 524 183 0 (0.65076336 0.34923664)  \n   2) Size< 3.5 360  28 0 (0.92222222 0.07777778)  \n     4) Normal< 3.5 334   8 0 (0.97604790 0.02395210)  \n       8) Bare< 2.5 308   0 0 (1.00000000 0.00000000) *\n       9) Bare>=2.5 26   8 0 (0.69230769 0.30769231)  \n        18) CT< 3.5 16   0 0 (1.00000000 0.00000000) *\n        19) CT>=3.5 10   2 1 (0.20000000 0.80000000) *\n     5) Normal>=3.5 26   6 1 (0.23076923 0.76923077) *\n   3) Size>=3.5 164   9 1 (0.05487805 0.94512195) *\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\nbcdata = r.BCdata\n\nX = bcdata.iloc[:, 1:10]\ny = bcdata[\"Target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=49865, test_size=0.25, shuffle=True\n)\n\nclass_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3).fit(\n    X_train, y_train\n)\n\ny_pred = class_tree.predict(X_test)\n\nconf = confusion_matrix(y_test, y_pred)\nprint(conf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[110  10]\n [  4  51]]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntree.plot_tree(class_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n:::\n\n# Evaluating Tree Models\n:::{.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntscores <- predict(BC_tree, type = \"class\")\nscores <- predict(BC_tree, test, type = \"class\")\n\n# Training misclassification rate\nsum(tscores != train$Target) / nrow(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03244275\n```\n:::\n\n```{.r .cell-code}\n# Test misclassification rate\nsum(scores != test$Target) / nrow(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05714286\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n\nrpart.plot(BC_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n:::\n\n# Regression Trees\n\nSame idea as classification trees but now our target is continuous. We no longer use Information / Gini since it no longers makes sense in this case. Instead, we are now trying to reduce the average sum of squares in each leaf.\n\nIn each node, the average of the observations will be calculated which is the predicted value for that node.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\nlibrary(mfp)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: survival\n```\n:::\n\n```{.r .cell-code}\ndata(bodyfat)\n```\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}