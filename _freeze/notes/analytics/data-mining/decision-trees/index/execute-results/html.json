{
  "hash": "76c7e0d92ecd76821a8138703f56db94",
  "result": {
    "markdown": "---\ntitle: Classification and Regression Trees (CART)\ndate: 10/02/2023\ndate-modified: 10/06/2023\n---\n\n\nA decision tree has a **root node** and a series of nodes splitting off from the node based on rules on the variables in the data. We typically use a binary tree as it shows empirically good results than $n$-ary tree.\n\nLeaf nodes are the final output of decision tree models.\n\n![Decision Tree Visualized](images/decision-tree.png){#fig-decision-tree}\n\nNotes on decision trees:\n\n-   Interpretable\n-   List of decisions that predict the outcome that are easy to implement\n-   Allow for nonlinear associations\n-   Allow for interactions\n-   Can handle missing values\n-   Are greedy algorithms (picks the \"best\" split at each point and settles on that split without going back)\n\n# Classification Trees\n\nClassification trees have an ordinal or categorical target. To determine the quality of a node, we use **purity**.\n\nPurity of a node is looking at how \"homogeneous\" the node is with respect to the target variable. @fig-purity shows an example of how purity is measured for the root node and one of the child nodes.\n\n![Classification Tree Purity](images/decision-tree-purity.png){#fig-purity}\n\n# Building the Tree Overview\n\n<!-- Need to put mermaid tables here -->\nA tree is built by recursively partioning the training data into successively *purer* subsets. Partiioning is done according to some condition which results in a binary split.\n\nFor categorical predictors, we need to put categories into two groups. An example for a predictor with A, B, and C levels is A vs. B and C or B vs. A and C.\n\nFor ordinal and quantitative variables, need to find the best value to split on. Data is \"binned\" into two groups and need to determine best way to bin based on the target.\n\n# Selecting the Best Split\n\n![Measures of Purity](images/purity-scale.png){#fig-purity-scale}\n\nFor a binary tree, a 50-50 split is the lowest purity we could have. The highest purity is 1.00 which means the node only contains a single level.\n\nHow do we choose the best split?\n\nLet $p(i|t)$ represent the the fraction of records belonging to class $i$ at a given node $t$. Let $c$ be the number of classes in a target variable.\n\nTwo common measures for **impurity** are **entropy** and **Gini's impurity**.\n\n## Entropy\n\n$$\n-\\sum_{i=1}^c p(i|t)\\log_2p(i|t)\n$$\n\n## Gini's Impurity\n\n$$\n1 - \\sum_{i=1}^{c} [p(i|t)]^2\n$$\n\n## Gain\n\nThe impurity of nodes is involved in calculating the **gain** of a node. Gain is measuring how much impurity we have reduced due to the split. We want gain to be high as it represents a significant difference in impurity.\n\n$$\n\\Delta = I(t) = (\\frac{n_L}{n}I(t_L) + \\frac{n_R}{n}I(t_R))\n$$\n\n-   $\\Delta$ is the gain\n-   $I(t)$ is the impurity of a parent node\n-   $I(t_L)$  and $I(t_R)$ are impurity in the left and right children nodes, respectively\n\nGain uses a weighted average between the impurity of the two children nodes.\n\n# Building Process\n\n1.  Compute the gain for all possible splits and select the best one.\n2.  Repeat process recursively until some stopping condition is met\n    1.  No splits meet some minimum Gain\n    2.  All leaves have some minimum number of observations\n    3.  A stopping condition is a way of *prepruning* the tree\n3.  Prune Tree\n\nPruning a tree refers to removing leaves / nodes in a bottom-up fashion, cutting splits with lowest gain first, while optimizing performance on validation data.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\nIn R, the children nodes are numbered $2n$ (left) and $2n + 1$ (right). The root node starts at number 1.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\nset.seed(7515)\n\nload(\"data/breast_cancer.Rdata\")\nperm <- sample(1:699)\nBC_randomOrder <- BCdata[perm, ]\ntrain <- BC_randomOrder[1:floor(0.75 * 699), ]\ntest <- BC_randomOrder[(floor(0.75 * 699) + 1):699, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nBC_tree <- rpart(Target ~ . - ID, data = train, method = \"class\", parms = list(split = \"gini\"))\nBC_tree\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nn= 524 \n\nnode), split, n, loss, yval, (yprob)\n      * denotes terminal node\n\n 1) root 524 183 0 (0.65076336 0.34923664)  \n   2) Size< 3.5 360  28 0 (0.92222222 0.07777778)  \n     4) Normal< 3.5 334   8 0 (0.97604790 0.02395210)  \n       8) Bare< 2.5 308   0 0 (1.00000000 0.00000000) *\n       9) Bare>=2.5 26   8 0 (0.69230769 0.30769231)  \n        18) CT< 3.5 16   0 0 (1.00000000 0.00000000) *\n        19) CT>=3.5 10   2 1 (0.20000000 0.80000000) *\n     5) Normal>=3.5 26   6 1 (0.23076923 0.76923077) *\n   3) Size>=3.5 164   9 1 (0.05487805 0.94512195) *\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import confusion_matrix\n\nbcdata = r.BCdata\n\nX = bcdata.iloc[:, 1:10]\ny = bcdata[\"Target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=49865, test_size=0.25, shuffle=True\n)\n\nclass_tree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=3).fit(\n    X_train, y_train\n)\n```\n:::\n\n\n:::\n\n# Evaluating Tree Models\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntscores <- predict(BC_tree, type = \"class\")\nscores <- predict(BC_tree, test, type = \"class\")\n\n# Training misclassification rate\nsum(tscores != train$Target) / nrow(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03244275\n```\n:::\n\n```{.r .cell-code}\n# Test misclassification rate\nsum(scores != test$Target) / nrow(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.05714286\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(rpart.plot)\n\nrpart.plot(BC_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ny_pred = class_tree.predict(X_test)\n\nconf = confusion_matrix(y_test, y_pred)\nprint(conf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[110  10]\n [  4  51]]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\ntree.plot_tree(class_tree)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n:::\n\n# Regression Trees\n\nSame idea as classification trees but now our target is continuous. We no longer use Information / Gini since it no longers makes sense in this case. Instead, we are now trying to reduce the average sum of squares in each leaf.\n\nIn each node, the average of the observations will be calculated which is the predicted value for that node.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata(bodyfat, package = \"TH.data\")\n\nset.seed(13172)\n\nsample <- sample.int(n = nrow(bodyfat), size = floor(0.75 * nrow(bodyfat)), replace = FALSE)\ntrain <- bodyfat[sample, ]\ntest <- bodyfat[-sample, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbody_model <- rpart(DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + kneebreadth, data = train, control = rpart.control(minsplit = 10))\nsummary(body_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\nrpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + \n    kneebreadth, data = train, control = rpart.control(minsplit = 10))\n  n= 53 \n\n          CP nsplit  rel error    xerror       xstd\n1 0.67492210      0 1.00000000 1.0387549 0.19298200\n2 0.12397892      1 0.32507790 0.3674702 0.09713363\n3 0.06337174      2 0.20109898 0.3207674 0.07055438\n4 0.04539485      3 0.13772723 0.3298414 0.07030078\n5 0.01311181      4 0.09233238 0.2635804 0.07215928\n6 0.01000000      5 0.07922057 0.2502471 0.06967234\n\nVariable importance\n   waistcirc      hipcirc  kneebreadth elbowbreadth          age \n          30           25           25           11            9 \n\nNode number 1: 53 observations,    complexity param=0.6749221\n  mean=31.98226, MSE=126.5508 \n  left son=2 (25 obs) right son=3 (28 obs)\n  Primary splits:\n      waistcirc    < 86.5   to the left,  improve=0.6749221, (0 missing)\n      hipcirc      < 109.25 to the left,  improve=0.6400649, (0 missing)\n      kneebreadth  < 9.35   to the left,  improve=0.4913650, (0 missing)\n      age          < 37     to the left,  improve=0.1897384, (0 missing)\n      elbowbreadth < 6.55   to the left,  improve=0.1514361, (0 missing)\n  Surrogate splits:\n      hipcirc      < 107.85 to the left,  agree=0.887, adj=0.76, (0 split)\n      kneebreadth  < 9.35   to the left,  agree=0.849, adj=0.68, (0 split)\n      elbowbreadth < 6.55   to the left,  agree=0.679, adj=0.32, (0 split)\n      age          < 37     to the left,  agree=0.660, adj=0.28, (0 split)\n\nNode number 2: 25 observations,    complexity param=0.04539485\n  mean=22.2016, MSE=24.60555 \n  left son=4 (11 obs) right son=5 (14 obs)\n  Primary splits:\n      hipcirc      < 96.5   to the left,  improve=0.49496490, (0 missing)\n      waistcirc    < 75.15  to the left,  improve=0.43069200, (0 missing)\n      age          < 31.5   to the left,  improve=0.33752490, (0 missing)\n      kneebreadth  < 8.55   to the left,  improve=0.24410760, (0 missing)\n      elbowbreadth < 6.65   to the left,  improve=0.08933998, (0 missing)\n  Surrogate splits:\n      waistcirc    < 72     to the left,  agree=0.76, adj=0.455, (0 split)\n      kneebreadth  < 8.25   to the left,  agree=0.76, adj=0.455, (0 split)\n      age          < 31.5   to the left,  agree=0.72, adj=0.364, (0 split)\n      elbowbreadth < 6.45   to the left,  agree=0.64, adj=0.182, (0 split)\n\nNode number 3: 28 observations,    complexity param=0.1239789\n  mean=40.715, MSE=55.90078 \n  left son=6 (25 obs) right son=7 (3 obs)\n  Primary splits:\n      kneebreadth  < 11.15  to the left,  improve=0.53126700, (0 missing)\n      hipcirc      < 109.9  to the left,  improve=0.49501800, (0 missing)\n      waistcirc    < 106    to the left,  improve=0.48547170, (0 missing)\n      elbowbreadth < 6.35   to the left,  improve=0.18322950, (0 missing)\n      age          < 64     to the left,  improve=0.06033762, (0 missing)\n\nNode number 4: 11 observations,    complexity param=0.01311181\n  mean=18.26455, MSE=16.37561 \n  left son=8 (7 obs) right son=9 (4 obs)\n  Primary splits:\n      age          < 56.5   to the left,  improve=0.48821750, (0 missing)\n      waistcirc    < 77.5   to the left,  improve=0.37437570, (0 missing)\n      hipcirc      < 94.1   to the left,  improve=0.26063300, (0 missing)\n      kneebreadth  < 8.55   to the left,  improve=0.02392090, (0 missing)\n      elbowbreadth < 6.15   to the left,  improve=0.02295546, (0 missing)\n  Surrogate splits:\n      waistcirc    < 77.5   to the left,  agree=0.818, adj=0.50, (0 split)\n      elbowbreadth < 6.55   to the left,  agree=0.818, adj=0.50, (0 split)\n      hipcirc      < 91.5   to the right, agree=0.727, adj=0.25, (0 split)\n\nNode number 5: 14 observations\n  mean=25.295, MSE=9.323925 \n\nNode number 6: 25 observations,    complexity param=0.06337174\n  mean=38.8272, MSE=23.49564 \n  left son=12 (11 obs) right son=13 (14 obs)\n  Primary splits:\n      hipcirc      < 109.9  to the left,  improve=0.72361790, (0 missing)\n      waistcirc    < 98.75  to the left,  improve=0.46229770, (0 missing)\n      elbowbreadth < 6.35   to the left,  improve=0.24168590, (0 missing)\n      kneebreadth  < 9.9    to the left,  improve=0.22697980, (0 missing)\n      age          < 60.5   to the left,  improve=0.03275597, (0 missing)\n  Surrogate splits:\n      waistcirc    < 98.75  to the left,  agree=0.84, adj=0.636, (0 split)\n      elbowbreadth < 6.45   to the left,  agree=0.76, adj=0.455, (0 split)\n      kneebreadth  < 8.75   to the left,  agree=0.68, adj=0.273, (0 split)\n      age          < 49.5   to the right, agree=0.60, adj=0.091, (0 split)\n\nNode number 7: 3 observations\n  mean=56.44667, MSE=48.76009 \n\nNode number 8: 7 observations\n  mean=16.12714, MSE=8.875049 \n\nNode number 9: 4 observations\n  mean=22.005, MSE=7.515725 \n\nNode number 12: 11 observations\n  mean=34.17545, MSE=7.173207 \n\nNode number 13: 14 observations\n  mean=42.48214, MSE=5.959931 \n```\n:::\n\n```{.r .cell-code}\nprintcp(body_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\nrpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + \n    kneebreadth, data = train, control = rpart.control(minsplit = 10))\n\nVariables actually used in tree construction:\n[1] age         hipcirc     kneebreadth waistcirc  \n\nRoot node error: 6707.2/53 = 126.55\n\nn= 53 \n\n        CP nsplit rel error  xerror     xstd\n1 0.674922      0  1.000000 1.03875 0.192982\n2 0.123979      1  0.325078 0.36747 0.097134\n3 0.063372      2  0.201099 0.32077 0.070554\n4 0.045395      3  0.137727 0.32984 0.070301\n5 0.013112      4  0.092332 0.26358 0.072159\n6 0.010000      5  0.079221 0.25025 0.069672\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain = r.train\ntest = r.test\n\nX_train = train[[\"age\", \"waistcirc\", \"hipcirc\", \"elbowbreadth\", \"kneebreadth\"]]\nX_test = test[[\"age\", \"waistcirc\", \"hipcirc\", \"elbowbreadth\", \"kneebreadth\"]]\ny_train = train[\"DEXfat\"]\ny_test = test[\"DEXfat\"]\n\nregressor = tree.DecisionTreeRegressor(random_state=12356, max_depth=4)\nreg_tree = regressor.fit(X_train, y_train)\n```\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimportance = regressor.feature_importances_\n\nfor i, v in enumerate(importance):\n    print(f\"Feature {i}, Score: {v:.5f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFeature 0, Score: 0.01353\nFeature 1, Score: 0.70797\nFeature 2, Score: 0.12906\nFeature 3, Score: 0.00809\nFeature 4, Score: 0.14136\n```\n:::\n:::\n\n\n:::\n\n## Pruning Tree\n\nTo prune the tree, there are different criteria that we can use. Two of the common methods:\n\n-   Go to the minimum value of \"xerror\" (cross-validation error)\n-   Use the 1-SE rule\n    -   Use the standard error of the crossvalidation error to find what is within 1 standard error of the lowest value and prune to that value\n\n\nIn R, we select the CP of the node we want to prune to:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbody_model2 <- prune(body_model, cp = 0.013112)\nprintcp(body_model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nRegression tree:\nrpart(formula = DEXfat ~ age + waistcirc + hipcirc + elbowbreadth + \n    kneebreadth, data = train, control = rpart.control(minsplit = 10))\n\nVariables actually used in tree construction:\n[1] hipcirc     kneebreadth waistcirc  \n\nRoot node error: 6707.2/53 = 126.55\n\nn= 53 \n\n        CP nsplit rel error  xerror     xstd\n1 0.674922      0  1.000000 1.03875 0.192982\n2 0.123979      1  0.325078 0.36747 0.097134\n3 0.063372      2  0.201099 0.32077 0.070554\n4 0.045395      3  0.137727 0.32984 0.070301\n5 0.013112      4  0.092332 0.26358 0.072159\n```\n:::\n:::\n\n\n# Advantages and Disadvantages\n\nAdvantages:\n\n-   Explainability\n-   Can handle missing values\n-   Can be used for variable selection\n-   Great for ensembles\n-   No assumptions to verify\n-   Generally immune to scale of input variables / standardization\n-   Generally immune to the effect of outliers or high leverage observations\n-   Can handle correlated inputs\n\nDisadvantages:\n\n-   Simplistic regression / decision surface\n-   All variables are forced to interact\n-   Greedy algorithm\n    -   Cannot return globally optimal tree\n-   Can be unstable (sensitive to small changes in input) both when training and making predictions\n\n# Conditional Trees\n\nConditional trees select the best variable based on which variable is **most associated with the response** with p-value adjustment. Once selected, the optimal split is chosen for that variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(partykit)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: grid\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: libcoin\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: mvtnorm\n```\n:::\n\n```{.r .cell-code}\nset.seed(7515)\nperm <- sample(1:699)\nBC_randomOrder <- BCdata[perm, ]\ntrain <- BC_randomOrder[1:floor(0.75 * 699), ]\nmodel1 <- ctree(Target ~ . - ID, data = train)\nplot(model1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\nConditional trees can be used to find bins by creating a model with the variable you want to bin:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- ctree(Target ~ Size, data = train)\nplot(model2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\nIn this case the bins are:\n\n-   Size $\\leq$ 1\n-   1 < Size $\\leq$ 2\n-   2 < Size $\\leq$ 3\n-   And so on!",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}