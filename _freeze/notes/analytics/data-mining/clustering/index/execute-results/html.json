{
  "hash": "75b2930c8208971e00ccd361c0e9e44b",
  "result": {
    "markdown": "---\ntitle: Clustering\ndate: 10/06/2023\n---\n\n\n**Clustering** is an unsupervised approach to modeling where the goal is to partition the data into groups.\n\n-   Observations within a cluster are similar in some sense\n-   Observations in difference clusters are different in some sense\n\nThere is no one-size-fits-all solutions, but there are good and bad cluster solutions. No method works best all the time. Keep in mind that clustering uses **ALL** the variables you provide it and clusters should add some business value.\n\n# Examples of Clustering\n\n-   Customer segmentation: Groups of customers with similar shopping or buying patterns\n-   Dimension reduction: Cluster individuals together and use cluster variable as proxy for demographic or behavioral variables\n-   Gather stores with similar characteristics for sales forecasting\n-   Find topics in text data\n-   Find communities in social networks\n\n# Hard vs. Fuzzy\n\n**Hard** clustering is characterized by objects only belonging to one cluster. **Fuzzy** clustering is characterized by objects having the capability to belong to more than one cluster (usually with some probability).\n\nHard:\n\n-   k-means\n-   DBSCAN\n-   Hierarchical\n\nFuzzy:\n\n-   Fuzzy C-means\n-   Gaussian Mixture Models / Expectation-Maximization\n\n# Hierarchical vs. Flat\n\n**Hierarchical** clusters form a tree so you can visually see which clusters are most similar to each other. **Flat** clusters are created according to some apriori process, usually iteratively updating cluster assignments.\n\nHierarchical is expensive when you have a large dataset.\n\n# K-Means\n\nK-Means revolves around using **centroids**. Centroids are \"centers of clusters\" or the means of a group of observations.\n\n![K-Means Clustering](images/k-means.png){#fig-k-means}\n\nWith K-Means we are trying to minimize the sum of squared distances from each point to its cluster centroid.\n\n$$\n\\sum_{C_k}\\sum_{x_i \\in C_k} \\lVert x_i - c_k \\rVert^2\n$$\n\n## Algorithm\n\n1.  Randomly initialize **k** points.\n2.  Assign each data point to the closest seed point.\n3.  The seed point then represents a cluster of data.\n4.  Reset seed points to be the centroids of the cluster by taking the **average** of all data points belonging to the cluster.\n5.  Repeat steps 2-4 updating the cluster centroids until they do not change.\n\nHow can we determine the number of clusters we should use?\n\nWe can use an \"elbow\" plot to find a place where the marginal benefit to objective function for adding a cluster becomes small.\n\n![Elbow Plot](images/elbow-plot.png){#fig-elbow-plot}\n\n## Advantages vs. Disadvantages\n\nAdvantages:\n\n-   Modest time/storage requirements\n-   Shown you can terminate method after small number of iterations with good results\n-   Good for wide variety of data types\n\nDisadvantages\n\n-   Dependent on initialization (different runs can provide different results)\n-   Can be sensitive to outliers as we are based centroids on the average of points\n    -   Consider using k-medoids\n-   Have to input the number of clusters\n-   Difficulty detecting non-spheroidal (globular) clusters\n\n## Preprocessing\n\nYou will need to do data epxloration before trying to cluster the data.\n\n-   Missing Values\n    -   You will need to impute or remove missing values for the variable\n-   Categorical Variables\n    -   Do you need to bin categorical variables?\n    -   You need to one-hot encode before putting them into algorithm\n-   Continuous Variables\n    -   If outliers or heavy skewness, potentially consider transformation\n    -   At a minimum, center the continuous variables after any transformations are made\n\nYou can try clustering on original data or you can try it on PCA of the variables, particularly if the data is big.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nhist(USArrests$Murder)\nhist(USArrests$Assault)\nhist(USArrests$Rape)\nhist(USArrests$UrbanPop)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/explore-plots-1.png){width=672}\n:::\n\n```{.r .cell-code}\narrest_scal <- scale(USArrests)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nclus2 <- kmeans(arrest_scal, centers = 2, nstart = 25)\nclus2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-means clustering with 2 clusters of sizes 30, 20\n\nCluster means:\n     Murder    Assault   UrbanPop       Rape\n1 -0.669956 -0.6758849 -0.1317235 -0.5646433\n2  1.004934  1.0138274  0.1975853  0.8469650\n\nClustering vector:\n       Alabama         Alaska        Arizona       Arkansas     California \n             2              2              2              1              2 \n      Colorado    Connecticut       Delaware        Florida        Georgia \n             2              1              1              2              2 \n        Hawaii          Idaho       Illinois        Indiana           Iowa \n             1              1              2              1              1 \n        Kansas       Kentucky      Louisiana          Maine       Maryland \n             1              1              2              1              2 \n Massachusetts       Michigan      Minnesota    Mississippi       Missouri \n             1              2              1              2              2 \n       Montana       Nebraska         Nevada  New Hampshire     New Jersey \n             1              1              2              1              1 \n    New Mexico       New York North Carolina   North Dakota           Ohio \n             2              2              2              1              1 \n      Oklahoma         Oregon   Pennsylvania   Rhode Island South Carolina \n             1              1              1              1              2 \n  South Dakota      Tennessee          Texas           Utah        Vermont \n             1              2              2              1              1 \n      Virginia     Washington  West Virginia      Wisconsin        Wyoming \n             1              1              1              1              1 \n\nWithin cluster sum of squares by cluster:\n[1] 56.11445 46.74796\n (between_SS / total_SS =  47.5 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(factoextra)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n```\n:::\n\n```{.r .cell-code}\nfviz_cluster(clus2, data = arrest_scal)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/pca-plot-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n:::\n\n# Silhouette Method\n\n**Silhouette** is another method to define the number of clusters to use. This method estimates how well each observations falls within its cluster (distance point is to all other points in cluster and compare it to distance from that point to points in other clusters). \n\nWe want to find the number of clusters that **maximizes this ratio**.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nfviz_nbclust(arrest_scal, kmeans, method = \"silhouette\", k.max = 9)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\nFrom silhouette, the optimal number of clusters selected is 2. \n\nWe can profile our original data by attaching the clusters to it:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprofile_kmeans <- cbind(USArrests, clus2$cluster)\nall_k <- profile_kmeans %>%\n    group_by(clus2$cluster) %>%\n    summarise(mean_assault = mean(Assault), mean_murder = mean(Murder), mean_rape = mean(Rape), mean_pop = mean(UrbanPop))\nall_k\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 Ã— 5\n  `clus2$cluster` mean_assault mean_murder mean_rape mean_pop\n            <int>        <dbl>       <dbl>     <dbl>    <dbl>\n1               1         114.        4.87      15.9     63.6\n2               2         255.       12.2       29.2     68.4\n```\n:::\n:::\n\n\n# Hierarchical Clustering\n\nEvery point starts as its own cluster and then we build up clusters as hierarchies. There are several different distant measures we can use, but the two we focus on are **Euclidean distance** and **Manhattan distance**.\n\nEuclidean distance:\n\n$$\n\\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n$$\n\nManhattan distance:\n\n$$\n|x_1 - x_2| + |y_1 - y_2|\n$$\n\n## Hierarchical Algorithm\n\n1.  Each point starts as its own cluster ($n$ clusters).\n2.  Calculate the distance between each point using the distance measure.\n3.  Choose two points that are the closest and form a cluster ($n - 1$ clusters).\n4.  Calculate distance between all single points and all clustered points.\n5.  Find smallest distance and combine to form a cluster.\n\n:::{layout-ncol=\"2\"}\n![Hierarchical Clustering Step 1](images/hierarchical-step-1.png)\n\n![Hierarchical Clustering Step 2](images/hierarchical-step-2.png)\n:::\n\nOnce all points are part of a multi-point cluster we then need to agglomerate clusters.\n\n![Hierarchical Clustering Agglomerative](images/hierarchical-agglom.png)\n\n## Linkages\n\nHow do we measure the distance between a point / cluster and a cluster?\n\n### Single Linkage\n\nDistance between the closest points in the clusters.\n\n![Single Linkage](images/single-linkage.png)\n\n### Complete Linkage\n\nDistance between the farthest points in the clusters.\n\n![Complete Linkage](images/complete-linkage.png)\n\n### Centroid Linkage\n\nDistance between the centroids (means) of each cluster.\n\n![Centroid Linkage](images/centroid-linkage.png)\n\n### Average Linkage\n\nAverage distance between all points in clusters.\n\n![Average Linkage](images/average-linkage.png)\n\n### Ward's Method\n\nMinimize SSE within the cluster compared to the centroid.\n\n$$\n\\sum_{j=1}^{N_i} \\lvert x_j - c_i \\rvert^2\n$$\n\n![Ward's Method](images/wards-method.png)\n\n## Advantages and Disadvantages\n\nAdvantages:\n\n-   Creates hierarchy that can help choose the number of clusters and examine how those clusters relate to each other.\n-   Do NOT need to know number of clusters apriori\n\nDisadvantages:\n\n-   Computationally intensive, large storage reqs., not good for large datasets.\n-   Lacks global objective function: only makes decision based on local criteria.\n-   Greedy algorithm. Merging decisions are final once a point is assigned to a cluster.\n-   Poor performance on noisy or high-dimensional data like text.\n\nAt the end of the day, you should always evaluate clustering algorithms based on how it makes sense in the business context.\n\n![Hierarchical Clustering Options](images/hierarchical-options.png)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cluster)\n\ndist_assault <- dist(arrest_scal, method = \"euclidean\")\nh1_comp_eucl <- agnes(dist_assault, method = \"complete\")\npltree(h1_comp_eucl, cex = 0.6, hang = -1, main = \"Dendrogram of agnes\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\nWe can also get the agglomerative coefficient which measures how strong the clustering structure is (want values close to 1):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(h1_comp_eucl$ac)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8531583\n```\n:::\n:::\n\n\nLoop through possibilities of linkage methods:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmethod <- c(average = \"average\", single = \"single\", complete = \"complete\", ward = \"ward\")\n\nac <- function(x) {\n    agnes(dist_assault, method = x)$ac\n}\n\nlapply(method, ac)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$average\n[1] 0.7379371\n\n$single\n[1] 0.6276128\n\n$complete\n[1] 0.8531583\n\n$ward\n[1] 0.934621\n```\n:::\n:::\n\n\nAlthough we can find the best mathematical cluster here, we want the best algorithm for business sense. Explore profiles to see which makes the most sense.\n\n# DBSCAN\n\nDensity-based spatial clustering of applications with noise. Groups together points that are close to each other based on a distance measure, a minimum number of points (\"neighbors\"), and a \"neighborhood about each point.\"\n\nPoints not near other points are deemed outliers. A cluster of points must have a minimum number of points around it to be considered a cluster.\n\n# Variable Clustering\n\nCluster variables that are related to reduce redundancies or multicollinearity. This is also a form of dimension reduction. Variable clustering uses eigenvalues to identify similar values and assess the goodness of the partition.\n\nIn R, the package is `ClustOfVar`. R can handle quantitative and qualitative variables, but you need to split these into two data matrices first. \n\n<!-- ```{r}\nlibrary(readr)\nlibrary(ClustOfVar)\ntelco <- read_csv(\"https://raw.githubusercontent.com/sjsimmo2/DataMining-Fall/master/TelcoChurn.csv\")\n\ntelco[is.na(telco)] = 0\nquant_var <- telco[,c(3,6,19,20)]\nqual_var <- telco[,c(2,4,5,6:18)]\n\nvar_clust_h <- hclustvar(quant_var, qual_var)\nstab <- stability(var_clust_h, B=50)\nplot(stab)\n``` -->",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}