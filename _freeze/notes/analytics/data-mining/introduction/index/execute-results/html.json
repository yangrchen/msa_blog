{
  "hash": "f0ba3c09f0351f06dd755bdc849f0f5e",
  "result": {
    "markdown": "---\ntitle: Introduction to Data Mining and Association Analysis\ndate: 09/27/2023\n---\n\n\n# Splitting Data\n\nWhen doing a supervised technique, it is important to split the data into training, validation, and test sets. We want the model to be generalizable and predict equally well on out-of-sample data.\n\nIf we're using an unsupervised technique like clustering analysis, it is not necessary to split the data.\n\nHow do you know when you do NOT have enough data? There are no hard rules, but it's good to have at least 10 observations per variable.\n\n## Training / Validation\n\nUse the training data to build your model. Evaluate and tune the model based on how it performs on the validation data (careful not to train on the validation data). Do not report accuracy measures from the training data--best to state on the **test** data.\n\nModel creation should be on the training data and the applied to validation to see if you might need to enhance it. Once a final model is chosen, retrain the model on training + validation data to finalize parameters. Use this model to run on the test data.\n\nBefore deployment, you can use ALL data to update parameters.\n\n# K-Fold Cross Validation\n\n1.  Divide your data into $k$ equally-sized folds\n2.  For each fold, train the model on all other data, using htat fold as a validation set\n3.  Record measures of error / accuracy\n4.  In the end, report summary of error / accuracy\n5.  Use that report summary to choose a model\n\n![10-fold Cross Validation Example](images/10-fold-cv.png){#fig-10-fold}\n\n# Bootstrapping\n\n**Bootstrapping** holds the assumption that the data we have is all the information we have for the population. The idea is to keep sampling the data **with replacement** and the **same sample size** to estimate the distribution of the quantity of interest.\n\n![Bootstrap Resampling](images/bootstrap-resampling.png){#fig-bootstrap-resampling}\n\nThe assumptions of bootstrapping are that the samples are **representative** and they observations are **independent**.\n\n## Variability of the Median\n\nWant to estimate the standard error of the median. We could also do this for any other statistic:\n\n1.  Get a bootstrap sample of the data (sample with replacement)\n2.  Calculate statistic of interest\n3.  Repeat steps 1 and 2 over and over to get the distribution of the median\n4.  Get quantiles of the distribution\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(datasets)\nlibrary(tidyverse)\n\nact_med <- median(co2)\nboot_med <- vector(length = 10000)\nfor (i in 1:length(boot_med)) {\n    boot_samp <- sample(co2, replace = TRUE)\n    boot_med[i] <- median(boot_samp)\n}\n\nsd(boot_med)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.364961\n```\n:::\n\n```{.r .cell-code}\nact_med\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 335.17\n```\n:::\n\n```{.r .cell-code}\nquantile(boot_med, probs = c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   2.5%   97.5% \n332.675 337.815 \n```\n:::\n\n```{.r .cell-code}\nggplot(data.frame(boot_med), aes(x = boot_med)) +\n    geom_histogram() +\n    labs(x = \"Bootstrap Sample\", y = \"Frequency\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/bootstrap-median-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n:::\n\n## Using Bootstrap to Test Significant Differences\n\nWe can use bootstrap to get the distribution of differences in medians (we can see if confidence interval includes 0).\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\names <- make_ordinal_ames()\n\ndiff_stat <- vector(length = 10000)\nyes <- ames %>%\n    filter(Central_Air == \"Y\") %>%\n    pull(Sale_Price)\n\nno <- ames %>%\n    filter(Central_Air == \"N\") %>%\n    pull(Sale_Price)\n\nfor (i in 1:10000) {\n    yes_vec <- median(sample(yes, length(yes), replace = TRUE))\n    no_vec <- median(sample(no, length(no), replace = TRUE))\n}\ndiff_stat[i] <- yes_vec - no_vec\n```\n:::\n\n\n# Python\n\n:::\n\n# Adjusting P-Values\n\nIf you are doing a lot of hypothesis testing, then you need to be aware of inflating your Type I error. Family-wise error rate is the same idea as we are controlling the overall probability of making a Type I error. \n\n**Bonferroni adjustment** correct for this by multiplying p-values by the number of tests you are doing--these are adjusted p-values.\n\n## False Discovery Rate (FDR)\n\nRecall that significance level, $\\alpha$, controls the Type I error rate for an individual hypothesis.\n\nThe **false discovery rate** controls the **rate** of Type I errors. This is the expected proportion of \"false discoveries\".\n\n# Dealing with Transactional Data\n\nTransactional data is long and has many rows per modeling observation. For example, the same person could have multiple bank deposits in the table.\n\nTypically, the solution for modeling with transactional data is to \"roll it up\" so it has one row per observation modeled. We transform the data from long to **wide** by grouping the data.\n\n# Data Cleaning\n\n## Missing Values\n\n**Highly Recommend:** Create a flag to indicate which values are missing and which ones are not (sometimes, missingness is informative)\n\n**Numeric:** Consider how much of the variable is missing (if over 50% consider how much information this variable is giving). If you want to keep the variable, you can either impute values or bin the variables and create a separate bin for missing values.\n\n**Categorical / Ordinal:** Consider creating a \"bin\" for missing values, but if too much is missing this can be a HUGE bin.\n\nIn any case, you have to always explore your data to see if the route you took is sensible.\n\n![Imputing Missing Values](images/imputing-missing.png){#fig-imputing-missing}\n\n# Transformations and Standardizations\n\n## Binning Numeric Variables\n\n### Unsupervised Approaches\nOne unsupervised approach is to bin the variable based on equal-width bins. Each bin has the same width in variable values, but each bin has different number of observations.\n\nWe can also do **equal depth** where we take percentiles of the population and each bin has the same number of observations.\n\n### Supervised Approaches\n\nWe can use target variable info to \"optimally\" bin numeric variables for prediction. WE typically do this in classification problems. \n\nDecision trees can also create these bins or we can use weight of evidence.\n\n## Standardization and Normalization\n\nStandardization in statistics transform units to \"number of standard deviations away from the mean\" to put variables on the same scale:\n\n$$\n\\frac{x - \\bar{x}}{\\sigma_x}\n$$\n\nThere are many different ways to standardize / normalize:\n\n-   Range standardization\n-   Min-Max standardization\n-   Divide by 2-nomr, 1-norm, divide by sum\n\n# Association Analysis\n\nAssociation analysis looks at relationships between items. How often do we see these items occurring together?\n\nThis is an unsupervised approach as there is no target or outcome variable for training. For example, based on a set of product orders association analysis gives us sets of products that are likely to be purchased together.\n\nIn order to find these relationships, you need to have your transactional data rolled up by ID. `{bread, egg, oat packet, papaya}  1`\n\nWe have **rules** that we are focused on quantifying: Butter $\\longrightarrow$ Bread is interpreted as for those who buy butter, do they tend to also buy bread? The left hand side is the **antecedent** and the right hand side is the **consequent**.\n\n## Quantifying Association Rules\n\n1.  Support: $P(A \\cap B)$ measures how often we find instances of this rule in the data\n2.  Confidence: $P(B|A) = \\frac{P(A \\cap B)}{P(A)}$ measures what percent of transactions containing A also contain B\n3.  Lift: $\\frac{P(B|A)}{P(B)} = \\frac{P(A \\cap B)}{P(A)P(B)}$ measures how much **more likely we are to buy B given that we also buy A than we are to buy B at random**\n    -  Want lift values greater than 1\n\n## Post-Hoc\n\n:::{.text-center}\nProduct A $\\longrightarrow$ Product B\n:::\n\nProduct B as a consequent helps us determine what can be done to boost its sales. Product A as an antecedent helps us determine what other products would be affected by changes to product A.\n\n## Direction of Association\n\nEither direction has the same support and same lift, but **different confidence**. We do not say those who buy A will then buy B.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(arules)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nuse_condaenv(\"msa\")\n\ntemp_dat <- read.table(\"https://raw.githubusercontent.com/sjsimmo2/DataMining-Fall/master/Grocery1.csv\", sep = \",\", header = TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntrans_dat <- as(split(temp_dat$Grocery, temp_dat$ID), \"transactions\")\ninspect(trans_dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     items                             transactionID\n[1]  {bread, egg, oat packet, papaya}  1            \n[2]  {bread, milk, oat packet, papaya} 2            \n[3]  {bread, butter, egg}              3            \n[4]  {egg, milk, oat packet}           4            \n[5]  {bread, butter, milk}             5            \n[6]  {milk, papaya}                    6            \n[7]  {bread, butter, papaya}           7            \n[8]  {bread, egg}                      8            \n[9]  {oat packet, papaya}              9            \n[10] {bread, milk, papaya}             10           \n[11] {egg, milk}                       11           \n```\n:::\n:::\n\n\nAlways make sure to check labels:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrans_dat@itemInfo$labels\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"bread\"      \"butter\"     \"egg\"        \"milk\"       \"oat packet\"\n[6] \"papaya\"    \n```\n:::\n:::\n\n\nWe can create an item frequency plot for the top 3 items:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nitemFrequencyPlot(trans_dat, topN = 3, type = \"absolute\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nNow we have our transaction table and we can get the rules using the `apriori` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrules <- apriori(trans_dat, parameter = list(supp = 0.1, conf = 0.001, target = \"rules\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nApriori\n\nParameter specification:\n confidence minval smax arem  aval originalSupport maxtime support minlen\n      0.001    0.1    1 none FALSE            TRUE       5     0.1      1\n maxlen target  ext\n     10  rules TRUE\n\nAlgorithmic control:\n filter tree heap memopt load sort verbose\n    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n\nAbsolute minimum support count: 1 \n\nset item appearances ...[0 item(s)] done [0.00s].\nset transactions ...[6 item(s), 11 transaction(s)] done [0.00s].\nsorting and recoding items ... [6 item(s)] done [0.00s].\ncreating transaction tree ... done [0.00s].\nchecking subsets of size 1 2 3 done [0.00s].\nwriting ... [32 rule(s)] done [0.00s].\ncreating S4 object  ... done [0.00s].\n```\n:::\n\n```{.r .cell-code}\nrules <- sort(rules, by = \"confidence\", decreasing = TRUE)\ninspect(rules[1:4])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    lhs                    rhs      support   confidence coverage  lift    \n[1] {butter}            => {bread}  0.2727273 1.0000000  0.2727273 1.571429\n[2] {bread, oat packet} => {papaya} 0.1818182 1.0000000  0.1818182 1.833333\n[3] {oat packet}        => {papaya} 0.2727273 0.7500000  0.3636364 1.375000\n[4] {papaya}            => {bread}  0.3636364 0.6666667  0.5454545 1.047619\n    count\n[1] 3    \n[2] 2    \n[3] 3    \n[4] 4    \n```\n:::\n:::\n\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}