{
  "hash": "6a5903d1e5237c31eb893e514e3a1fa8",
  "result": {
    "markdown": "---\ntitle: KNN and Other Ideas\ndate: 10/13/2023\n---\n\n\n# K-Nearest Neighbor\n\nWe want to identify several cases that are most similar to a given observation. We can also use the information from \"neighbors\" to classify/predict the new observation.\n\n![K-Nearest Neighbors](images/knn.png){#fig-knn}\n\n## Considerations\n\n-   How should I measure nearness?\n    -   Numeric attributes?\n    -   Ordinal attributes?\n    -   Categorical attributes?\n    -   How to combine all attributes?\n-   How should I combine the results of neighbors?\n    -   Classification\n        -   Majority rules?\n        -   Weight votes by nearness?\n    -   Prediction\n        -   Mean\n        -   Median      \n-   How many neighbors should I use?\n\n## Choosing $k$\n\nSmaller values of $k$ lead to higher variance which tends toward overfitting. Larger values of $k$ leadto higher bias which tends toward underfitting.\n\nCommon practice: use $k = \\sqrt{n}$ where $n$ is the number of training examples.\n\nBest practice to tune this parameter with a validation set or with cross-validation.\n\n## Advantages and Disadvantages\n\nAdvantages:\n\n-   Easy to explain, intuitive, understandable\n-   Applicable to any type of data\n-   Makes no assumptions about the underlying distribution of the data\n-   Large/representative training set is only assumption\n\nDisadvantages:\n\n-   Computationally expensive in classification phase\n-   Requires storage for the training set\n-   Results dependent on choice of distance function, combinatino function, and number of neighbors, $k$\n-   Susceptible to noise\n-   Require lots of data preprocessing and consideration for distance metrics\n-   Does not produce a model and so it does not help us understand how features are related to classes\n\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(class)\n\nset.seed(7515)\nload(\"data/breast_cancer.Rdata\")\n\nperm <- sample(1:699)\nBC_randomOrder <- BCdata[perm,]\n\ntrain <- BC_randomOrder[1:floor(0.75 * 699), -c(1, 7)]\ntest <- BC_randomOrder[(floor(0.75 * 599) + 1):699, -c(1, 7)]\n\ntrain_x <- subset(train, select=-Target)\ntrain_y <- as.factor(train$Target)\n\ntest_x <- subset(test, select=-Target)\ntest_y <- as.factor(test$Target)\n\npredict_test <- knn(train_x, test_x, train_y, k = 5)\nsum(predict_test == test_y) / length(test_y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.96\n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\ntrain_x = r.train_x\ntrain_y = r.train_y\n\ntest_x = r.test_x\ntest_y = r.test_y\n\nknn = KNeighborsClassifier(n_neighbors=5).fit(train_x, train_y)\ny_pred = knn.predict(test_x)\nprint(\"Accuracy with k = 5: \", accuracy_score(test_y, y_pred) * 100.0)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy with k = 5:  96.0\n```\n:::\n:::\n\n\n:::\n\n# Multidimensional Scaling (MDS)\n\nMDS is a technique for visualizing high-dimensional data by projecting it into a lower-dimensional space. It is a non-linear dimensionality reduction technique similar to PCA. \n\nTo perform MDS, we need a dissimilarity matrix (or distance matrix).\n\n## Classical MDS\n\nClassical MDS is a method for finding a low-dimensional representation of the data that preserves the distances between points as well as possible.\n\n## Non-metric MDS\n\nNon-metric MDS is a method for finding a low-dimensional representation of the data that preserves the rank order of the distances between points as well as possible. Think about \"squashing pictures\" to make them fit on a page.\n\n## Difference between PCA and MDS\n\nPCA is more focused on dimensions themselves (wants to explain maximum variance) where MDS is more focused on relations among scaled objects.\n\nTo visualize data, we may prefer MDS over PCA because MDS preserves the distances between points. However, if the data will be used for analysis then PCA should be used.\n\n# Curse of Dimensionality\n\nWhen we have a large number of predictors, finding the true signal is difficult. Can be hidden in all of the dimensions. In training, it could look like the model is getting better, but in reality we are just adding noise.\n\n# Ensemble Methods\n\nYou have a number of models and you combine their predictions--this is **ensemble**.\n\nLet's say we create a decision tree, logistic model, and KNN model. We can combine the predictions from these models to create a final prediction by averaging or weight-averaging the probabilities. This is called **model averaging**. We could also do majority rules or proportion voting.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}