{
  "hash": "1bf5a1417505de9e936fb12b8a3e02f3",
  "result": {
    "markdown": "---\ntitle: Multiple Linear Regression\ndate: 07/06/2023\neditor:\n    render-on-save: false\n---\n\n\nModels with more than one predictor variable are called **multiple regression models**.\n\n$$\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\cdots + \\beta_kx_k + \\varepsilon\n$$\n\n-   Estimate $\\hat{\\beta}_j$ is the predicted average change in $y$ with a one unit in crease in $x_j$ given all other variables are held constant.\n\nWe are still trying to minimize the sum of squared errors:\n\n$$\nSSE = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n$$\n\nLinear in MLR refers to the linear combination of variables in the model--not how they're visualized in multiple dimensions.\n\nA model like $y = \\beta_0 + \\beta_1x_1 + \\beta_1x_1^2$ is still a linear regression!\n\n# Global F-Test\n\nIn SLR, we could use a single t-test to determine model utility. In MLR, we have the Global F-Test to determine if an overall model is useful for predicting $y$ overall.\n\n::: {.text-center}\n$H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0$\n\n$H_a:$ At least one variable is useful in predicting the target\n:::\n\nThe F-statistic is calculated as:\n\n$$\nF = \\frac{(\\frac{SSR}{k})}{(\\frac{SSE}{n - k - 1})}\n$$\n\n-   $\\frac{SSR}{k}$ is the average amount of variation each variable explains\n-   $\\frac{SSE}{n - k - 1}$ is the average amount of variation left per data point\n\nWhen it comes to comparing which variable is more important. You should **not** look at the parameter estimates. Instead, compare the t-values which do not change based on the scale of the variable. \n\n## R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(reticulate)\n\nuse_condaenv(\"blues_clues\")\n\names <- make_ordinal_ames()\nset.seed(123)\names <- ames |> mutate(id = row_number())\ntrain <- ames |> sample_frac(0.7)\ntest <- anti_join(ames, train, by = \"id\")\n\names_lm2 <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, train)\nsummary(ames_lm2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-528656  -30077   -1230   21427  361465 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    42562.657   5365.721   7.932 3.51e-15 ***\nGr_Liv_Area      136.982      4.207  32.558  < 2e-16 ***\nTotRms_AbvGrd -10563.324   1370.007  -7.710 1.94e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56630 on 2048 degrees of freedom\nMultiple R-squared:  0.5024,\tAdjusted R-squared:  0.5019 \nF-statistic:  1034 on 2 and 2048 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n## Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport pandas as pd\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\names = pd.read_csv(\"../../../data/ames.csv\")\ntrain, test = train_test_split(ames, random_state=123)\ntrain.head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        Id  MSSubClass MSZoning  ...  SaleType  SaleCondition SalePrice\n1446  1447          20       RL  ...        WD         Normal    157900\n1123  1124          20       RL  ...        WD         Normal    118000\n186    187          80       RL  ...        WD         Normal    173000\n1020  1021          20       RL  ...        WD         Normal    176000\n67      68          20       RL  ...        WD         Normal    226000\n\n[5 rows x 81 columns]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\names_lm2 = smf.ols(\"SalePrice ~ GrLivArea + TotRmsAbvGrd\", train).fit()\n\names_lm2.f_pvalue\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n9.720219891564917e-163\n```\n:::\n:::\n\n\n# Evaluating MLR\n\n## Assumptions\n\n-   The mean of $y$ is accurately modeled by a linear function of the independent variables\n-   $\\varepsilon$ is Normal with a mean of 0\n-   $\\varepsilon$ has a constant variance\n-   The errors are independent\n-   No perfect collinearity\n\n## Multicollinearity\n\n**Multicollinearity** is when predictor variables are correlated with one another.\n\nNo **perfect** collinearity means no predictor variables as a perfect linear combination of each other. In practice, we only care when collinearity has a significant impact.\n\n### R Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow = c(2, 2))\nplot(ames_lm2)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n### Python Code\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain[\"resid\"] = ames_lm2.resid\ntrain[\"predict\"] = ames_lm2.predict()\nax = sns.relplot(train, x=\"predict\", y=\"resid\")\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=480}\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nsm.qqplot(train[\"resid\"])\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-3.png){width=672}\n:::\n:::\n\n\n# MLR vs. SLR\n\nWe can investigate more independent variables simultaneously. However, there is increased complexity which makes it difficult to determine which model is \"best\".\n\n# What is MLR Good At?\n\n## Predict\n\nDevelop a model to predict future values of a response variable based on its relationship with other predictor variables.\n\nThe parameters in the model and their statistical significance are secondary importance. Focus is on producing a model that can predict future values well.\n\nWe should take care to be aware of and address overfitting a model in this case.\n\n## Explain\n\nTo develop an understanding of relationships between the response and predictor.\n\nThe statistical significance of coefficients as well as the magnitudes and signs of coefficients are important.\n\n# The $R^2$ Problem\n\nIn MLR, the addition of any variable will make the $R^2$ value increase regardless of the actual strength of the variable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmlr_random <- lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), 0, 1), train)\n\nsummary(mlr_random)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd + rnorm(length(Sale_Price), \n    0, 1), data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-528651  -29901   -1274   21236  359999 \n\nCoefficients:\n                                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      42589.405   5365.920   7.937 3.38e-15 ***\nGr_Liv_Area                        137.023      4.208  32.565  < 2e-16 ***\nTotRms_AbvGrd                   -10578.883   1370.137  -7.721 1.79e-14 ***\nrnorm(length(Sale_Price), 0, 1)  -1207.529   1269.595  -0.951    0.342    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 56630 on 2047 degrees of freedom\nMultiple R-squared:  0.5026,\tAdjusted R-squared:  0.5019 \nF-statistic: 689.4 on 3 and 2047 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n-   $R^2$ increased despite the insignificant variable!\n\n## Adjusted Coefficient of Determination\n\n$R_a^2$ penalizes a model for adding variables that do not provide useful information.\n\n$$\n\\begin{align*}\nR_a^2 &= 1 - [(\\frac{n - 1}{n - k - 1})(\\frac{SSE}{TSS})] \\\\\n&= 1 - [(1 - R^2)(\\frac{n - 1}{n - k - 1})]\n\\end{align*}\n$$\n\n-   $R_a^2 \\leq R^2$\n\nAlthough better at determining utility of model, we lose the interpretation as the coefficient can be negative.\n\n# Categorical Predictors\n\nWe can encode categorical variables with **dummy encoding**.\n\n$$\nx_1 = \n\\begin{cases}\n1 if Y \\\\\n0 if N\n\\end{cases}\n$$",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}