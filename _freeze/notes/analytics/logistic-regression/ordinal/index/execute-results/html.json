{
  "hash": "bfcd804509cbab89d81e6cabc9ec105e",
  "result": {
    "markdown": "---\ntitle: Ordinal Logistic Regression\ndate: 09/15/2023\n---\n\n\nWhen outcomes are **ordered** we can generalize the binary logistic regression model. Models can also be used when the continuous response variable has a **restricted range** and need to be split into categories.\n\n# Ordinal Logistic Regression\n\nProbability that observation $i$ has **at most** event $j$, $j = 1, \\cdots, m$.\n\nWe'll be using the \"Found the Wallet\" data set which contains 195 observations divided between three different classes:\n\n-   Keep Both (1)\n-   Keep Money Only (2)\n-   Return All (3)\n\n## Proportional Odds Model\n\nThere are three methods for modeling ordinal logistic regression:\n\n1.  Cumulative Logit Model\n2.  Adjacent Categories Model\n3.  Continuation Ratio Model\n\nBy far, the predominant way of doing ordinal logistic regression is the cumulative logit model.\n\n## Cumulative Logits\n\nWe are building multiple binary logistic regression models. We have $m$ categories with probabilities ($p_1, p_2, \\cdots, p_m$) then cumulative logits are:\n\n$$\n\\log\\left(\\frac{p_{i,1}}{p_{i,2} + p_{i,3} + \\cdots + p_{i,m}}\\right), \\log\\left(\\frac{p_{i,1} + p_{i,2}}{p_{i,3} = \\cdots + p_{i,m}}\\right), \\cdots, \\log\\left(\\frac{p_{i,1} + \\cdots + p_{i,m-1}}{p_m}\\right)\n$$\n\nNotice that the sum of all the probabilities 1 through $m$ is equal to 1. The denominator is actually the difference between the cumulative probability and 1!\n\n$$\n\\log\\left(\\frac{p_{i,1}}{1 - p_{i,1}}\\right), \\log\\left(\\frac{p_{i,1} + p_{i,2}}{1 - (p_{i,1} + p_{i,2})}\\right), \\cdots\n$$\n\nThis results in $m - 1$ binary logistic regressions. Our new equation for ordinal logistic regression represents the probability that observation $i$ has **at most** event $m$:\n\n$$\n\\beta_{0,j} + \\beta_1x_{1,i} + \\cdots + \\beta_kx_{k,i}\n$$\n\nIntercept changes, but slope parameters stays the same (called proportional odds assumption)\n\n# Proportional Odds Model\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(MASS)\n\ntrain <- read.csv(\"data/wallet.csv\")\ntrain$punish <- factor(train$punish)\n\nclogit_model <- polr(factor(wallet) ~ male + business + punish + explain, method = \"logistic\", data = train)\nsummary(clogit_model)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nRe-fitting to get Hessian\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\npolr(formula = factor(wallet) ~ male + business + punish + explain, \n    data = train, method = \"logistic\")\n\nCoefficients:\n           Value Std. Error t value\nmale     -1.0598     0.3274  -3.237\nbusiness -0.7389     0.3556  -2.078\npunish2  -0.6276     0.4048  -1.551\npunish3  -1.4031     0.4823  -2.909\nexplain   1.0519     0.3408   3.086\n\nIntercepts:\n    Value   Std. Error t value\n1|2 -2.5679  0.4190    -6.1287\n2|3 -0.7890  0.3709    -2.1273\n\nResidual Deviance: 307.3349 \nAIC: 321.3349 \n```\n:::\n:::\n\n\n# Python\n:::\n\nWe have to test to see if the slopes are statistically different from each other in the proportional odds model.\n\n:::{.text-center}\n$H_0:$ Proportional Odds Correct (Slopes Equal)\n\n$H_a:$ Proportional Odds Incorrect (Slopes NOT Equal Across Models)\n:::\n\n## Brant Test\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(brant)\nbrant(clogit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n-------------------------------------------- \nTest for\tX2\tdf\tprobability \n-------------------------------------------- \nOmnibus\t\t5.46\t5\t0.36\nmale\t\t0.51\t1\t0.47\nbusiness\t0.58\t1\t0.45\npunish2\t\t0.99\t1\t0.32\npunish3\t\t2.81\t1\t0.09\nexplain\t\t0.25\t1\t0.62\n-------------------------------------------- \n\nH0: Parallel Regression Assumption holds\n```\n:::\n:::\n\n\nProportional odds assumption may not be met for all variables. We have two approaches:\n\n-   Partial Proportional Odds Model (some variables not met)\n-   Multinomial Logistic Regression\n\n## Partial Proportional Odds Model\n\nKeep in mind we have to do the Brant test first to determine which variables should be included in the `parallel` argument. For the sake of completeness, we chose `business` arbitrarily.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(VGAM)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: stats4\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: splines\n```\n:::\n\n```{.r .cell-code}\nplogit_model <- vglm(factor(wallet) ~ male + business + punish + explain, data = train, family = cumulative(parallel = FALSE ~ business))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning in eval(slot(family, \"initialize\")): response should be ordinal---see\nordered()\n```\n:::\n\n```{.r .cell-code}\nsummary(plogit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nvglm(formula = factor(wallet) ~ male + business + punish + explain, \n    family = cumulative(parallel = FALSE ~ business), data = train)\n\nCoefficients: \n              Estimate Std. Error z value Pr(>|z|)    \n(Intercept):1  -2.6695     0.4466  -5.978 2.26e-09 ***\n(Intercept):2  -0.7730     0.3678  -2.102  0.03557 *  \nmale            1.0707     0.3258   3.287  0.00101 ** \nbusiness:1      0.9722     0.4789   2.030  0.04236 *  \nbusiness:2      0.6376     0.3810   1.674  0.09423 .  \npunish2         0.6300     0.4008   1.572  0.11594    \npunish3         1.3956     0.4727   2.952  0.00316 ** \nexplain        -1.0532     0.3413  -3.086  0.00203 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNames of linear predictors: logitlink(P[Y<=1]), logitlink(P[Y<=2])\n\nResidual deviance: 306.8392 on 382 degrees of freedom\n\nLog-likelihood: -153.4196 on 382 degrees of freedom\n\nNumber of Fisher scoring iterations: 5 \n\nNo Hauck-Donner effect found in any of the estimates\n\n\nExponentiated coefficients:\n      male business:1 business:2    punish2    punish3    explain \n 2.9174165  2.6438481  1.8918696  1.8776731  4.0373205  0.3488076 \n```\n:::\n:::\n\n\n# Interpretation\n\nWith cumulative logits, increasing right-hand side of the equation leads to an increased log(odds) of **higher** outcome category.\n\nInterpretation is still an odds ratios: $100 \\cdot (e^{\\hat{\\beta}_j} - 1)\\%$ higher expected odds of being in a higher category.\n\nIn a proportional odds model, there is the same increase in odds across al singular jumps in category. In our data example, males have a coeffcient of -1.0598 so they have **65.35%** lower expected odds of being in a higher ethical category as compared to females.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}