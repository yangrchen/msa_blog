{
  "hash": "ca6e5c3e810b895c247339c42373dda7",
  "result": {
    "markdown": "---\ntitle: Model Assessment\ndate: 09/07/2023\ndate-modified: 09/15/2023\ncategories:\n    -   modeling\n---\n\n\n# Setup {.unnumbered}\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames |>\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\ntrain <- sample_frac(ames, 0.7)\n\nlogit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\names = r.ames\ntrain = r.train\nlog_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()\n```\n:::\n\n:::\n\n\n# Comparing Models\n\nRemember that statistical models are created for two different purposes: estimation and prediction.\n\nEstimation and prediction may not necessarily agree and they can result in tradeoffs of interpretation for predictive power.\n\n# Deviance / Likelihood Measures\n\nAIC and BIC approximate out-of-sample prediction error by applying penalty for model complexity.\n\n-   AIC is a crude, large-sample approximation of leave-one-out cross-validation\n-   BIC favors smaller models and penalizes model complexity more\n\nFor logistic regression, we also have \"pseudo\"-$R^2$ quantities. Higher values indicate a \"better\" model.\n\n## Generalized / Nagelkerkge $R^2$\n\n$$\nR_G^2 = 1 - (\\frac{L_0}{L_1})^{\\frac{2}{n}}\n$$\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\n\nAIC(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1287.964\n```\n:::\n\n```{.r .cell-code}\nBIC(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1394.86\n```\n:::\n\n```{.r .cell-code}\nPseudoR2(logit_model, which = \"Nagelkerke\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNagelkerke \n 0.7075796 \n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(log_model.aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1287.964395223345\n```\n:::\n\n```{.python .cell-code}\nprint(log_model.bic_llf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1394.8599676267202\n```\n:::\n\n```{.python .cell-code}\nprint(log_model.pseudo_rsquared())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5247678540521266\n```\n:::\n:::\n\n:::\n\n# Probability Metrics\n\nLogistic regression was originally designed to rank-order probabilities. It *can* be used as a classification model as well.\n\nYou might want that insight into rank-ordering than you might think. An example is when customers are predicted to buy a product but in reality that are on the fence.\n\n## Discrimination vs. Calibration\n\n**Discrimination** is the ability to separate the events from the non-events. How good is a model at distinguishing the 1's from the 0's.\n\n**Calibration** is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.\n\nThese two metrics may not agree with each other.\n\n## Coefficient of Discrimination\n\n**Coefficient of determination** is the difference in average predicted probability between 1's and 0's:\n\n$$\nD = \\bar{\\hat{p}}_1 - \\bar{\\hat{p}}_0\n$$\n\nThis is a comparison metric to see which model can separate the 1's and 0's better.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain$p_hat <- predict(logit_model, type = \"response\")\np1 <- train$p_hat[train$Bonus == 1]\np0 <- train$p_hat[train$Bonus == 0]\n\ncoef_discrim <- mean(p1) - mean(p0)\n\nggplot(train, aes(p_hat, fill = factor(Bonus))) +\n    geom_density(alpha = 0.7) +\n    scale_fill_grey() +\n    labs(x = \"Predicted Probability\", fill = \"Outcome\", title = paste(\"Coefficient of Discrimination = \", round(coef_discrim, 3), sep = \"\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain[\"p_hat\"] = log_model.predict()\n\np1 = train.loc[train[\"Bonus\"] == 1, \"p_hat\"]\np0 = train.loc[train[\"Bonus\"] == 0, \"p_hat\"]\n\ncoef_discrim = p1.mean() - p0.mean()\n```\n:::\n\n:::\n\n## Rank-order Statistics\n\nHow well does a model order predictions? Recall concordance. For a pair of subjects with and without the event, the one **with the event** had the **higher** predicted probability.\n\nDiscordance is where for a pair of subjects with and without the event, the one **with the event** had the **lower** predicted probability.\n\n### Concordance\n\nInterpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event $Concordance\\%$ of the time.\n\nCommon metrics based on concordance:\n\n-   C-Statistic: $c = Concordance\\% + \\frac{1}{2}Tied\\%$\n-   Somer's D (Gini): $D_{xy} = 2c - 1$\n-   Kendall's $\\tau_{\\alpha}: \\tau_{\\alpha} = \\frac{\\text{\\#concordant} - \\text{\\#discordant}}{\\frac{n(n - 1)}{2}}$\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Hmisc)\nsomers2(train$p_hat, train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           C          Dxy            n      Missing \n   0.9428394    0.8856789 2051.0000000    0.0000000 \n```\n:::\n:::\n\n\nOur model assigned the higher predicted probability to the observation with the bonus eligible home 94.3% of the time (c-statistic).\n\n# Python\n:::\n\n# Assessing Predictive Power\n\nWe want our model to correctly classify events and non-events. **Classification** forces the model to predict either 1 or 0 based on whether the predicted probability exceeds some threshold.\n\nStrict classification-based measures completely discard any information about the actual quality of the model's predicted probablities.\n\n![Logistic Discrimination](images/logistic-discrimination.png)\n\n![Classification Table](images/classification-table.png)\n\n## Sensitivity vs. Specificity\n\nOf all the actual 1's how many did you get right (**sensitivity**):\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\nOf all the actual 0's how many did you get right (**specificity**):\n\n$$\nTNR = \\frac{TN}{TN + FP}\n$$\n\nWhen we raise the cutoff, we make our model more specific. When we lower the cutoff, we make our model more sensitive.\n\nAlways consider the cost of false positives and false negatives when doing classification. When **NOT** considering costs, there are different techniques to \"optimize\" cut-off.\n\n### Youden J Statistic\n\n$$\nJ = \\text{Sensitivity} + \\text{Specificity} - 1\n$$\n\nFalse positives and false negatives are weighed equally, so select cut-off that products highest Youden $J$ statistic.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nWe can use the `ROCit` library to calculate different metrics between our predictions and actual values. We will calculate accuracy (`ACC`), sensitivity (`SENS`), and specificity (`SPEC`).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ROCit)\n\nlogit_meas <- measureit(train$p_hat, train$Bonus, measure = c(\"ACC\", \"SENS\", \"SPEC\"))\n\nyouden_table <- data.frame(Cutoff = logit_meas$Cutoff, Sens = logit_meas$SENS, Spec = logit_meas$SPEC)\n\nyouden_table %>% head(10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Cutoff        Sens      Spec\n1        Inf 0.000000000 1.0000000\n2  0.9999996 0.000000000 0.9991742\n3  0.9999963 0.001190476 0.9991742\n4  0.9999952 0.002380952 0.9991742\n5  0.9999786 0.003571429 0.9991742\n6  0.9999653 0.004761905 0.9991742\n7  0.9999573 0.005952381 0.9991742\n8  0.9999546 0.007142857 0.9991742\n9  0.9999381 0.008333333 0.9991742\n10 0.9998792 0.009523810 0.9991742\n```\n:::\n:::\n\n\nWe could calculate the Youden index for every cutoff, but we can also use the `rocit` function to calculate the optimal Youden index. The plot shown is the ROC curve which is covered next.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_roc <- rocit(train$p_hat, train$Bonus)\nlogit_plot <- plot(logit_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlogit_plot$optimal\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    value       FPR       TPR    cutoff \n0.7352326 0.1552436 0.8904762 0.4229724 \n```\n:::\n:::\n\n\n# Python\n\n:::\n\n### ROC Curve\n\n![ROC Curve](images/roc-curve.png)\n\nThe ROC curve plots sensitivity (TPR) vs. 1 - specificity (FPR) for different cutoff thresholds. In essence, we are trying to **balance sensitivity and specificity**. The \"best\" ROC curve is one that reaches the upper left hand side as the model would have high levels of sensitivity and specificity. The worst ROC curve is the diagonal line as the model would be just as good randomly assigning events and non-events to our observations. \n\nThe lower left hand side is predicting every observation is a 0. Specificity is high, but sensitivity is 0. The upper right hand side is predicting every observation is a 1. Sensitivity is high, but specificity is 0.\n\nArea under the curve (AUC or AUROC) summarizes the overall quality of ROC curve. Mathematically, AUC is equivalent to the $c$-statistic. AUC curves can be a useful metric in *comparing* models, but they do not necessarily detail how good the model itself is.\n\n$$\nAUC = \\%\\text{Concordant} + \\frac{1}{2}(\\%\\text{Tied})\n$$\n\n![AUC Calculation Visualized](images/auc-concordance-visual.png)\n\n:::{.panel-tabset group=\"language\"}\n# R\n\nLike with Youden's index, we can plot the AUC curve through the `rocit` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_roc <- rocit(train$p_hat, train$Bonus)\nplot(logit_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nWe can get the calculated AUC through the summary:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(logit_roc)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                            \n Method used: empirical     \n Number of positive(s): 840 \n Number of negative(s): 1211\n Area under curve: 0.9428   \n```\n:::\n:::\n\n\n# Python\n:::\n\n### KS Statistic\n\nThe **KS statistic** is a popular metric in the finance and banking industry. The two-sample KS statistic determines whether there is a difference between two cumulative distribution functions. \n\nIn binary classification, the KS statistic is the maximum distance between the distribution functions for the event and non-event groups. The point at which this max distance occurs is the optimal cut-off for the model. \n\nRemember that this is assuming that the cost for each observation is the same. You should ask the business what costs drive the cut-off decision rather than always select the mathematically optimal cut-off.\n\n$$\nD = \\max_{depth}(TPR - FPR) = \\max_{depth}(\\text{Sensitivity} + \\text{Specificity} - 1)\n$$\n\nNote that this is exactly the same as maximizing the Youden index!\n\n::: {.panel-tabset group=\"language\"}\n# R\n\nTo plot the two cumulative distribution functions as well as the maximum distance between the two CDFs we can use `ksplot`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_ks <- ksplot(logit_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n\n```{.r .cell-code}\nlogit_ks$\"KS Cutoff\"\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4229724\n```\n:::\n:::\n\n\n:::\n\n### KS-Statistic or Youden?\n\n$D$ test statistic is used for model comparison. However, it is mathematically equivalent to Youden's J statistic. The point at which we have the maximum $D$ statistic is the optimal cutoff.\n\n## Precision vs. Recall\n\nIn a precision-recall perspective, our focus is on the 1's.\n\n**Recall** is the same as sensitivity:\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\n**Precision** is slightly different than specificity:\n\n$$\nPPV = \\frac{TP}{TP + FP}\n$$\n\n## Best Cut-off?\n\nAlways consider the cost of false positives and false negatives when doing classification.\n\nWhen not considering costs, many different techniques to \"optimize\" cutoff.\n\n$$\nF_1 = 2\\left(\\frac{PPV \\cdot TPR}{PPV + TPR}\\right)\n$$\n\nPrecision and recall are weighted equally, so select cut-off that produces highest $F_1$ score.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_meas <- measureit(train$p_hat, train$Bonus, measure = c(\"PREC\", \"REC\", \"FSCR\"))\n\nfscore_table <- data.frame(Cutoff = logit_meas$Cutoff, FScore = logit_meas$FSCR)\n\nfscore_table %>%\n    arrange(desc(FScore)) %>%\n    head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Cutoff    FScore\n1 0.4229724 0.8423423\n```\n:::\n:::\n\n\n# Python\n:::\n\n## Precision and Lift\n\nCommon calculation in marketing. Great for interpretation around validity of model ranking / classifying observations correctly.\n\n$$\n\\text{Lift} = \\frac{PPV}{\\pi_1}\n$$\n\n-   $\\pi_1$ is the proportion of 1's in your original population\n\nThe top **depth%** of your customers, based on predicted probability, you get **lift** times as many responses compared to targeting a random sample of **depth%** of your customers.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_lift <- gainstable(logit_roc)\nlogit_lift\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Bucket Obs CObs Depth Resp CResp RespRate CRespRate CCapRate  Lift CLift\n1       1 205  205   0.1  200   200    0.976     0.976    0.238 2.382 2.382\n2       2 205  410   0.2  190   390    0.927     0.951    0.464 2.263 2.323\n3       3 205  615   0.3  167   557    0.815     0.906    0.663 1.989 2.211\n4       4 205  820   0.4  134   691    0.654     0.843    0.823 1.596 2.058\n5       5 206 1026   0.5   92   783    0.447     0.763    0.932 1.090 1.863\n6       6 205 1231   0.6   42   825    0.205     0.670    0.982 0.500 1.636\n7       7 205 1436   0.7   12   837    0.059     0.583    0.996 0.143 1.423\n8       8 205 1641   0.8    1   838    0.005     0.511    0.998 0.012 1.247\n9       9 205 1846   0.9    2   840    0.010     0.455    1.000 0.024 1.111\n10     10 205 2051   1.0    0   840    0.000     0.410    1.000 0.000 1.000\n```\n:::\n:::\n\n\nBucket 1 contains the top 10% of homes in terms of probability of being bonus eligible. Bucket 10 contains the 10% of homes with the lowest probabilities. The lift amount in the first row represents how much higher our model is able to predict bonus eligible homes than if you were to randomly select from 10% of the population. Response rate (RespRate) is the proportion of responses the model was able to correctly predict.\n\nCumulative lift (CLift) on the second row is saying our model is able to predict bonus eligible homes at a 2.32 higher rate than if you were to randomly select from 20% of the population.\n\nCCapRate refers to how many 1's we captured within the top $k\\%$ of the data. For example, bucket 1 captured 24% of the 1's in the data.\n:::\n\n## Accuracy and Error\n\nBe careful selecting models based on accuracy and error. If your data has 10% events and 90% non-events then you can have a 90% accurate model by guessing non-events for every observation.\n\n### Accuracy\n\n$$\n\\text{Acc} = \\frac{TP + TN}{TP + FP + TN + FN}\n$$\n\n### Error\n\n$$\n\\text{Error} = \\frac{FP + FN}{n}\n$$\n\n# Closing Thoughts On Classification\n\nClassification is a decision that is outside of statistical modeling. Classification assumes cost for each individual is the same. Useful for groups, but be careful about single observation decisions.",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}