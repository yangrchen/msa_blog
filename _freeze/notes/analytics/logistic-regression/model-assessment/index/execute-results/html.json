{
  "hash": "6d320fff6b90b6fb898f3ce4753add7d",
  "result": {
    "markdown": "---\ntitle: Model Assessment\ndate: 09/07/2023\ndate-modified: 09/13/2023\ncategories:\n    -   modeling\n---\n\n\n# Setup {.unnumbered}\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames |>\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\ntrain <- sample_frac(ames, 0.7)\n\nlogit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\names = r.ames\ntrain = r.train\nlog_model = GLM.from_formula('Bonus ~ Gr_Liv_Area + C(House_Style) + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces', data=train, family=Binomial()).fit()\n```\n:::\n\n:::\n\n\n# Comparing Models\n\nRemember that statistical models are created for two different purposes: estimation and prediction.\n\nEstimation and prediction may not necessarily agree and they can result in tradeoffs of interpretation for predictive power.\n\n# Deviance / Likelihood Measures\n\nAIC and BIC approximate out-of-sample prediction error by applying penalty for model complexity.\n\n-   AIC is a crude, large-sample approximation of leave-one-out cross-validation\n-   BIC favors smaller models and penalizes model complexity more\n\nFor logistic regression, we also have \"pseudo\"-$R^2$ quantities. Higher values indicate a \"better\" model.\n\n## Generalized / Nagelkerkge $R^2$\n\n$$\nR_G^2 = 1 - (\\frac{L_0}{L_1})^{\\frac{2}{n}}\n$$\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DescTools)\n\nAIC(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1287.964\n```\n:::\n\n```{.r .cell-code}\nBIC(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1394.86\n```\n:::\n\n```{.r .cell-code}\nPseudoR2(logit_model, which = \"Nagelkerke\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNagelkerke \n 0.7075796 \n```\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(log_model.aic)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1287.964395223345\n```\n:::\n\n```{.python .cell-code}\nprint(log_model.bic_llf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n1394.8599676267202\n```\n:::\n\n```{.python .cell-code}\nprint(log_model.pseudo_rsquared())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.5247678540521266\n```\n:::\n:::\n\n:::\n\n# Assessing Predictive Power\n\nLogistic regression was originally designed to rank-order probabilities. It *can* be used as a classification model as well.\n\nYou might want that insight into rank-ordering than you might think. An example is when customers are predicted to buy a product but in reality that are on the fence.\n\n## Discrimination vs. Calibration\n\n**Discrimination** is the ability to separate the events from the non-events. How good is a model at distinguishing the 1's from the 0's.\n\n**Calibration** is how well predicted probabilities agree with the actual frequency of the outcomes. Are predicted probabilities systematically too low or too high? This is used when we care about if the probability output reflects the actual probability of an occurrence.\n\nThese two metrics may not agree with each other.\n\n## Coefficient of Discrimination\n\n**Coefficient of determination** is the difference in average predicted probability between 1's and 0's:\n\n$$\nD = \\bar{\\hat{p}}_1 - \\bar{\\hat{p}}_0\n$$\n\nThis is a comparison metric to see which model can separate the 1's and 0's better.\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain$p_hat <- predict(logit_model, type = \"response\")\np1 <- train$p_hat[train$Bonus == 1]\np0 <- train$p_hat[train$Bonus == 0]\n\ncoef_discrim <- mean(p1) - mean(p0)\n\nggplot(train, aes(p_hat, fill = factor(Bonus))) +\n    geom_density(alpha = 0.7) +\n    scale_fill_grey() +\n    labs(x = \"Predicted Probability\", fill = \"Outcome\", title = paste(\"Coefficient of Discrimination = \", round(coef_discrim, 3), sep = \"\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\ntrain[\"p_hat\"] = log_model.predict()\n\np1 = train.loc[train[\"Bonus\"] == 1, \"p_hat\"]\np0 = train.loc[train[\"Bonus\"] == 0, \"p_hat\"]\n\ncoef_discrim = p1.mean() - p0.mean()\n```\n:::\n\n:::\n\n# Rank-order Statistics\n\nHow well does a model order predictions? Recall concordance. For a pair of subjects with and without the event, the one **with the event** had the **higher** predicted probability.\n\nDiscordance is where for a pair of subjects with and without the event, the one **with the event** had the **lower** predicted probability.\n\n## Concordance\n\nInterpretation: For all possible (1, 0) pairs, the model assigned the higher predicted probability to the observation with the event $Concordance\\%$ of the time.\n\nCommon metrics based on concordance:\n\n-   C-Statistic: $c = Concordance\\% + \\frac{1}{2}Tied\\%$\n-   Somer's D (Gini): $D_{xy} = 2c - 1$\n-   Kendall's $\\tau_\\alpha$: $\\tau_\\alpha = \\frac{\\text{#concordant} - \\text{#discordant}}{\\frac{n(n - 1)}{2}}$\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(Hmisc)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'Hmisc'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:DescTools':\n\n    %nin%, Label, Mean, Quantile\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n```\n:::\n\n```{.r .cell-code}\nsomers2(train$p_hat, train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           C          Dxy            n      Missing \n   0.9428394    0.8856789 2051.0000000    0.0000000 \n```\n:::\n:::\n\n\n# Python\n:::\n\n# Assessing Predictive Power\n\nWe want our model to correctly classify events and non-events. **Classification** forces the model to predict either 1 or 0 based on whether the predicted probability exceeds some threshold.\n\nStrict classification-based measures completely discard any information about the actual quality of the model's predicted probablities.\n\n![Logistic Discrimination](images/logistic-discrimination.png)\n\n![Classification Table](images/classification-table.png)\n\n# Sensitivity vs. Specificity\n\nOf all the actual 1's how many did you get right (**sensitivity**):\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\nOf all the actual 0's how many did you get right (**specificity**):\n\n$$\nTNR = \\frac{TN}{TN + FP}\n$$\n\nWhen we raise the cutoff, we make our model more specific. When we lower the cutoff, we make our model more sensitive.\n\nAlways consider the cost of false positives and false negatives when doing classification. When **NOT** considering costs, there are different techniques to \"optimize\" cut-off.\n\n## Youden J Statistic\n\n$$\nJ = \\text{Sensitivity} + \\text{Specificity} - 1\n$$\n\nFalse positives and false negatives are weighed equally, so select cut-off that products highest Youden $J$ statistic.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n    mutate(Bonus_hat = ifelse(p_hat > 0.5, 1, 0))\n\ntable(train$Bonus_hat, train$Bonus)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n       0    1\n  0 1062  127\n  1  149  713\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ROCit)\n\nlogit_meas <- measureit(train$p_hat, train$Bonus, measure = c(\"ACC\", \"SENS\", \"SPEC\"))\n```\n:::\n\n\n# Python\n\n:::\n\n## ROC Curve\n\nThe ROC curve plots TPR vs. FPR for a grid of thresholds. Area under the curve (AUC or AUROC) summarizes the overall quality of ROC curve. Equivalent to C-statistic.\n\nWe want high sensitivity and high specificity.\n<!-- Need to show AUC calculation -->\n\n![ROC Curve](images/roc-curve.png)\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_roc <- rocit(train$p_hat, train$Bonus)\nplot(logit_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(logit_roc)$optimal\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n    value       FPR       TPR    cutoff \n0.7352326 0.1552436 0.8904762 0.4229724 \n```\n:::\n:::\n\n\n# Python\n:::\n\n# KS Statistic\n\nThe **Two-Sample KS statistic** can determine if there is a difference between two cumulative distribution functions.\n\n::: {.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nksplot(logit_roc)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n:::\n\nThe two curves represent the CDFs of the distribution of 1's and 0's. The KS statistic, $D$ is the maximum distance between the two curves.\n\n## KS-Statistic or Youden?\n\n$D$ test statistic is used for model comparison. However, it is mathematically equivalent to Youden's J statistic. The point at which we have the maximum $D$ statistic is the optimal cutoff.\n\n# Precision vs. Recall\n\nIn a precision-recall perspective, our focus is on the 1's.\n\n**Recall** is the same as sensitivity:\n\n$$\nTPR = \\frac{TP}{TP + FN}\n$$\n\n**Precision** is slightly different than specificity:\n\n$$\nPPV = \\frac{TP}{TP + FP}\n$$\n\n## Best Cut-off?\n\nAlways consider the cost of false positives and false negatives when doing classification.\n\nWhen not considering costs, many different techniques to \"optimize\" cutoff.\n\n$$\nF_1 = 2\\left(\\frac{PPV \\cdot TPR}{PPV + TPR}\\right)\n$$\n\nPrecision and recall are weighted equally, so select cut-off that produces highest $F_1$ score.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_meas <- measureit(train$p_hat, train$Bonus, measure = c(\"PREC\", \"REC\", \"FSCR\"))\n\nfscore_table <- data.frame(Cutoff = logit_meas$Cutoff, FScore = logit_meas$FSCR)\n\nfscore_table %>%\n    arrange(desc(FScore)) %>%\n    head(1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Cutoff    FScore\n1 0.4229724 0.8423423\n```\n:::\n:::\n\n\n# Python\n:::\n\n## Precision and Lift\n\nCommon calculation in marketing. Great for interpretation around validity of model ranking / classifying observations correctly.\n\n$$\n\\text{Lift} = \\frac{PPV}{\\pi_1}\n$$\n\n-   $\\pi_1$ is the proportion of 1's in your original population\n\nThe top **depth%** of your customers, based on predicted probability, you get **lift** times as many responses compared to targeting a random sample of **depth%** of your customers.\n\n:::{.panel-tabset group=\"language\"}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_lift <- gainstable(logit_roc)\nlogit_lift\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Bucket Obs CObs Depth Resp CResp RespRate CRespRate CCapRate  Lift CLift\n1       1 205  205   0.1  200   200    0.976     0.976    0.238 2.382 2.382\n2       2 205  410   0.2  190   390    0.927     0.951    0.464 2.263 2.323\n3       3 205  615   0.3  167   557    0.815     0.906    0.663 1.989 2.211\n4       4 205  820   0.4  134   691    0.654     0.843    0.823 1.596 2.058\n5       5 206 1026   0.5   92   783    0.447     0.763    0.932 1.090 1.863\n6       6 205 1231   0.6   42   825    0.205     0.670    0.982 0.500 1.636\n7       7 205 1436   0.7   12   837    0.059     0.583    0.996 0.143 1.423\n8       8 205 1641   0.8    1   838    0.005     0.511    0.998 0.012 1.247\n9       9 205 1846   0.9    2   840    0.010     0.455    1.000 0.024 1.111\n10     10 205 2051   1.0    0   840    0.000     0.410    1.000 0.000 1.000\n```\n:::\n:::\n\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}