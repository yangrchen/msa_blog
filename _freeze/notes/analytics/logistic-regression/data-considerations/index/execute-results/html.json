{
  "hash": "22bf955e06c42cecb1ce2f0783885f80",
  "result": {
    "markdown": "---\ntitle: Data Considerations\ndate: 08/28/2023\nengine: knitr\ncategories:\n    -   logistic regression\n---\n\n\nThe following considerations apply to any binary classification problem.\n\n# Rare Event Modeling\n\nMany algorithms and models have a problem trying to predict small proportions. 5% or smaller in a target category can lead to classification problems. Common situations include fraud, default, marketing response, weather event.\n\n# Telecomm Churn Data Set\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchurn <- read.csv(\"https://raw.githubusercontent.com/IAA-Faculty/statistical_foundations/master/tele_churn.csv\")\n```\n:::\n\n\n# Python\n:::\n\n# Rare Event Sampling Correction\n\n![Rare Event Sampling Correction](images/rare-event-sampling.png)\n\n## Oversampling\n\n-   Duplicate current event cases in training set to balance better with non-event cases\n-   Keep test set as original population proportion\n\nCan be a problem because we're \"repeating\" the signal\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\nset.seed(12345)\n\ntrain_o <- churn %>%\n    sample_frac(0.7) %>%\n    mutate(id = row_number())\n\n# Each positive observation is repeated 10 times\ntrain_o_T <- train_o %>%\n    filter(churn == TRUE) %>%\n    slice(rep(1:n(), each = 10))\n\ntrain_o_F <- train_o %>%\n    filter(churn == FALSE)\n\ntrain_o  <- rbind(train_o_F, train_o_T)\ntest_o <- churn[-train_o$id,]\n\ntable(train_o$churn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n 1996  1070 \n```\n:::\n\n```{.r .cell-code}\ntable(test_o$churn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n  747   154 \n```\n:::\n:::\n\n:::\n\n# Undersampling\n\n-   Randomly sample current non-event cases to keep in the training set to balance with event cases\n-   Keep test set as original population proportion\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12345)\n\ntrain_u <- churn %>%\n    mutate(id = row_number()) %>%\n    group_by(churn) %>%\n    sample_n(104)\ntest_u <- churn[-train_u$id, ]\n\ntable(train_u$churn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n  104   104 \n```\n:::\n\n```{.r .cell-code}\ntable(test_u$churn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nFALSE  TRUE \n 2746    50 \n```\n:::\n:::\n\n\n# Python\n:::\n\n# Running the Telecomm Model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogit_model <- glm(churn ~ factor(international.plan) + factor(voice.mail.plan) + total.day.charge + customer.service.calls, data = train_u, family = binomial())\nsummary(logit_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = churn ~ factor(international.plan) + factor(voice.mail.plan) + \n    total.day.charge + customer.service.calls, family = binomial(), \n    data = train_u)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -5.81880    0.95939  -6.065 1.32e-09 ***\nfactor(international.plan)yes  2.97995    0.57057   5.223 1.76e-07 ***\nfactor(voice.mail.plan)yes    -0.85107    0.41372  -2.057   0.0397 *  \ntotal.day.charge               0.12898    0.02234   5.773 7.79e-09 ***\ncustomer.service.calls         0.78520    0.14947   5.253 1.50e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 288.35  on 207  degrees of freedom\nResidual deviance: 195.24  on 203  degrees of freedom\nAIC: 205.24\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n:::\n\n\n# Adjustments to Oversampling\n\n![Oversampling Effect](images/oversampling-effect.png)\n\nWhen sample proportion is out of line with population proportion, we need to adjust to correct the bias.\n\nThere are two possible methods:\n\n-   Adjusting the intercept\n-   Weighting observations\n\n## Adjusting the Intercept\n\nNeed to correct for bias created by oversampling. Adjustment is only applied to intercept. This creates an unbiased estimate of our probabilities.\n\n$$\n\\hat{p}_i = \\frac{\\hat{p}_i^*\\rho_0\\pi_1}{(1 - \\hat{p}_i^*)\\rho_1 + \\hat{p}_i^*\\rho_0\\pi_1}\n$$\n\n-   Population proportion: $\\pi_1, \\pi_0$\n-   Sample proportion: $\\rho_1, \\rho_0$\n-   Unadjusted predictions: $\\hat{p}_i^*$\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_u_p_bias <- predict(logit_model, newdata = test_u, type = \"response\")\ntest_u_p <- (test_u_p_bias * (104 / 208) * (154 / 3004)) / ((1 - test_u_p_bias) * (104 / 208) * (2850 / 3004) + test_u_p_bias * (104 / 208) * (154 / 3004))\n\ntest_u <- data.frame(test_u, \"Pred\" = test_u_p)\n\nhead(test_u_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         1          2          3          4          5          6 \n0.04788873 0.00516951 0.03230002 0.91516214 0.56312205 0.29766875 \n```\n:::\n:::\n\n:::\n\n## Weighted Observations\n\nWeighting observations adjusts while model is being built instead of after it is built.\n\nWe use weighted **maximum likelihood estimation (MLE)** so each observation has potentially different weights to the MLE calculation.\n\n$$\n\\text{weight} = \\begin{cases}\n1, & y = 1 \\\\\n\\rho_1\\pi_0/\\rho_0\\pi_1, & y = 0\n\\end{cases}\n$$\n\nWe can think of the 0 as gaining a multiplier of $\\frac{\\pi_0}{\\pi_1}$ per observation in order to get back to the original population proportion. The weight expressed in the above formula is actually $\\frac{(\\pi_0 / \\pi_1)}{(\\rho_0 / \\rho_1)}$\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_u$weight <- ifelse(train_u$churn == \"TRUE\", 1, 18.49)\nlogit_model_w <- glm(churn ~ factor(international.plan) + factor(voice.mail.plan) + total.day.charge + customer.service.calls, data = train_u, family = binomial(), weights = weight)\nsummary(logit_model_w)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = churn ~ factor(international.plan) + factor(voice.mail.plan) + \n    total.day.charge + customer.service.calls, family = binomial(), \n    data = train_u, weights = weight)\n\nCoefficients:\n                              Estimate Std. Error z value Pr(>|z|)    \n(Intercept)                   -9.76831    0.69648 -14.025  < 2e-16 ***\nfactor(international.plan)yes  3.33560    0.32429  10.286  < 2e-16 ***\nfactor(voice.mail.plan)yes    -1.07451    0.27107  -3.964 7.37e-05 ***\ntotal.day.charge               0.16320    0.01647   9.911  < 2e-16 ***\ncustomer.service.calls         0.72693    0.08810   8.251  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 820.31  on 207  degrees of freedom\nResidual deviance: 585.33  on 203  degrees of freedom\nAIC: 590.92\n\nNumber of Fisher Scoring iterations: 6\n```\n:::\n:::\n\n\n# Python\n:::\n\nWe actually get difference significant variables between the adjusted intercept model and the weighted model. When do we use which technique? For the most part, we use weighted.\n\n![Weighted vs. Adjust Intercept](images/weighted-vs-intercept.png)\n\n# Missing Values\n\nWhen you build a model, observations with missing values are typically dropped. This is referred to as **complete case analysis**. However, this can result in losing a lot of data that could be useful.\n\nSolutions to missing values:\n\n-   Delete\n-   Keep\n-   Replace\n\n## Deleting Variables\n\nIf a majority of your data is missing, then consider deleting the variable all together.\n\nMore than 50% missing is a good rule of thumb for removing the variable. We may not be confident that the data we do have is good.\n\n## Keeping Variables\n\nMissing values in predictor variables are not necessarily bad. For categorical variables we can add a missing category.\n\n## Replacing Variables\n\nWe can estimate a missing value with **imputation**. Not best to do with categorical variables as you can just add a missing category.\n\nApproaches:\n\n1.  Simple mean/median replacement\n2.  Predictive model using other variables (not empirically shown to add value)\n\nIf you impute missing continuous values then you need to create a missing value binary flag for each of the continuous variables you impute.\n\n![Imputing Continuous Variables](images/imputing-continuous.png)\n\n## Overall Considerations\n\nAny operations you apply to your training data need to be applied to your test data as well. Early on you should consider creating a pipeline that can apply the transforms you used on the test data when it comes to evaluation.\n\n# Convergence Problems\n\n## Linear Separation\n\nProblems with linear separation have to be considered for categorical variables.\n\n**Complete linear separation** occurs when some combination of the predictors perfectly predict **every** outcome.\n\n|             | Yes |  No |\n|-------------|----:|----:|\n| **Group A** | 100 |   0 |\n| **Group B** |   0 | 150 |\n\n**Quasi-complete separation** occurs when the outcome can be perfectly predicted for only a subset of the data. In this case, Group B is perfectly predicting the **No** category.\n\n|             | Yes |  No |\n|-------------|----:|----:|\n| **Group A** |  77 |  23 |\n| **Group B** |   0 |  50 |\n\n![Convergence Problems](images/convergence-problems.png)\n\n::: panel-tabset\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(train_u$customer.service.calls, train_u$churn)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n    FALSE TRUE\n  0    29   25\n  1    34   25\n  2    23   20\n  3    15   12\n  4     2   13\n  5     1    4\n  6     0    3\n  7     0    2\n```\n:::\n:::\n\n\nCategories 6 and 7 are perfectly predicting churn. We have quasi-complete separation.\n:::\n\n## Solutions\n\n-   **Collapse the categories of the predictor variable to eliminate the 0 cell count**\n-   Penalized maximum likelihood\n-   Eliminate the category altogether (may not be reasonable since the category seems important)\n-   Create a dummy observation that adds to the cell counts\n\n## Thresholding (Ordinal Variables)\n\n| Customer Service Calls | Sample Size |   0 |   1 |\n|-----------------------:|------------:|----:|----:|\n|                      0 |          54 |  29 |  25 |\n|                      1 |          59 |  34 |  25 |\n|                      2 |          43 |  23 |  20 |\n|                      3 |          27 |  15 |  12 |\n|                      4 |          15 |   2 |  13 |\n|                      5 |           5 |   1 |   4 |\n|                      6 |           3 |   0 |   3 |\n|                      7 |           2 |   0 |   2 |\n\nGroup 4 - 7 categories together:\n\n| Customer Service Calls | Sample Size |   0 |   1 |\n|-----------------------:|------------:|----:|----:|\n|                      0 |          54 |  29 |  25 |\n|                      1 |          59 |  34 |  25 |\n|                      2 |          43 |  23 |  20 |\n|                      3 |          27 |  15 |  12 |\n|                     4+ |          25 |   3 |  22 |\n\n## Clustering Levels - Greenacre Method (Ordinal Variables)\n\n![Greenacre Method](images/greenacre-method.png)\n\nWe select the clustering which results in the least amount of information lost from our original counts. In this case we group B and C together.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}