{
  "hash": "303a7587fd3d8cf78c36c0b0406a0479",
  "result": {
    "markdown": "---\ntitle: Introduction to Statistical Inference\ndate: 06/28/2023\ndate-modified: 07/22/2023\n---\n\n\nLast time we talked about different statistical measures like mean and standard deviation. These are called **point estimates**.\n\n-   Want to learn about an entire population\n-   Take a representative sample and calculate sample statistics\n-   Sample statistics have som error as they are *estimates* of their population parameters\n\nThere is *variability* among samples. However, we can have a margin of error for our estimate via the **Central Limit Theorem**\n\n::: callout-note\n# Central Limit Theorem\n\nDistribution of sample means is approximately Normal, regardless of the population parameter's distribution given a large sample size ($n \\geq 50$).\n:::\n\n# Standard Error of the Mean\n\n**Standard error** measures the variability of our statistic estimate. Compare to the sample standard deviation, $s$, which is a measure of the **variability in your data**.\n\n-   Standard error is the variation on the statistic\n-   If you were to resample data and compute the new sample average many times, how much variability would you expect in the results?\n\n$$\ns_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\n$$\n\nAs the sample size grows larger, our confidence in the standard error grows\n\n# Confidence Intervals\n\nWhen we create a confidence interval for the true population mean:\n\n$$\n\\bar{x} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\n$$\n\n-   $t$ is a quantile from the $t$ distribution\n-   $t$ changes as a result of the confidence level we select\n\nIn other words, the t-value indicates the number of standard error from the mean for the margin of error.\n\nWith a 95% confidence interval, we are 95% \"confident\" that the true population mean exists. Confidence is **not** probability. If we repeated our experiment 100 times then 95 times we will have captured the true value.\n\n# Hypothesis Testing\n\nA hypothesis test investigates if we can prove that the true population value is significantly different than an assumed value.\n\n## Procedure\n\n1.  Start with a null hypothesis $H_0$ about a parameter of interest. We assume $H_0$ is true.\n2.  Select an acceptable significance level $\\alpha$ which represents the likelihood that you incorrectly reject $H_0$ (probability of Type I error)\n3.  Alternative hypothesis $H_a$ is the logical opposite. Note that alternative hypothesis is the \"significantly different\" statement--no equal signs should appear in alternative\n4.  Collect data, compute statistic\n5.  Determine the probability that you observed a statistic as extreme or more extreme as the one you did assuming $H_0$ is true $\\rightarrow$ **p-value**\n6.  If p-value $\\leq \\alpha$, reject $H_0$ and fail to reject otherwise. Make sure to *interpret* the p-value.\n\n# Simulation Study\n\nWe simulate the flipping of a coin in R. With a fair coin and 100 flips you expect about 50 Heads.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(c(\"Heads\", \"Tails\"), 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Heads\"\n```\n:::\n\n```{.r .cell-code}\nn <- 100\noutcomes <- sample(c(\"Heads\", \"Tails\"), n, replace = T)\n\nsum(outcomes == \"Heads\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 58\n```\n:::\n:::\n\n\nWith 10000 trials:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\ntrials <- 10000\nn <- 100\nset.seed(11)\nnumber_heads <- vector()\n\nfor (i in 1:trials) {\n    outcomes <- sample(c(\"Heads\", \"Tails\"), n, replace = T)\n    number_heads[i] <- sum(outcomes == \"Heads\")\n}\n\ndf <- data.frame(number_heads)\n\nggplot(df, aes(x = number_heads)) +\n    geom_density(color = \"blue\") +\n    labs(x = \"Number of heads in 100 tosses\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n# One-Sample t-tests\n\nTesting a mean against a hypothesized value. A one-sided test is looking to see if the true mean is greater than or less than a hypothesized value. A two-sided test is looking to see if the true mean is different than a hypothesized value.\n\nWe first need to calculate the t-statistic:\n\n$$\nt = \\frac{\\bar{x} - \\mu_0}{s_{\\bar{x}}}\n$$\n\n-   $H_0$ is rejected when the t-value is more extreme than one would expect to happen by chance when $H_0$ is true\n\n## Assumptions\n\nOne-sample t-tests need a large enough sample size for the Central Limit Theorem to hold. If you don't have sample size, then the population distribution needs to be Normal.\n\n## Example in R\n\n> We want to know if the true Sales Price is different then \\$178,000.\n>\n> The null hypothesis is $H_0$: $\\mu = 178000$ and the alternative is $H_a$: $\\mu \\neq 178000$. $\\alpha = 0.05$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\n\names <- make_ordinal_ames()\n\nt.test(ames$Sale_Price, mu = 178000)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.05825\nalternative hypothesis: true mean is not equal to 178000\n95 percent confidence interval:\n 177902.3 183689.9\nsample estimates:\nmean of x \n 180796.1 \n```\n:::\n:::\n\n\nDo not reject the null hypothesis as p-value $> \\alpha$\n\nTo conduct a directional t-test:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(ames$Sale_Price, mu = 178000, alternative = \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.02913\nalternative hypothesis: true mean is greater than 178000\n95 percent confidence interval:\n 178367.7      Inf\nsample estimates:\nmean of x \n 180796.1 \n```\n:::\n\n```{.r .cell-code}\nt.test(ames$Sale_Price, mu = 178000, alternative = \"less\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  ames$Sale_Price\nt = 1.8945, df = 2929, p-value = 0.9709\nalternative hypothesis: true mean is less than 178000\n95 percent confidence interval:\n     -Inf 183224.4\nsample estimates:\nmean of x \n 180796.1 \n```\n:::\n:::\n\n\n# Two-Sample t-tests\n\nWe are now testing the difference between two means.\n\n## Assumptions\n\n-   Independent observations\n-   Normally distributed data for each group\n-   Equal variances for each group\n    -   Tested formally with F-test to determine which t-test to use\n\n## F-Test for Equality of Variances\n\n::: text-center\n$H_0: \\sigma_1^2 = \\sigma_2^2$\n\n$H_a: \\sigma_1^2 \\neq \\sigma_2^2$\n\n$F = \\frac{\\max(s_1^2, s_2^2)}{\\min(s_1^2, s_2^2)}$\n:::\n\n## Two-Sample t-test in R\n\nWe first need to verify the normality condition:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ames, aes(sample = Sale_Price, color = Central_Air)) +\n    stat_qq() +\n    stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nNormality seems to fail with houses that have central air conditioning. However, for illustration we will still conduct the two-sample t-test.\n\nNote that in practice if normality fails then some groups consider not even conducting a t-test when variances are equal--just go straight to variances are not equal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvar.test(Sale_Price ~ Central_Air, data = ames)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tF test to compare two variances\n\ndata:  Sale_Price by Central_Air\nF = 0.2258, num df = 195, denom df = 2733, p-value < 2.2e-16\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.1854873 0.2800271\nsample estimates:\nratio of variances \n         0.2257977 \n```\n:::\n:::\n\n\nReject $H_0$ based on the p-value so we conclude that the variances are **not** equal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(Sale_Price ~ Central_Air, data = ames, var.equal = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWelch Two Sample t-test\n\ndata:  Sale_Price by Central_Air\nt = -27.433, df = 336.06, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group N and group Y is not equal to 0\n95 percent confidence interval:\n -90625.69 -78498.92\nsample estimates:\nmean in group N mean in group Y \n       101890.5        186452.8 \n```\n:::\n:::\n\n\nWith a regular two-sample t-test we reject the null hypothesis that the means are equal.\n\nHowever, our normality assumption wasn't satisfied so we should use a nonparametric test that does not rely on normality.\n\n## Wilcoxon Rank\n\nThe question we are answering with this test is, \"Are the median sale prices of houses with and without central air the same?\"\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwilcox.test(Sale_Price ~ Central_Air, data = ames)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tWilcoxon rank sum test with continuity correction\n\ndata:  Sale_Price by Central_Air\nW = 63164, p-value < 2.2e-16\nalternative hypothesis: true location shift is not equal to 0\n```\n:::\n:::\n\n\n### Interpretations of Wilcoxon\n\n| Conditions                                                                      | Interpretation of Significant Mann-Whiteney-Wilcoxon Test |\n|:-----------------------------------|:-----------------------------------|\n| Group distributions are identical in shape, variance and symmetric              | Difference in means                                       |\n| Group distributions are identical in shape, variance, but not symmetric         | Difference in medians                                     |\n| Group distributions are not identical in shape, variance, and are not symmetric | Difference in location (distributional dominance)         |\n\n# Conclusions in Testing\n\n1.  Always test for normality (either formally or informally first)\n2.  Test for variance equality\n3.  Conduct two-sample t-test or nonparametric depending on the normality condition",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}