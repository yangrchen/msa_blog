{
  "hash": "0b963b38c09cddd2d0da719e29f8fc15",
  "result": {
    "markdown": "---\ntitle: Diagnostics and Subset Selection\ndate: 09/05/2023\ndate-modified: 09/06/2023\ncategories:\n    -   logistic regression\n---\n\n\n# Setup\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(AmesHousing)\nlibrary(tidyverse)\nlibrary(reticulate)\n\nset.seed(123)\n\nuse_condaenv(\"msa\")\n\names <- make_ordinal_ames()\names <- ames |>\n    mutate(Bonus = ifelse(Sale_Price > 175000, 1, 0))\ntrain <- sample_frac(ames, 0.7)\n```\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\names = r.ames\ntrain = r.train\n```\n:::\n\n:::\n\n# Subset Selection Methods\n\nJust like with linear regression, we can use normal stepwise selection techniques (forward, backward, stepwise) to get different models.\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfull_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + factor(Half_Bath) + Lot_Area + factor(Central_Air) + Second_Flr_SF + TotRms_AbvGrd + First_Flr_SF, data = train, family = binomial())\n\nempty_model <- glm(Bonus ~ 1, data = train, family = binomial())\n\nstep_model <- step(empty_model, scope = list(lower = formula(empty_model), upper = formula(full_model)), direction = \"both\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nStart:  AIC=2777.81\nBonus ~ 1\n\n                      Df Deviance    AIC\n+ factor(Full_Bath)    4   1911.5 1921.5\n+ Gr_Liv_Area          1   1926.4 1930.4\n+ Garage_Area          1   2135.4 2139.4\n+ First_Flr_SF         1   2294.1 2298.1\n+ Fireplaces           1   2423.7 2427.7\n+ TotRms_AbvGrd        1   2449.7 2453.7\n+ factor(House_Style)  7   2542.1 2558.1\n+ factor(Half_Bath)    2   2608.1 2614.1\n+ Lot_Area             1   2621.9 2625.9\n+ Second_Flr_SF        1   2631.8 2635.8\n+ factor(Central_Air)  1   2654.3 2658.3\n<none>                     2775.8 2777.8\n\nStep:  AIC=1921.48\nBonus ~ factor(Full_Bath)\n\n                      Df Deviance    AIC\n+ Garage_Area          1   1616.3 1628.3\n+ Fireplaces           1   1659.5 1671.5\n+ Gr_Liv_Area          1   1665.7 1677.7\n+ First_Flr_SF         1   1711.7 1723.7\n+ Lot_Area             1   1811.5 1823.5\n+ factor(Half_Bath)    2   1812.2 1826.2\n+ factor(Central_Air)  1   1827.6 1839.6\n+ factor(House_Style)  7   1822.3 1846.3\n+ TotRms_AbvGrd        1   1885.6 1897.6\n+ Second_Flr_SF        1   1908.2 1920.2\n<none>                     1911.5 1921.5\n- factor(Full_Bath)    4   2775.8 2777.8\n\nStep:  AIC=1628.26\nBonus ~ factor(Full_Bath) + Garage_Area\n\n                      Df Deviance    AIC\n+ Fireplaces           1   1440.9 1454.9\n+ Gr_Liv_Area          1   1481.3 1495.3\n+ First_Flr_SF         1   1540.6 1554.6\n+ factor(Half_Bath)    2   1546.5 1562.5\n+ factor(House_Style)  7   1571.5 1597.5\n+ factor(Central_Air)  1   1585.6 1599.6\n+ Lot_Area             1   1589.0 1603.0\n+ TotRms_AbvGrd        1   1606.6 1620.6\n+ Second_Flr_SF        1   1608.7 1622.7\n<none>                     1616.3 1628.3\n- Garage_Area          1   1911.5 1921.5\n- factor(Full_Bath)    4   2135.4 2139.4\n\nStep:  AIC=1454.86\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces\n\n                      Df Deviance    AIC\n+ Gr_Liv_Area          1   1381.5 1397.5\n+ factor(Half_Bath)    2   1389.8 1407.8\n+ factor(House_Style)  7   1386.2 1414.2\n+ First_Flr_SF         1   1413.8 1429.8\n+ factor(Central_Air)  1   1426.2 1442.2\n+ Lot_Area             1   1435.1 1451.1\n+ Second_Flr_SF        1   1436.5 1452.5\n<none>                     1440.9 1454.9\n+ TotRms_AbvGrd        1   1440.3 1456.3\n- Fireplaces           1   1616.3 1628.3\n- Garage_Area          1   1659.5 1671.5\n- factor(Full_Bath)    4   1943.5 1949.5\n\nStep:  AIC=1397.55\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces + Gr_Liv_Area\n\n                      Df Deviance    AIC\n+ factor(House_Style)  7   1310.0 1340.0\n+ TotRms_AbvGrd        1   1335.1 1353.1\n+ factor(Central_Air)  1   1354.3 1372.3\n+ factor(Half_Bath)    2   1356.4 1376.4\n+ First_Flr_SF         1   1368.2 1386.2\n+ Second_Flr_SF        1   1370.7 1388.7\n<none>                     1381.5 1397.5\n+ Lot_Area             1   1381.2 1399.2\n- Gr_Liv_Area          1   1440.9 1454.9\n- Fireplaces           1   1481.3 1495.3\n- Garage_Area          1   1546.1 1560.1\n- factor(Full_Bath)    4   1626.6 1634.6\n\nStep:  AIC=1339.99\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces + Gr_Liv_Area + \n    factor(House_Style)\n\n                      Df Deviance    AIC\n+ factor(Half_Bath)    2   1261.2 1295.2\n+ TotRms_AbvGrd        1   1268.8 1300.8\n+ factor(Central_Air)  1   1288.2 1320.2\n<none>                     1310.0 1340.0\n+ Lot_Area             1   1309.7 1341.7\n+ Second_Flr_SF        1   1309.9 1341.9\n+ First_Flr_SF         1   1309.9 1341.9\n- factor(House_Style)  7   1381.5 1397.5\n- Gr_Liv_Area          1   1386.2 1414.2\n- Fireplaces           1   1394.8 1422.8\n- Garage_Area          1   1403.9 1431.9\n- factor(Full_Bath)    4   1511.2 1533.2\n\nStep:  AIC=1295.15\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces + Gr_Liv_Area + \n    factor(House_Style) + factor(Half_Bath)\n\n                      Df Deviance    AIC\n+ TotRms_AbvGrd        1   1228.3 1264.3\n+ factor(Central_Air)  1   1248.4 1284.4\n+ First_Flr_SF         1   1259.0 1295.0\n<none>                     1261.2 1295.2\n+ Second_Flr_SF        1   1260.3 1296.3\n+ Lot_Area             1   1260.7 1296.7\n- factor(Half_Bath)    2   1310.0 1340.0\n- Gr_Liv_Area          1   1331.8 1363.8\n- Fireplaces           1   1335.7 1367.7\n- Garage_Area          1   1336.7 1368.7\n- factor(House_Style)  7   1356.4 1376.4\n- factor(Full_Bath)    4   1490.6 1516.6\n\nStep:  AIC=1264.3\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces + Gr_Liv_Area + \n    factor(House_Style) + factor(Half_Bath) + TotRms_AbvGrd\n\n                      Df Deviance    AIC\n+ factor(Central_Air)  1   1218.9 1256.9\n<none>                     1228.3 1264.3\n+ First_Flr_SF         1   1226.7 1264.7\n+ Lot_Area             1   1227.3 1265.3\n+ Second_Flr_SF        1   1227.7 1265.7\n- TotRms_AbvGrd        1   1261.2 1295.2\n- factor(Half_Bath)    2   1268.8 1300.8\n- Fireplaces           1   1291.2 1325.2\n- Garage_Area          1   1294.7 1328.7\n- factor(House_Style)  7   1315.8 1337.8\n- Gr_Liv_Area          1   1330.3 1364.3\n- factor(Full_Bath)    4   1453.8 1481.8\n\nStep:  AIC=1256.88\nBonus ~ factor(Full_Bath) + Garage_Area + Fireplaces + Gr_Liv_Area + \n    factor(House_Style) + factor(Half_Bath) + TotRms_AbvGrd + \n    factor(Central_Air)\n\n                      Df Deviance    AIC\n<none>                     1218.9 1256.9\n+ First_Flr_SF         1   1217.7 1257.7\n+ Lot_Area             1   1218.0 1258.0\n+ Second_Flr_SF        1   1218.5 1258.5\n- factor(Central_Air)  1   1228.3 1264.3\n- TotRms_AbvGrd        1   1248.4 1284.4\n- factor(Half_Bath)    2   1253.1 1287.1\n- Fireplaces           1   1272.7 1308.7\n- Garage_Area          1   1272.9 1308.9\n- factor(House_Style)  7   1300.0 1324.0\n- Gr_Liv_Area          1   1324.1 1360.1\n- factor(Full_Bath)    4   1428.7 1458.7\n```\n:::\n:::\n\n:::\n\nWe don't use forward selection as much as it cannot remove variables from your model. Instead, we might favor backward selection or stepwise selection. When we decide on our main effects after data exploration, we can create interactions within our subset and use forward selection to see which interactions stay in our model.\n\n![Interactions with Forward Selection](images/forward-selection-interactions.png)\n\n# P-Value vs. AIC/BIC Metrics\n\nP-values are falling out of popularity, but it is primarily because people are not taking into account the $\\alpha$ level.\n\nMathematically, AIC and BIC for adding or removing variables with stepwise selection is the same thing as using p-values in LRT.\n\nAIC is not necessarily better than p-values when determining variable significance:\n\n$$\nAIC = -2\\log(L) + 2p\n$$\n\n-   $L$ is the likelihood function\n-   $p$ is the number of variables being estimated in the model\n\nIf a model is better with a lower AIC:\n\n$$\n\\begin{align*}\n-2\\log(L_{p+1}) + 2(p + 1) &< -2\\log(L_p) + 2p \\\\\n2 &< 2(\\log(L_{p+1}) - \\log(L_p))\n\\end{align*}\n$$\n\nRight hand side is an LRT that follows a $\\chi^2$ distribution with 1 degree of freedom. The significance level from this LRT can be calculated as:\n\n$$\n1 - P(\\chi_1^2 > 2) = 0.1573\n$$\n\nNotice how this AIC calculation does not account for sample size at all.\n\nThis is a relatively high significance level and for large datasets does not seem like a good technique for selecting variables.\n\nRecall the BIC calculation:\n\n$$\nBIC = -2\\log(L) + p\\log(n)\n$$\n\nIf we work through the math again then we find that BIC adjusts for sample size:\n\n$$\n1 - P(\\chi_1^2 > \\log(n)) =\\cdots\n$$\n\n![P-Value vs. BIC Selection](images/pvalue-bic.png)\n\n# Diagnostics\n\nLinear regression residuals have properties useful for model diagnostics. In a binary response model setting, we have residuals but they are not as intuitive.\n\n-   Deviance residuals\n-   Partial residuals\n-   Pearson residuals\n-   Etc.\n\n## Deviance\n\nA **saturated** model is a model that fits the data perfectly by essentially overfitting it. We create a variable for each unique combination of inputs. **Deviance** is a measure of how far the fitted model is from the saturated model--the error. Logistic regression minimizes the sum of squared deviances.\n\nDeviance residuals tell us how much each observation reduces the deviance.\n\n## Influence Statistics\n\n-   Cook's D\n    -   Measures the overall impact to the coefficients in the model\n-   DFBETAS\n    -   Measures standardized change in each parameter estimate with deletion of observation\n-   DIFCHISQ\n    -   Measures change in Pearson Chi-square with deletion of observation\n-   DIFDEV\n    -   Measures change in deviance with deletion of the observation\n\n::: {.panel-tabset}\n# R\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n\nlogit_model <- glm(Bonus ~ Gr_Liv_Area + factor(House_Style) + Garage_Area + Fireplaces + factor(Full_Bath) + Lot_Area + factor(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces, data = train, family = binomial())\n\n# influence.measures(logit_model)\n```\n:::\n\n\nTo plot Cook's D:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(logit_model, 4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nTo plot DFBETAS:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndfbetasPlots(logit_model, terms = \"Gr_Liv_Area\", id.n = 5, col = ifelse(logit_model$y == 1, \"red\", \"blue\"))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n# Python\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom statsmodels.genmod.families import Binomial\nfrom statsmodels.genmod.generalized_linear_model import GLM\n\nlog_model = GLM.from_formula(\n    \"Bonus ~ Gr_Liv_Area + Garage_Area + Fireplaces + C(Full_Bath) + Lot_Area + C(Central_Air) + TotRms_AbvGrd + Gr_Liv_Area:Fireplaces\",\n    data=train,\n    family=Binomial(),\n).fit()\nlog_model.summary()\n```\n\n::: {.cell-output-display}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>         <td>Bonus</td>      <th>  No. Observations:  </th>  <td>  2051</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  2039</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>    11</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -655.10</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Wed, 06 Sep 2023</td> <th>  Deviance:          </th> <td>  1310.2</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>22:49:11</td>     <th>  Pearson chi2:      </th> <td>5.34e+05</td>\n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>7</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.5106</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n             <td></td>               <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th>              <td>   -9.7344</td> <td>    1.441</td> <td>   -6.753</td> <td> 0.000</td> <td>  -12.560</td> <td>   -6.909</td>\n</tr>\n<tr>\n  <th>C(Full_Bath)[T.1]</th>      <td>   -0.0390</td> <td>    1.143</td> <td>   -0.034</td> <td> 0.973</td> <td>   -2.279</td> <td>    2.201</td>\n</tr>\n<tr>\n  <th>C(Full_Bath)[T.2]</th>      <td>    2.3699</td> <td>    1.143</td> <td>    2.073</td> <td> 0.038</td> <td>    0.129</td> <td>    4.610</td>\n</tr>\n<tr>\n  <th>C(Full_Bath)[T.3]</th>      <td>    4.5001</td> <td>    1.519</td> <td>    2.963</td> <td> 0.003</td> <td>    1.523</td> <td>    7.477</td>\n</tr>\n<tr>\n  <th>C(Full_Bath)[T.4]</th>      <td>   -1.2645</td> <td>    2.126</td> <td>   -0.595</td> <td> 0.552</td> <td>   -5.431</td> <td>    2.902</td>\n</tr>\n<tr>\n  <th>C(Central_Air)[T.Y]</th>    <td>    2.2895</td> <td>    0.567</td> <td>    4.037</td> <td> 0.000</td> <td>    1.178</td> <td>    3.401</td>\n</tr>\n<tr>\n  <th>Gr_Liv_Area</th>            <td>    0.0038</td> <td>    0.000</td> <td>    8.659</td> <td> 0.000</td> <td>    0.003</td> <td>    0.005</td>\n</tr>\n<tr>\n  <th>Garage_Area</th>            <td>    0.0046</td> <td>    0.000</td> <td>    9.520</td> <td> 0.000</td> <td>    0.004</td> <td>    0.006</td>\n</tr>\n<tr>\n  <th>Fireplaces</th>             <td>    1.9427</td> <td>    0.542</td> <td>    3.582</td> <td> 0.000</td> <td>    0.880</td> <td>    3.006</td>\n</tr>\n<tr>\n  <th>Lot_Area</th>               <td> 1.568e-05</td> <td> 1.57e-05</td> <td>    1.000</td> <td> 0.317</td> <td>-1.51e-05</td> <td> 4.64e-05</td>\n</tr>\n<tr>\n  <th>TotRms_AbvGrd</th>          <td>   -0.5049</td> <td>    0.080</td> <td>   -6.342</td> <td> 0.000</td> <td>   -0.661</td> <td>   -0.349</td>\n</tr>\n<tr>\n  <th>Gr_Liv_Area:Fireplaces</th> <td>   -0.0006</td> <td>    0.000</td> <td>   -1.859</td> <td> 0.063</td> <td>   -0.001</td> <td> 3.45e-05</td>\n</tr>\n</table>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport statsmodels.stats.tests.test_influence\n\nlog_diag = log_model.get_influence()\nlog_diag.summary_frame().head()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   dfb_Intercept  dfb_C(Full_Bath)[T.1]  ...  hat_diag  dffits_internal\n0      -0.004889               0.001706  ...  0.003598         0.023667\n1      -0.013708               0.004351  ...  0.011756        -0.167066\n2       0.023485               0.001300  ...  0.006500        -0.135552\n3      -0.002182               0.000401  ...  0.002478         0.008381\n4      -0.000883              -0.000017  ...  0.000711        -0.002877\n\n[5 rows x 16 columns]\n```\n:::\n:::\n\n::: {.cell}\n\n```{.python .cell-code}\nimport matplotlib.pyplot as plt\n\nlog_diag.plot_influence()\nplt.show()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n:::\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}