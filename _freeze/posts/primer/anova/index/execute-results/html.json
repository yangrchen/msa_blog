{
  "hash": "12ecb6a0db3e5973eb42f0507e093a1e",
  "result": {
    "markdown": "---\ntitle: Analysis of Variance\nformat: \n    html:\n        code-fold: show\n        toc: true\n        number-sections: true\n        default-image-extension: png\neditor:\n    render-on-save: true\nauthor: Yang Chen\ndate: 06/12/2023\n---\n\n\n# Two Sample Hypothesis Testing\n\nOne sample hypothesis tests are focused on one population parameter. However, sometimes we would like to compare multiple parameters against each other. This is the foundation of an analysis called **analysis of variance (ANOVA)**.\n\nRecall the one-sample case:\n\n$$\nH_0: \\mu \n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ \\ \\mu_0\n$$\n\n$$\nH_a: \\mu\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\mu_0\n$$\n\nIn the two-sample case, there are two parameters we are calculating so now we have an expression:\n\n$$\nH_0: \\mu_1 - \\mu_2\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ 0\n$$\n\n$$\nH_a: \\mu_1 - \\mu_2\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\ 0\n$$\n\n## Assumptions\n\n-   Assume two samples are independent of each other\n-   We have to take into account whether the variances are equal or not\n    -   Different hypothesis test structures depends on whether or not variances are equal\n\nRecall that our general test statistic is calculated as\n\n$$\n\\begin{align*}\n\\text{Test Statistic} &= \\frac{\\text{Statistic} - \\text{Null Value}}{\\text{Standard Error}} \\\\\n\\ \\\\\n&= \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{s_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\n\\end{align*}\n$$\n\n-   $s_p$ is the pooled standard deviation\n\nWe then calculate our p-value based on the t-distribution with $d.f. = n_1 - 1 + n_2 - 1 = n_1 + n_2 - 2$\n\n## Pooled Standard Deviation\n\nUnder assumption of *equal variances* we have two estimates of population variance--$s_1^2$ and $s_2^2$. We should combine both to get our estimate:\n\n$$\n\\begin{align*}\ns_p &= \\sqrt{\\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}} \\\\\n\\ \\\\\n&= \\sqrt{\\frac{\\sum (x_{1,i} - \\bar{x}_1)^2 + \\sum (x_{2,i} - \\bar{x}_2)^2}{n_1 + n_2 - 2}}\n\\end{align*} \n$$\n\n### Assumption on Pooled Std. Deviation\n\n-   Each population has an approximate Normal distribution\n-   Variances of two groups are equal\n\n## Confidence Interval\n\n$$\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2} \\times s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\n$$\n\n## Testing Difference in Means - Unequal Variances\n\n**Hypothesis Statements:**\n\n-   Same as the prior tests\n\n**Our Test Statistic:**\n\n$$\n\\text{Test Statistic} = \\frac{(\\bar{x}_1 - \\bar{x}_2) - D_0}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n$$\n\n-   Standard error changes since we need to test estimates of our two separate population variances separately $\\rightarrow$ cannot \"pool\" them\n\nThe degrees of freedom on our t-test are a more complicated expression:\n\n$$\nd.f. = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(\\frac{s_1^2}{n_1})^2}{n_1 - 1} + \\frac{(\\frac{s_2^2}{n_2})^2}{n_2 - 1}}\n$$\n\n## Confidence Interval\n\nFor different variances, we don't use the pooled variance\n\n$$\n(\\bar{x}_1 - \\bar{x}_2) \\pm t_{\\alpha/2}^* \\times \\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}\n$$\n\n-   Standard error changes using separate population variances\n\n## Example - Comparing Two Means\n\n> A human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager assumes the variability of salaries between genders is different, but wants to test if makes have higher average salary than females. The manager samples 62 males and 77 females. The sample of males had an average salary of $87,547 with a s.d. of $5,910. The sample of females had an average salary of $78,289 with a s.d. of $6,276. Run a hypothesis test.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# H_a: mu_1 > mu_2 with mu_1 representing mean of males\n\nt <- ((87547 - 78289) - 0) / sqrt(5910^2 / 62 + 6276^2 / 77)\nsprintf(\"Test statistic equals %.3f\", t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test statistic equals 8.930\"\n```\n:::\n\n```{.r .cell-code}\ndf <- (5910^2 / 62 + 6276^2 / 77)^2 / ((5910^2 / 62)^2 / 61 + (6276^2 / 77)^2 / 76)\nsprintf(\"Df: %d\", floor(df))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Df: 133\"\n```\n:::\n\n```{.r .cell-code}\npt(t, df, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.468658e-15\n```\n:::\n:::\n\n\n# Testing Differences in Variances\n\nPreviously we needed to assume the relationship between two variances to conduct test between means. We can run a **hypothesis test comparing two variances** to tell us which means test to use.\n\nKeep in mind there is no formal statistical test for comparing standard deviations--only variances.\n\n$$\nH_0: \\frac{\\sigma_1^2}{\\sigma_2^2}\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq \n\\end{cases}\n\\ \\ 1\n$$\n\n$$\nH_a: \\frac{\\sigma_1^2}{\\sigma_2^2}\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\ \\ 1\n$$\n\n-   Assume two samples are ind. of each other--value selected in one sample has no bearing on value selected on other sample\n-   Original populatino distributions are assumed to be approx. Normal\n\n**Hypothesis Statements:**\n\n$$\nH_0: \\frac{\\sigma_1^2}{\\sigma_2^2} \\hspace{1cm} H_a: \\frac{\\sigma_1^2}{\\sigma_2^2} \\neq 1\n$$\n\n**Test Statistic:**\n\n$$\nF = \\frac{s_i^2}{s_j^2}\n$$\n\n$$\nd.f._i = n_i - 1 \\hspace{1cm} d.f._j = n_j - 1\n$$\n\n-   The numerator is the larger standard deviation between two samples\n\n## Example - Comparing Two Variances\n\n> A human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager has no assumption about the variability of salaries between genders, but wants to test if makes have higher average salary than females. The manager samples 62 males and 77 females. The sample of males had an average salary of $87,547 with a s.d. of $5,910. The sample of females had an average salary of $78,289 with a s.d. of $6,276. Run a hypothesis test.\n>\n> **Need to first test if variances are equal or not before running test of means**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- 6276^2 / 5910^2\nsprintf(\"F Statistic: %.3f\", f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"F Statistic: 1.128\"\n```\n:::\n\n```{.r .cell-code}\ndf1 <- 76\ndf2 <- 61\n\npf(f, df1, df2, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3147624\n```\n:::\n:::\n\n\nAt a significance level of 0.05 we would not reject the null hypothesis that our variances are different.\n\n# Matched / Paired Differences\n\nSome instances where a paired difference (matched) sample is used to control for sources of variation that might distort conclusions\n\n## Example - Sources of Variation\n-   You have SAT scores for both boys and girls from a local school\n-   You believe that the boys and girls have the same avg. test score, but want to test otherwise\n-   Of the 39 females, 32 of them are part of the accelerated math and language arts program\n-   Of the 39 males, 11 of them are part of the accelerated math and language arts program\n\n**Matched samples** are samples selected such that each data value from one sample is related (or matched / paired) with a corresponding data value from a second sample\n\nIn the previous example, we would match boys and girls who were in the accelerated program and ones who were not.\n\nOur focus turns from individual values in the populations and to the values of the differences in the populations. All assumptions and calculations are done on the differences, not individual samples.\n\n$$\nH_0: \\mu_d \n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq\n\\end{cases}\n\\ \\ D_0\n$$\n\n$$\nH_a: \\mu_d\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\ \\ D_0\n$$\n\n-   Assumptions for matched pairs hypothesis test are same as for regular hypothesis test for means\n-   Large sample ($n > 50$) of differences\n-   Small samples with differences having Normal distribution\n\n**Hypothesis Statements:**\n\n$$\nH_0: \\mu_d \n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq \n\\end{cases}\n\\ \\ D_0\n\\hspace{2cm}\nH_a: \\mu_d\n\\begin{cases}\ntest\n\\end{cases}\n$$\n\n**Test Statistic:**\n\n$$\nt = \\frac{\\bar{x}_d - D_0}{\\frac{s_d}{\\sqrt{n_d}}}\n$$\n\n$$ d.f. = n_d - 1 $$\n\n## Confidence Interval\n$$\n\\bar{x}_d \\pm t_{\\alpha/2}^* \\times \\frac{s_d}{\\sqrt{n_d}}\n$$\n\n## Example - Paired Samples\n\n> A human resources manager of a large business firm is trying to determine if there exists gender bias in the pay scale of employees at the company. The manager samples 51 pairs of male and female employees where the pair has the same job title and experience at the company. The average difference in salaries is $2,131 with a s.d. of differences of $7,898. Run a hypothesis test.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt <- (2131 - 0) / (7898 / sqrt(51))\nsprintf(\"Test statistic: %.3f\", t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test statistic: 1.927\"\n```\n:::\n\n```{.r .cell-code}\ndf <- 50\n\npt(t, df, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02984447\n```\n:::\n:::\n\n\nAt a significant level of 0.05 we reject the null hypothesis that there is no gender bias in the pay scale of employees at the company.\n\n# Testing Population Proportions\n\n$$\nH_0: p_1 - p_2 \n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq \n\\end{cases}\n\\ \\ 0\n$$\n\n$$\nH_a: p_1 - p_2\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\ \\ 0\n$$\n\n-   Assume two samples are ind. of each other\n-   Sample size has to be large just like with single proportion\n-   $n_1p_1 \\geq 5 \\hspace{1cm} n_2p_2 \\geq 5$\n-   $(1 - n_1)p_1 \\geq 5 \\hspace{1cm} (1 - n_2)p_2 \\geq 5$\n\n**Hypothesis Statements:**\n\n$$\nH_0: p_1 - p_2\n\\begin{cases}\n\\geq \\\\\n= \\\\\n\\leq \n\\end{cases}\n\\ \\ D_0\n\\hspace{2cm}\nH_a: p_1 - p_2\n\\begin{cases}\n< \\\\\n\\neq \\\\\n>\n\\end{cases}\n\\ \\ D_0\n$$\n\n**Test Statistic:**\n\n$$ \n\\text{Test Statistic} = \\frac{(p_1 - p_2) - D_0}{\\sqrt{\\bar{p}(1 - \\bar{p})(\\frac{1}{n_1} + \\frac{1}{n_2})}}\n$$ \n\n$$\n\\bar{p} = \\frac{n_1p_1 + n_2p_2}{n_1 + n_2}\n$$\n\n-   Estimates of the proportion are combined together\n\nWhen we calculate our p-value we use the Normal distribution.\n\n## Confidence Interval\n\n$$\n(p_1 - p_2) \\pm z^* \\times \\sqrt{\\frac{p_1(1 - p_1)}{n_1} + \\frac{p_2(1 - p_2)}{n_2}}\n$$\n\n## Comprehensive Example\n> A researcher at a large university on the west coast is interested in comparing some factors between upperclassmen (juniors and seniors) and underclassmen (freshmen and sophomores) in the undergraduate school. The researcher believes that more experience in college may help students perform better in the classroom. The researcher is interested in testing if the average GPA of upperclassmen is greater than the average GPA of underclassmen. The researcher sampled 89 underclassmen with an average GPA of 2.75 with a s.d. of 0.91 and 102 upperclassmen with an average GPA of 3.07 and a s.d. of 1.02.\n\n> 1. The researcher did not use matched sampling. Do you agree with their decision?\n\nNo. There are other factors that can influence GPA like major that we could match on for equal comparison\n\n> 2. Conduct a hypothesis test on the variances to see if they are equal.\n\n$$\nH_0: \\frac{\\sigma_1^2}{\\sigma_2^2} \\leq 1\n\\hspace{2cm}\nH_a: \\frac{\\sigma_1^2}{\\sigma_2^2} > 1\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- 1.02^2 / 0.91^2\nsprintf(\"F Statistic: %0.3f\", f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"F Statistic: 1.256\"\n```\n:::\n\n```{.r .cell-code}\ndf1 <- 101\ndf2 <- 88\n\npf(f, df1, df2, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1367661\n```\n:::\n:::\n\n\nBased on a significance level of 0.05 we do not reject the null hypothesis. We do not have enough evidence to say that the variances between the two populations is different.\n\n> 3. Conduct the appropriate hypothesis test on the means to see if they are equal.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ns_p <- sqrt(((101 * 1.02^2) + (88 * 0.91^2)) / (102 + 89 - 2))\nsprintf(\"Pooled std. dev: %0.3f\", s_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Pooled std. dev: 0.970\"\n```\n:::\n\n```{.r .cell-code}\nt <- ((3.07 - 2.75) - 0) / (s_p * sqrt(1 / 102 + 1 / 89))\nsprintf(\"Test Statistic: %0.3f\", t)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Test Statistic: 2.274\"\n```\n:::\n\n```{.r .cell-code}\npt(t, 89 + 102 - 2, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.01205843\n```\n:::\n:::\n\n\nAt a significance level of 0.05 we reject our null hypothesis that more experience in college does not lead to higher GPA performance.\n\n## Example Continued\n\n>  Same researcher as before also believes that a higher proportion of upperclassmen live off campus compared to the proportion of underclassmen. While sampling the students in the previous sample, the researcher also asked whether the student lived off campus. Of the 89 underclassmen sampled, 27 lived off campus. Of the 102 upperclassmen sampled, 65 lived off campus.\n\n> Construct a 95% confidence interval for the difference between the proportion of upperclassmen living off campus to the proportion of underclassmen living off campus.\n\n$$\nH_0: p_1 - p_2 \\leq 0 \\hspace{1cm} H_a: p_1 - p_2 > 0\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\np1 <- 65 / 102\np2 <- 27 / 89\nsprintf(\"Upper p: %0.3f, Under p: %0.3f\", p1, p2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Upper p: 0.637, Under p: 0.303\"\n```\n:::\n\n```{.r .cell-code}\np_mean <- (102 * p1 + 89 * p2) / (102 + 89)\nsprintf(\"p_mean: %0.3f\", p_mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"p_mean: 0.482\"\n```\n:::\n\n```{.r .cell-code}\nz <- ((p1 - p2) - 0) / sqrt(p_mean * (1 - p_mean) * (1 / 102 + 1 / 89))\nsprintf(\"Z Statistic: %0.3f\", z)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"Z Statistic: 4.607\"\n```\n:::\n\n```{.r .cell-code}\nz_crit <- qnorm(0.05 / 2, lower.tail = FALSE)\nz_crit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.959964\n```\n:::\n\n```{.r .cell-code}\nmargin <- z_crit * sqrt(p1 * (1 - p1) / 102 + p2 * (1 - p2) / 89)\nmargin\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1335203\n```\n:::\n\n```{.r .cell-code}\nsprintf(\"95 perc. CI: %0.3f plus-minus %0.3f\", p1 - p2, margin)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"95 perc. CI: 0.334 plus-minus 0.134\"\n```\n:::\n:::\n\n\n> Conduct the appropriate hypothesis test to test the researcher's claim.\n\n$$\nH_0: p_1 - p_2 \\leq 0 \\hspace{1cm} H_a: p_1 - p_2 > 0\n$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\npnorm(z, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.044913e-06\n```\n:::\n:::\n\n\nAt a significance level of 0.0005 we reject the null hypothesis that the proportion of upperclassmen living off campus is not greater than the proportion of underclassmen living off campus.\n\n> Can you compare the confidence interval and the hypothesis test?\n\nNo as the hypothesis test is one-sided while the confidence interval is two-sided.\n\n# Analysis of Variance\n\nThe comparison of more than two categories of parameters is called **analysis of variance**.\n\n## One-Way ANOVA\n\nSimplest form of ANOVA is the one-way model.\n\n-   Independent samples are obtained from $k$ levels (categories) of a single factor (explanatory variable), then testing whether the $k$ levels have equal means.\n-   Similar to regression analysis in that we have one categorical variable predicting continuous response\n\n$$\nH_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\n$$\n\n$$\nH_a: \\text{At least one mean different than another}\n$$\n\n### Assumptions\n\n-   Normally distributed categories\n-   Equality of variances between categories\n-   Independence\n\n**Test Statistic:**\n\n$$\nF = \\frac{s_{max}^2}{s_{min}^2}\n$$\n\nThe p-value is calculated from Hartley-s F-max distribution which isn't covered here.\n\n### Sources of Variation\n\n-   **Within-Sample Variability**\n    -   Variability in response that exists within category of a variable\n        -   What you categories cannot explain (like SSE)\n-   **Between-Sample Variability**\n    -   Variability in response that exists between categories of a variable\n        -   What you categories can explain (like SSR)\n\n### Sum of Squares Within\n\nWithin sample is variability that you cannot explain by just knowing which category your observation falls into\n\n$$\nSSW = \\sum_{i=1}^k\\sum_{j=1}^{n_i} (x_{i,j} - \\bar{x}_i)^2\n$$\n\n### Sum of Squares Between\n\nBetween sample is variability that you can explain by just knowing which category your observation falls into\n\n$$\nSSB = \\sum_{i=1}^k n_i(\\bar{x}_i - \\bar{\\bar{x}})^2\n$$\n\n## Partitioning Variability in ANOVA\n\n\n```{mermaid}\n%%| fig-width: 10\n\nflowchart LR\nA(Total Variability) --> B(SSR + SSE)\nB --> C[Variability Between Groups]\nB --> D[Variability Within Groups]\n```\n\n\n## ANOVA F-test\n\n$$\n\\begin{align*}\nH_0&: \\mu_1 = \\mu_2 = \\cdots = \\mu_k \\\\\nH_a&: \\text{At least one mean different than another}\n\\end{align*}\n$$\n\nTest follows an F-distribution and is calculated as\n\n$$\nF = \\frac{(\\frac{SSB}{k - 1})}{(\\frac{SSW}{N - k})}\n$$\n\n-   $k$ categories would be $k - 1$ variables in a regression model\n-   $N$ is the total sample size across all categories\n\n## One-Way ANOVA Table\n\n![One-Way ANOVA Table](images/one_way_anova_table.png)\n\n## One-Way ANOVA Example\n> A marketing analyst is interested in testing the effectiveness of 4 different commercials describing their company’s new product. The marketing analyst randomly assigns a commercial to each of 32 cities across the country and measures the average increase in sales of their new product at their stores. The marketing analyst wants to test if there is a difference in sales between the commercials.\n\n> 1. Fill in the blanks on the ANOVA table.\n\n| Source | DF | SS | MS | F-Value | P-Value\n| --- | ---: | ---: | ---: | ---: | ---: |\n| Between | 3 | 2.3236 | 0.775 | 22.794 |\n| Within | 28 | 0.9587 | 0.034 | \n| Total | 31 | 3.2823 |\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npf(22.794, 3, 28, lower.tail = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.128339e-07\n```\n:::\n:::\n\n\n# Multiple Comparisons\n\nIf you reject the null hypothesis on the F-test then you have evidence that at least one category is different.\n\nOnce a difference is detected, we have to test each individual pair of categories to find where the differences are.\n\nThis process is called **multiple comparisons** or **ad-hoc testing**.\n\n## Multiple Comparisons Problem\n\n> You have a test which makes an error 5% of the time when performed.\n\n-   What is the probability of making an error on your first test?\n    -   5%\n-   What is the probability of making an error on your second test?\n    -   5%\n-   What is the probability of making *at least* one error in two tests?\n    -   9.75%\n\n### Different Types of Error\n\n**Comparison-wise error rate**\n\n-   Error rate for each individual test or comparison\n\n**Experiment-wise error rate**\n\n-   Error rate across all comparisons--proportion of experiments/comparisons in which at least one error occurs\n\nTests and confidence intervals usually control for comparison-wise, $\\alpha$, but ideally want to control for experiment-wise.\n\n## Multiple Comparison Methods\n\n| Number of Groups Compared | Number of Comparisons | Experimentwise Error Rate |\n| ---: | ---: | ---: |\n| 2 | 1 | 0.05 |\n| 3 | 3 | 0.14 |\n| 4 | 6 | 0.26 |\n| 5 | 10 | 0.40 |\n\n-   $EER \\leq 1 - (1 - \\alpha)^{nc}$ where $nc$ is the number of comparisons\n\n\n\n```{mermaid}\n%%| fig-width: 10\n\nflowchart LR\nA(Control Comparisonwise Error Rate) --> B(Pairwise t-tests)\nC(Control Experimentwise Error Rate) --> D[Compare All Pairs Tukey]\n```\n\n\n## Tukey's HSD Test\n\nHSD represents the **Honest Significant Difference** or **Critical Range**\n\nWe use Tukey's when we consider pairwise comparisons\n\n-   Experimentwise error rate is equal to $\\alpha$ when all pairwise comparisons are considered\n-   Experimentwise error rate is less than $\\alpha$ when fewer than all pairwise comparisons are considered\n-   Replaces margin of error calculation for a typical confidence interval for a difference in means with an adjusted margin of error\n\n$$\n\\text{Critical Range (Margin of Error)} = q_a \\times \\sqrt{\\frac{MSW}{2} \\times (\\frac{1}{n_i} + \\frac{1}{n_j})}\n$$\n\n-   $q_a$ is from studentized range distribution\n\n# Fixed vs. Random Effects\n\nInference drawn from ANOVA depends on whether the factor levels in the procedure are selected on purpose or randomly.\n\n**Fixed Effects**\n\n-   Inferences extend only to factor levels being analyzed because the levels were purposefully chosen as levels of interest\n\n**Random Effects**\n\n-   Inferences extend beyond just factor levels being tested because the levels were randomly selected from a larger group of levels\n\n## Randomized Blocking\n\n### Sources of Variation\n\nGenerally, comparing many population means works well in certain situations\n\nThere are some instances where **blocking** is used to control for sources of variation that might distort conclusions\n\n## Example\n> The same marketing analyst as before is interested in testing the effectiveness of 4 different commercials describing their company’s new product. The marketing analyst randomly assigns a commercial to each of 32 cities across the country and measures the average increase in sales of their new product at their stores. The four commercial average sales were $1.2M for commercial A, $1.8M for B, **$0.76M for C**, and $1.3M for D. Where are the differences in sales?\n\n-   What if the new product is a warm coat and a majority of the cities seeing C were warm weather cities?\n-   The same marketing analyst as before is interested in testing the effectiveness of 4 different commercials describing their company's new product. Split (**block**) country into 8 regions. Show each commercial to one city in each region. Sample size still 32.\n\n## Assumptions\n\nSame as One-Way ANOVA:\n\n-   Normally distributed categories\n-   Equality of variances between categories\n-   Independence\n\nBlocking can come from collection of data as well as the analysis of the data as a variable being added to the model.\n\nWhen a new variable is added, we get a new source of variation--**sum of squares of blocking**.\n\n## Sum of Squares Blocks\n\n$$\nSSBL = \\sum_{j=1}^b k(\\bar{x}_j - \\bar{\\bar{x}})^2\n$$\n\nThe sum of squares comes out of the error sum of squares and gets brought into the model--the SSW shrinks even more.\n\n## Blocking ANOVA Table\n\n![Blocking ANOVA Table](images/one_way_anova_table_blocking.png)\n\n-   The F-Value in the Blocking row is the F-test with $H_a$ at least one block mean not equal\n\n## Post-hoc Analysis for blocking\n\nTukey-Kramer ANOVA comparisons do not work for blocking.\n\nInstead, we have **Fisher's Least Significant Difference**. Fisher's LSD is a recalculation of the margin of error for the difference in means confidence interval just like Tukey's critical range.\n\n$$\nLSD = t^* \\times \\sqrt{MSW} \\times \\sqrt{\\frac{2}{b}}\n$$",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}