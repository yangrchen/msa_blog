{
  "hash": "0ee329b7189a5080a2b3a3decf9431f7",
  "result": {
    "markdown": "---\ntitle: Categorical Data Analysis\nformat: \n    html:\n        code-fold: show\n        toc: true\n        number-sections: true\n        default-image-extension: png\neditor:\n    render-on-save: true\nauthor: Yang Chen\ndate: 06/14/2023\n---\n\n# Overview\n| Type of Predictors \\| Type of Response | Categorical | Continuous | Continuous and Categorical \n---- | --- | --- | --- | --- \nContinuous | Analysis of Variance | Ordinary Least Squares Regression | Analysis of Covariance\nCategorical | Tests of Association | Logistic Regression | Logistic Regression\n\n# Describing Categorical Data\n\nIn categorical data analysis, we use qualitative data types which describes data whose measurement scale is by categorical.\n\n## Qualitative Data Types\n\n**Nominal**\n\n-   Categories with no logical ordering\n\n**Ordinal**\n\n-   Categories with a logical order / only two ways to order the categories (binary is ordinal)\n\n## Examining Categorical Variables\n\nBy examining distributions of categorical variables we can\n\n1.   Determine the frequencies of data values\n2.   Recognize possible associations among variables\n\nAssociation exists between two categorical variables if distribution of one variable changes when the level of the other variable changes.\n\nIf there is no association, distribution of first variable is the same regardless of the level of the other.\n\n# Tests of Association\n\n|  | Happy | Sad \n| --- | --: | --: |\nSunny | 87% | 13%\nStormy | 40% | 60%\n\n-   How much of a change is required to believe there is actually a difference in manager mood based on weather?\n\n**Hypothesis Statements:**\n\n$$\nH_0: \\text{There is no association between Mood and Weather}\n$$\n\n$$\nH_a: \\text{There is an association between Mood and Weather}\n$$\n\n## Chi-Square Tests\n\n$$\n\\begin{align*}\nH_0&: \\text{No Association} &\\text{Observed freq = Expected freq.} \\\\\nH_a&: \\text{Association} &\\text{Observed freq.} \\neq \\text{Expected freq.}\n\\end{align*}\n$$\n\nExpected freq. are calculated by the formula \n\n$$\n\\frac{\\text{Row Total} \\times \\text{Column Total}}{\\text{Sample Size}}\n$$\n\n### $\\chi^2$ Distribution\n\n-   Bounded below by zero\n-   Right skewed\n-   One set of degrees of freedom\n\n::: {.cell execution_count=1}\n``` {.python .cell-code code-fold=\"true\"}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import chi2\n\nx = np.arange(0, 20, 0.001)\n\nplt.plot(x, chi2.pdf(x, df=4))\n```\n\n::: {.cell-output .cell-output-display}\n![Plot of a $\\chi^2$ distribution with d.f. 4](index_files/figure-html/fig-chi-square-output-1.png){#fig-chi-square width=588 height=411}\n:::\n:::\n\n\n### Pearson $\\chi^2$ Test\n\n$$\nQ_P = \\sum_{i=1}^{R} \\sum_{j=1}^{C} \\frac{(Obs_{i,j} - Exp_{i,j})^2}{Exp_{i,j}}\n$$\n\n$$\nd.f. = (\\#\\text{Rows} - 1)(\\#\\text{Columns} - 1)\n$$\n\n### Likelihood Ratio $\\chi^2$ Test\n\n$$\nQ_{LR} = 2 \\times \\sum_{i=1}^{R}\\sum_{j=1}^{C} Obs_{i,j} \\times \\log{(\\frac{Obs_{i,j}}{Exp_{i,j}})}\n$$\n\n$$\nd.f. = (\\#\\text{Rows} - 1)(\\#\\text{Columns} - 1)\n$$\n\n## Example\n\n> A manager of a major car dealership wants to determine if the membership of a client in the loyalty program is associated with the color of car that they buy. With this knowledge, it potentially could help the sales staff show different cars to different clients to help improve the likelihood of a sale. The manager pull information from the previous years sales.\n\n> 1. Calculate the expected counts in the right table\n\n![](images/chi_square_example_1_table.png)\n\nRecall that expected frequency is given by the the product of row total and column total over sample size.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nd = {\n    'black': {'yes': 149, 'no': 101},\n    'white': {'yes': 101, 'no': 66},\n    'blue': {'yes': 72, 'no': 108},\n    'red': {'yes': 96, 'no': 161},\n    'green': {'yes': 39, 'no': 65}\n}\ndf_cars = pd.DataFrame(d).T\ndf_cars['total'] = df_cars['yes'] + df_cars['no']\ndf_cars['exp_y'] = df_cars['total'] * \\\n    df_cars['yes'].sum() / df_cars['total'].sum()\ndf_cars['exp_n'] = df_cars['total'] * \\\n    df_cars['no'].sum() / df_cars['total'].sum()\ndf_cars.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>yes</th>\n      <th>no</th>\n      <th>total</th>\n      <th>exp_y</th>\n      <th>exp_n</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>black</th>\n      <td>149</td>\n      <td>101</td>\n      <td>250</td>\n      <td>119.258873</td>\n      <td>130.741127</td>\n    </tr>\n    <tr>\n      <th>white</th>\n      <td>101</td>\n      <td>66</td>\n      <td>167</td>\n      <td>79.664927</td>\n      <td>87.335073</td>\n    </tr>\n    <tr>\n      <th>blue</th>\n      <td>72</td>\n      <td>108</td>\n      <td>180</td>\n      <td>85.866388</td>\n      <td>94.133612</td>\n    </tr>\n    <tr>\n      <th>red</th>\n      <td>96</td>\n      <td>161</td>\n      <td>257</td>\n      <td>122.598121</td>\n      <td>134.401879</td>\n    </tr>\n    <tr>\n      <th>green</th>\n      <td>39</td>\n      <td>65</td>\n      <td>104</td>\n      <td>49.611691</td>\n      <td>54.388309</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n> 2. Compute $Q_P$ and $Q_{LR}$ and summarize results.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndef calculate_pearson(row):\n    return (row['yes'] - row['exp_y']) ** 2 / row['exp_y'] + (row['no'] - row['exp_n']) ** 2 / row['exp_n']\n\n\ndef calculate_likelihood(row):\n    return 2 * ((row['yes'] * np.log(row['yes'] / row['exp_y'])) + (row['no'] * np.log(row['no'] / row['exp_n'])))\n\n\nq_pearson = df_cars.apply(calculate_pearson, axis=1).sum()\nlikelihood = df_cars.apply(calculate_likelihood, axis=1).sum()\n\nprint(f'Q_p: {q_pearson}, Q_LR: {likelihood}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nQ_p: 44.76457096344832, Q_LR: 45.07972866310165\n```\n:::\n:::\n\n\n## Ordinal Compared to Nominal Tests\n\n-   Pearson and Likelihood Ratio $\\chi^2$ tests can handle any type of categorical variable\n-   Ordinal variables provide extra information since order of the categories matters compared to nominal\n-   Can test for even more with ordinal vars. against other ordinal vars.--whether two ordinal vars. have a linear relationship as compared to just a general one\n\n**Hypothesis Statements:**\n\n$$\n\\begin{align*}\nH_0&: \\text{No Linear Association} \\\\\nH_a&: \\text{Linear Association} \\\\\n\\end{align*}\n$$\n\n## Mantel-Haenszel $\\chi^2$ Test\n\n$$\nQ_{MH} = (n - 1)r^2\n$$\n\n-   $r^2$ is the Pearson correlation between row and column variables\n\n# Measures of Association\n\n$\\chi^2$ deteremines whether an association exists but it *does not* measure strength of association.\n\nMeasures of association do not measure whether an association exists. Some different measures of association include:\n\n-   Odds Ratio (Only for 2x2 tables, binary vs. binary)\n-   Cramer's V (Any size table)\n\n## Odds Ratio\n\nOdds ratio measure how much more likely, with respect to **odds**, a certain event occurs in one group relative to its occurrence in another group.\n\nOdds of an event occurring is **not** the same as the probability that an event occurs.\n\n$$\n\\text{Odds} = \\frac{p}{1 - p}\n$$\n\n### Probability vs. Odds of an Outcome\n\n|  | Yes | No |\n| :-: | --: | --: |\n| Loyal | 20 | 60 |\n| Non-Loyal | 10 | 90 |\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nd = {'yes': [20, 10], 'no': [60, 90]}\ndf_loyalty = pd.DataFrame(d, index=['Loyal', 'Non-Loyal'])\n\ndf_loyalty['prob_y'] = df_loyalty['yes'] / df_loyalty.iloc[:, :2].sum(axis=1)\ndf_loyalty['prob_n'] = df_loyalty['no'] / df_loyalty.iloc[:, :2].sum(axis=1)\ndf_loyalty['odds_y'] = (df_loyalty['prob_y'] / df_loyalty['prob_n']).round(3)\ndf_loyalty['odds_n'] = df_loyalty['prob_n'] / df_loyalty['prob_y']\n\ndf_loyalty.head()\nprint(\n    f'Odds Ratio, Loyal to Non-Loyal: {df_loyalty.loc[\"Loyal\", \"odds_y\"] / df_loyalty.loc[\"Non-Loyal\", \"odds_y\"]}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOdds Ratio, Loyal to Non-Loyal: 3.0\n```\n:::\n:::\n\n\n-   Loyal program customers have **3 times the odds** of buying the product as compared to customers not in the loyalty program.\n\n## Cramer's V\n\nWhen you have more than >2 categories in one or both variables we use Cramer's V.\n\n$$\nV = \\sqrt{\\frac{(\\frac{Q_P}{n})}{\\min(\\#\\text{Rows} - 1, \\#\\text{Columns} - 1)}}\n$$\n\n-   Bounded between 0 and 1 (-1 and 1 for 2x2 scenario) where closer to 0 the weaker the relationship\n\n## Example\n\n> The same manager as the previous example now wants to know the strength of the relationship between the color of car and loyalty program. Use the appropriate measure of association to calculate this.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nn = df_cars['total'].sum()\nrows, cols = df_cars.iloc[:, :2].shape\n\ncramer_v = np.sqrt((q_pearson / n) / np.min([rows - 1, cols - 1]))\nnp.round(cramer_v, 3)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n0.216\n```\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}